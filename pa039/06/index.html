<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="What makes GPU powerful? Parallelism types Task parallelism the problem is decomposed to parallel tasks tasks are typically complex, they can perform different jobs complex synchronization best for lower number of high-performance processors/cores Data parallelism the parallelism on a level of data structures typically the same operation on multiple elements of a data structure can be executed on simpler processors Programmer point of view some problems are more task-parallel, some more data-parallel (tree traversal vs."><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pa039/06/><title>notes 6 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="notes 6"><meta itemprop=description content="What makes GPU powerful? Parallelism types Task parallelism the problem is decomposed to parallel tasks tasks are typically complex, they can perform different jobs complex synchronization best for lower number of high-performance processors/cores Data parallelism the parallelism on a level of data structures typically the same operation on multiple elements of a data structure can be executed on simpler processors Programmer point of view some problems are more task-parallel, some more data-parallel (tree traversal vs."><meta itemprop=datePublished content="2022-05-05T00:00:00+00:00"><meta itemprop=dateModified content="2022-05-05T00:00:00+00:00"><meta itemprop=wordCount content="801"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="notes 6"><meta name=twitter:description content="What makes GPU powerful? Parallelism types Task parallelism the problem is decomposed to parallel tasks tasks are typically complex, they can perform different jobs complex synchronization best for lower number of high-performance processors/cores Data parallelism the parallelism on a level of data structures typically the same operation on multiple elements of a data structure can be executed on simpler processors Programmer point of view some problems are more task-parallel, some more data-parallel (tree traversal vs."><meta property="og:title" content="notes 6"><meta property="og:description" content="What makes GPU powerful? Parallelism types Task parallelism the problem is decomposed to parallel tasks tasks are typically complex, they can perform different jobs complex synchronization best for lower number of high-performance processors/cores Data parallelism the parallelism on a level of data structures typically the same operation on multiple elements of a data structure can be executed on simpler processors Programmer point of view some problems are more task-parallel, some more data-parallel (tree traversal vs."><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pa039/06/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="article:published_time" content="2022-05-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-05T00:00:00+00:00"><meta property="og:site_name" content="idk"><meta property="article:published_time" content="2022-05-05 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pa039/06/>notes 6</a></h2><div class=post-content><h1 id=what-makes-gpu-powerful>What makes GPU powerful?</h1><ul><li>Parallelism types</li><li>Task parallelism</li><li>the problem is decomposed to parallel tasks</li><li>tasks are typically complex, they can perform different jobs</li><li>complex synchronization</li><li>best for lower number of high-performance processors/cores</li><li>Data parallelism</li><li>the parallelism on a level of data structures</li><li>typically the same operation on multiple elements of a data structure</li><li>can be executed on simpler processors</li><li>Programmer point of view</li><li>some problems are more task-parallel, some more data-parallel (tree traversal vs. vector addition)</li><li>Hardware designer point of view</li><li>processors for data-parallel computations can be simpler</li><li>so we can get more arithmetic power per square centimeter</li><li>simpler memory access patterns allows to create a memory with higher bandwidth</li></ul><h1 id=cpu-vs-gpu>CPU vs GPU</h1><ul><li>hundreds ALU in tens of cores vs. tens of thousands ALU in tens of multiprocessors</li><li>out-of-order vs. in-order</li><li>MIMD, SIMD for short vectors vs. SIMT for long vectors</li><li>big cache vs. small cache, often read-only</li></ul><p>GPUs use more transistors for ALUs than for cache and instruction control => higher peak performance, less universal</p><h1 id=high-end-gpu>High-end GPU</h1><ul><li>co-processor with dedicated memory</li><li>asynchronous instructions execution</li><li>connected via PCI-E to the rest of the system</li></ul><h1 id=cuda-compute-unified-device-architecture>CUDA (Compute Unified Device Architecture)</h1><ul><li>architecture for parallel computations developed by NVIDIA</li><li>a programming model allowing to implement general programs on GPUs</li><li>can be used with multiple programming languages</li></ul><h1 id=c-for-cuda>C for CUDA</h1><ul><li>explicit separation of a host (CPU) and a device (GPU) code</li><li>threads hierarchy</li><li>memory hierarchy</li><li>synchronization mechanisms</li><li>API (context manipulation, memory, errors handling etc.)</li></ul><h1 id=threads-hierarchy>Threads hierarchy</h1><ul><li>threads are organized into thread-blocks</li><li>thread-blocks creates a grid</li><li>a computational problem is typically decomposed into independent sub-problems, solved by thread-blocks</li><li>subproblems are further parallelized and solved by (potentially collaborating) threads</li><li>ensures good scaling</li></ul><h1 id=memory-hierarchy>Memory hierarchy</h1><p>Multiple types of memory</p><ul><li>differ visibility</li><li>differ in life-time</li><li>differ in latency and bandwidth</li></ul><h1 id=c-for-cuda-1>C for CUDA</h1><p>The syntax of C is extended by function type quantifiers, determining from where the function can be called and where it is executed</p><ul><li><strong>device</strong> function is executed on device (GPU) and called from device code</li><li><strong>global</strong> function is executed on device and called from host (CPU)</li></ul><h1 id=example--vector-addition>Example â€“ vector addition</h1><p>For complete computation of vector addition, we need to:</p><ul><li>allocate memory for the vectors, and fill it with some data</li><li>allocate GPU memory</li><li>copy vectors a and b to GPU memory</li><li>compute vector addition on GPU</li><li>copy back the result from GPU memory into c</li></ul><p>Kernel execution:</p><ul><li>the kernel is called as a C-function; between the name and the arguments, there are triple angle brackets with specification of grid and block size</li><li>we need to know block size and their count</li><li>we will use 1D block and grid with fixed block size</li><li>the size of the grid is determined in a way to compute the whole problem of vector sum</li></ul><h1 id=gpu-memory-management>GPU memory management</h1><p>We use managed memory, so CUDA automatically copies data between CPU and GPU.</p><ul><li>memory coherency is automatically ensured</li><li>we cannot access managed memory while any GPU kernel is running</li></ul><h1 id=thread-local-memory>Thread-local memory</h1><p>Registers:</p><ul><li>the fastest memory, directly used by instructions</li><li>local variables and intermediate results are stored into registers</li><li>if there is enough registers</li><li>if compiler can determine array indexes in compile time</li><li>life-time of a thread</li></ul><p>Local memory</p><ul><li>what cannot fit into registers, goes to the local memory</li><li>physically stored in global memory, have longer latency and lower bandwidth</li><li>life-time of a thread</li></ul><h1 id=block-local-memory>Block-local memory</h1><p>Shared memory</p><ul><li>the speed is close to registers</li><li>can have dynamic size (determined during kernel execution), if declared as extern without specification of the array size</li><li>life-time of a thread block</li></ul><h1 id=gpu-local-memory>GPU-local memory</h1><p>Global memory</p><ul><li>order-of-magnitude lower bandwidth compared to the shared memory</li><li>latency in hundreds of GPU clocks</li><li>coalesced access necessary for efficient access</li><li>life-time of an application</li><li>can be cached (depending on GPU architecture)</li></ul><h1 id=other-memories>Other memories</h1><ul><li>constant memory</li><li>texture memory</li><li>system memory</li></ul><h1 id=thread-block-scope-synchronization>Thread-block-scope synchronization</h1><ul><li>native barrier</li><li>has to be visited by all threads within a thread-block</li><li>only one instruction, very fast if not reduce parallelism</li></ul><h1 id=atomic-operations>Atomic operations</h1><ul><li>perform read-modify-write operations using shared or global memory</li><li>no interference with other threads</li><li>arithmetic and bitwise operations</li></ul><h1 id=synchronization-of-memory-operations>Synchronization of memory operations</h1><p>Compiler can optimize access into shared and global memory by placing intermediate results into registers, and it can change order of memory operations</p><ul><li>threadfence() and threadfence block() can be used to ensure data we are storing are visible for others</li><li>variables declared as volatile are always read/written from/to global or shared memory</li></ul><h1 id=thread-block-synchronization>Thread-block synchronization</h1><ul><li>global memory visible for all blocks</li><li>but weak possibilities to synchronize between blocks</li><li>in general no global barrier (can be implemented if all blocks are persistent on GPU)</li><li>using atomic operations can solve some problems</li><li>generic global barrier only by kernel invocation</li><li>harder to program, but allows better scaling</li></ul><h1 id=global-synchronization-via-atomic-operations>Global synchronization via atomic operations</h1><p>Alternative implementation of vector reduction</p><ul><li>each thread-block reduces a subvector</li><li>the last running thread-block adds results of all thread-blocks</li></ul></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.c7e937cd76fb1097985d5c8ae541eb44da0ddf0ecfc093bb5e2fe6be756ac425b77825c1c7d6f19ac4c0c4d18ae466ea8bf31c3f646c557dedb72d8a31bbd7e2.js integrity="sha512-x+k3zXb7EJeYXVyK5UHrRNoN3w7PwJO7Xi/mvnVqxCW3eCXBx9bxmsTAxNGK5Gbqi/McP2RsVX3tty2KMbvX4g=="></script></body></html>