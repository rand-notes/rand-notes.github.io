<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="Parallel computers Small-scale multiprocessing
2–several hundreds of cores mostly SMP (shared memory systems) Large-scale multiprocessing from hundreds to millions of cores Most often distributed memory Architecture
Single Instruction Multiple Data, SIMD Multiple Instruction Multiple Data, MIMD Programming models
Single Program Multiple Data, SPMD
Multiple programs Multiple Data, MPMD
Concurrent: A single program with multiple tasks in progress
Parallel: A single program with multiple task closely cooperating
Distributed: Several programs (loosely) cooperating"><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pa039/02/><title>notes 02 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="notes 02"><meta itemprop=description content="Parallel computers Small-scale multiprocessing
2–several hundreds of cores mostly SMP (shared memory systems) Large-scale multiprocessing from hundreds to millions of cores Most often distributed memory Architecture
Single Instruction Multiple Data, SIMD Multiple Instruction Multiple Data, MIMD Programming models
Single Program Multiple Data, SPMD
Multiple programs Multiple Data, MPMD
Concurrent: A single program with multiple tasks in progress
Parallel: A single program with multiple task closely cooperating
Distributed: Several programs (loosely) cooperating"><meta itemprop=datePublished content="2022-05-05T00:00:00+00:00"><meta itemprop=dateModified content="2022-05-05T00:00:00+00:00"><meta itemprop=wordCount content="2178"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="notes 02"><meta name=twitter:description content="Parallel computers Small-scale multiprocessing
2–several hundreds of cores mostly SMP (shared memory systems) Large-scale multiprocessing from hundreds to millions of cores Most often distributed memory Architecture
Single Instruction Multiple Data, SIMD Multiple Instruction Multiple Data, MIMD Programming models
Single Program Multiple Data, SPMD
Multiple programs Multiple Data, MPMD
Concurrent: A single program with multiple tasks in progress
Parallel: A single program with multiple task closely cooperating
Distributed: Several programs (loosely) cooperating"><meta property="og:title" content="notes 02"><meta property="og:description" content="Parallel computers Small-scale multiprocessing
2–several hundreds of cores mostly SMP (shared memory systems) Large-scale multiprocessing from hundreds to millions of cores Most often distributed memory Architecture
Single Instruction Multiple Data, SIMD Multiple Instruction Multiple Data, MIMD Programming models
Single Program Multiple Data, SPMD
Multiple programs Multiple Data, MPMD
Concurrent: A single program with multiple tasks in progress
Parallel: A single program with multiple task closely cooperating
Distributed: Several programs (loosely) cooperating"><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pa039/02/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="article:published_time" content="2022-05-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-05T00:00:00+00:00"><meta property="og:site_name" content="idk"><meta property="article:published_time" content="2022-05-05 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pa039/02/>notes 02</a></h2><div class=post-content><h1 id=parallel-computers>Parallel computers</h1><p>Small-scale multiprocessing</p><ul><li>2–several hundreds of cores</li><li>mostly SMP (shared memory systems)
Large-scale multiprocessing</li><li>from hundreds to millions of cores</li><li>Most often distributed memory</li></ul><p>Architecture</p><ul><li>Single Instruction Multiple Data, SIMD</li><li>Multiple Instruction Multiple Data, MIMD</li></ul><p>Programming models</p><ul><li><p>Single Program Multiple Data, SPMD</p></li><li><p>Multiple programs Multiple Data, MPMD</p></li><li><p>Concurrent: A single program with multiple tasks in progress</p></li><li><p>Parallel: A single program with multiple task closely cooperating</p></li><li><p>Distributed: Several programs (loosely) cooperating</p></li></ul><h1 id=simd>SIMD</h1><p>All processors synchronized</p><ul><li>All performing the same instruction in any given time</li><li>Analogy of vector processors
Simple processors
Simple programming model: but difficult programming</li></ul><h1 id=vector-processor>Vector processor</h1><p>Processor able to work directly with vectors of data</p><ul><li>vector is a data type of the underlying instruction set</li><li>Cray introduced even vector registers (otherwise direct work with the memory)</li></ul><p>Vector Load/Store</p><ul><li>“composing” vector from different memory words/areas</li><li>vector of registers that keep addresses of memory words with actual data</li><li>“to localize” data for further processing</li><li>in practice the scather/gather operations directly over the main memory</li></ul><h1 id=vector-processor-1>Vector processor</h1><p>Memory subsystem</p><ul><li>as a default does not work with caches</li><li>interleaved (banked) memory</li><li>several concurrent operations over the main memory</li><li>it has higher throughput than use of caches</li></ul><h1 id=interleaved-memory>Interleaved memory</h1><p>Interleaved memory is designed to compensate for the relatively slow speed of dynamic random-access memory (DRAM) or core memory by spreading memory addresses evenly across memory banks.</p><h1 id=mimd>MIMD</h1><ul><li>Fully asynchronous system</li><li>Individual processors fully independent</li><li>Higher flexibility, At least in theory higher efficiency</li><li>Explicit synchronization</li><li>Difficult programming (easy race condition)</li></ul><h1 id=communication-models>Communication models</h1><ul><li>Shared Memory Architecture</li><li>Message passing</li></ul><h1 id=shared-memory-architecture>Shared Memory Architecture</h1><ul><li>Memory separated from processors</li><li>Uniform access to the memory</li><li>Bus as the easiest interconnect</li><li>Cheap interprocess/thread communication</li><li>Complex overlap of processing and communication (active waiting)</li></ul><h1 id=message-passing>Message Passing</h1><ul><li>Each processor “visible”</li><li>Each processor has its own memory</li><li>Explicit communication – message passing</li><li>High communication cost (exchange of data)</li><li>More easier overlap of processing and communication</li></ul><h1 id=hybrid-systems>Hybrid systems</h1><ul><li>Nonuniform memory access architecture (NUMA)</li><li>Cache-only memory access architecture (COMA)</li><li>Distributed shared-memory (DSM)</li></ul><h1 id=non-uniform-memory-access>Non-uniform memory access</h1><ul><li>Access to different memory addresses takes different time</li><li>Provides for higher scalability</li><li>Potentially lower throughput</li><li>Cache memory coherence problem</li></ul><h1 id=cache-only-memory-access>Cache only memory access</h1><ul><li>NUMA but resembling cache memory behavior</li><li>Data moves close to the processors that uses them</li><li>No hierarchy like with caches</li><li>Experimental</li></ul><h1 id=distributed-shared-memory>Distributed shared-memory</h1><ul><li>Distributed system – cluster</li><li>Local memory on each node</li><li>Remote memory on other nodes</li><li>Hardware solution</li><li>Usually based on virtual memory principles</li><li>transparent</li><li>Software solution</li></ul><h1 id=cache-memory-coherence>Cache Memory Coherence</h1><p>Reasons for cache miss:</p><ul><li>Compulsory miss: 1st access to data</li><li>Capacity miss: insufficient capacity</li><li>Conflict miss: different memory areas mapped to the same cache row</li><li>Coherence miss: different data in different caches</li></ul><p>Multiprocessors exposed to the last case:</p><ul><li>But it can happen in a single processor case, too</li></ul><h1 id=invalidation>Invalidation</h1><ul><li>Reaction on content change in remote (cache) memory</li><li>Row in the actual (“snooping”) cache memory invalidated</li><li>If the same row is needed later, it is retrieved from the memory (again)</li></ul><h1 id=update>Update</h1><ul><li>The cache memory row is updated immediately</li><li>If data are needed (again), they are already in the cache</li><li>Drawbacks: False sharing/High load on the bus</li><li>Invalidation and Update are equivalent performance-wise</li><li>Unless a specific memory access pattern is used</li></ul><h1 id=coherence-cache-miss-solutions>Coherence Cache Miss solutions</h1><ul><li>Cache memory must be aware of a change elsewhere</li><li>Broadcast based protocols</li><li>Directory based protocols</li></ul><h1 id=snoopy-cache>Snoopy cache</h1><ul><li>Broadcast based protocol</li><li>Each processor follows/watches all memory accesses</li></ul><h1 id=directory-based-protocols>Directory based protocols</h1><ul><li>Snoopy protocol based on broadcast</li><li>Not scalable and not usable for more complex interconnect networks</li><li>Solution: Reduction of actively “touched” caches – Directories</li><li>Tag at each memory block</li><li>Cache memory with a copy of such a block explicitly references the tag</li><li>Special exclusivity tag (writing)</li><li>Three based schemas:</li><li>Fully mapped directories</li><li>Limited directories (partially mapped)</li><li>Chained directories</li><li>We will compare them based on the following features</li><li>Size of the additional memory needed</li><li>Number of necessary instructions/steps (latency introduced)</li></ul><h1 id=fully-mapped-directories>Fully mapped directories</h1><ul><li>Each memory block is able to directly reference all caches (processors) simultaneously</li><li>Bit vector of copies: If a bit is set, the corresponding cache keeps a copy of the data block</li><li>Exclusivity tag</li><li>One per a block</li><li>Writing can be performed on one processor (one cache) only</li><li>Additional tags for each block in each cache</li><li>Validity tag</li><li>Exclusivity tag</li></ul><h1 id=limited-directories>Limited directories</h1><ul><li>Full directories rather expensive (long bit vectors)</li><li>Additional memory needed: <code>P^2 * M / B</code></li><li>P number of of cache memories (processors)</li><li>M size of the main memory</li><li>B block size</li><li>Cumulative capacity of all caches usually smaller that the size of the main memory</li><li>Data block are usually not extensively shared: Most directory entries contain zeros</li><li>Solution: Use of direct references; However, one bit per cache is not enough</li><li>Set of pointers to the caches: Dynamic allocation as needed</li><li>Features:</li><li>Number of bits per pointer: log_2 P</li><li>Number of pointers in the pointer pool: k</li><li>More memory efficient than directly mapped if <code>k &lt; P/(log_2 P)</code></li><li>Invalidation information sent only to caches keeping the copy of changed data</li></ul><h1 id=overflow>Overflow</h1><ul><li>If the pointer’s pool is exhausted: Too many shared blocks</li><li>Possible reactions:</li><li>Invalidation of all shared blocks</li><li>One entry selection (even random) and invalidation</li></ul><h1 id=other-modifications>Other modifications</h1><ul><li>Coarse-vector (Dir_i CV_r)</li><li>Switch of interpretation (whether a single processor or a region) as a result of overflow</li></ul><h1 id=chained-protocols>Chained protocols</h1><ul><li>Cache-Based linked-list</li><li>Only one pointer per memory block</li><li>Other pointers part of the cache memories: The cache memories (their blocks) are “chained”</li><li>Advantages: Memory footprint minimization</li><li>Drawbacks: Complex protocol, More communication, Write has higher latency</li></ul><h1 id=hierarchical-directories>Hierarchical directories</h1><ul><li>Usable in systems with multiple buses</li><li>Hierarchy of caches</li><li>Higher hierarchy at each bus interconnect</li><li>Higher memory requirements</li><li>In principle a hierarchy of snoopy protocols with special extensions</li></ul><h1 id=scalability>Scalability</h1><ul><li>No single definition</li><li>Most used definition: Scalable is such a system for which the following holds:</li><li>Performance grows linearly with price</li><li>The ratio Price/Performance is fixed</li><li>Both are equivalent, but usually only one condition is explicitly used</li><li>Alternative parameter – <strong>Scalability Extent</strong></li><li>Performance change as a result of a processor addition</li><li>Price change as a result of a processor addition</li><li>Rational extent of number of processors considered</li></ul><h1 id=speedup--commentary>Speedup – commentary</h1><p>//TODO: formula</p><ul><li>Theoretical feature, reality depends on the application</li><li>Different values for different applications (on the same system)</li><li>Amdahl law – extent of possible parallelization</li><li>Parallelizable and serial part of the task</li><li>Parallelizability has its limits</li></ul><h1 id=extensible-interconnecting-networks>Extensible interconnecting networks</h1><ul><li>Parallel systems must include interconnecting network</li><li>It influences behavior of the parallel system</li><li>Ideal extensible network</li><li>Low cost growing linearly with the number of processors (N)</li><li>Minimal latency independent of N</li><li>Throughput grows linearly with N</li></ul><h1 id=network-properties>Network properties</h1><p>Three basic components:</p><ul><li>topology</li><li>switching</li><li>routing</li></ul><h1 id=interconnecting-networks>Interconnecting networks</h1><p>We distinguish the following parameters:</p><ul><li>Network size – number of nodes N</li><li>Node degree d (number of edges from a node)</li><li>Network radius D: Longest shortest path</li><li>Bisection width B</li><li>Network redundancy A</li><li>Minimal number of links that must be removed for the network to become split into two disconnected parts</li><li>Cost C: Number of links in the network</li></ul><h1 id=bisection-width>Bisection width</h1><p>Bisection width: Minimal number of links that must be removed for the network to split into two same size parts
Bisection bandwidth: Commutative throughput of all removed links
Ideal properties: Bisection bandwidth per process is constant</p><h1 id=topology-of-interconnecting-networks>Topology of interconnecting networks</h1><p>Classification based on the number of dimensions:</p><ul><li>One-dimensional</li><li>Two-dimensional, planar</li><li>Three-dimensional, cubic</li><li>Hypercube, tesseract (higher dimensions)</li></ul><h1 id=one-dimensional-interconnects>One-dimensional interconnects</h1><ul><li>Linear array</li><li>Individual nodes serially connected: “(string of) beads”</li><li>Simplest</li><li>Worst properties (for N > 2)</li></ul><h1 id=two-dimensional-interconnects>Two-dimensional interconnects</h1><ul><li>ring: closed linear array</li><li>star</li><li>tree:</li><li>Decreases network radius</li><li>Still bad redundancy and bisection (band)width</li><li>Fat tree: Adds redundant links at higher levels, Improves bisection bandwidth</li></ul><h1 id=two-dimensional-mesh>Two-dimensional mesh</h1><p>//TODO: formulas</p><ul><li>Very popular</li><li>Good properties</li><li>However a higher cost and variable node degree</li><li>torus</li><li>closed two-dimensional mesh</li></ul><h1 id=three-dimensional-mesh>Three-dimensional mesh</h1><p>Difficult for construction</p><h1 id=hypercube-tesseract>Hypercube, tesseract</h1><ul><li>In general n-dimensional cube</li><li>Meshes are special cases of hypercube (lower dimensionality)</li><li>Routing very simple: Based on binary numbering of nodes</li></ul><h1 id=fully-connected-network>Fully connected network</h1><ul><li>More as a theoretical construct</li><li>Excellent radius (1)</li><li>Unacceptable cost (N ∗ (N − 1)/2) and node degree (N − 1)</li></ul><h1 id=switching>Switching</h1><ul><li>Specific mechanism how the packet move from input to output (on a transit node)</li><li>Basic approaches</li><li>Packet switching, store-and-forward</li><li>Circuit switching</li><li>Virtual connection (cut-through)</li><li>Wormhole routing</li></ul><h2 id=store-and-forward>Store-and-forward</h2><ul><li>The whole packet is stored on the transit node</li><li>And is send out only after the full packet is received</li><li>Simple (first generation of parallel computers)</li><li>High latency P/B ∗ D</li><li>P is packet size, B is throughput and D is number of “hops” distance</li></ul><h2 id=circuit-switching>Circuit switching</h2><ul><li>Three stages</li><li>Connection setup – initiated by a probe</li><li>Data transmission</li><li>Connection tearing</li><li>Visibly lower latency P/B * D + M/B</li><li>P is the size of the probe and M is a size of the message (packets are not needed nor relevant)</li><li>For <code>P &lt;&lt; M</code> latency is independent of the path size</li></ul><h1 id=virtual-connection>Virtual connection</h1><ul><li>Message is split into smaller blocks – flow control digits/units (flits)</li><li>The first flit contains the path info (initially the target address)</li><li>Next flits contain the data (payload)</li><li>Last flit breaks the path</li><li>Individual flits are sent as a continuous stream</li><li>With buffers of sufficient size, this responds to the circuit switching</li><li>Latency (HF)/B * D + M/B</li><li>HF is flit length, usually <code>HF &lt;&lt; M</code></li></ul><h1 id=wormhole-routing>Wormhole routing</h1><ul><li>Special case of virtual circuit</li><li>Buffer size exactly fits the flit size</li><li>Latency does not depend on the distance (the length of the path)</li><li>Pipeline analogue</li><li>The whole packet resides in several buffers at different nodes on the path – thus the wormhole</li><li>Latency is considered at the level of the whole message transfer, not per flits; number of flits is much higher than the distance (the length of the path = number of hops)</li><li>transmission time is a sum of the length of the path and number of the transferred flits</li><li>while for store-and-forward it relates to the product of these two values
The protocol supports packet replication: convenient for multicast and broadcast</li></ul><h1 id=virtual-channels>Virtual channels</h1><ul><li>Physical channels sharing</li><li>Several buffers over the same channel: Flits stored in the appropriate buffer</li><li>Use:</li><li>Overloaded links</li><li>Deadlock avoidance</li><li>Logical to physical topology mapping</li><li>Guarantees of sufficient transport capacity for system data</li></ul><h1 id=routing-in-the-interconnecting-networks>Routing in the interconnecting networks</h1><ul><li>Path discovery</li><li>Properties:</li><li>Static routing: Source based, distributed</li><li>Adaptive routing (always ditributed)</li><li>Minimal but also non-minimal routes</li></ul><h1 id=fault-tolerance-of-interconnecting-networks>Fault tolerance of interconnecting networks</h1><ul><li>Error check</li><li>Message acknowledgment</li><li>Message re-transmission</li></ul><h1 id=memory-latency>Memory latency</h1><ul><li>Memory considerably slower than the processor</li><li>Waiting for memory substantially decreases efficiency of the whole system</li><li>Solutions:</li><li>Latency decrease – access speedup</li><li>Latency hiding – interleaving of computing and data transmission</li></ul><h2 id=memory-latency-decrease>Memory Latency Decrease</h2><p>NUMA: Non-Uniform Memory Access</p><ul><li>Each logical address is mapped to a concrete physical address
COMA: Cache-Only Memory Architecture</li><li>Main memory considered as an attraction memory</li><li>Memory rows can be moved freely</li><li>A memory row can have several copies</li></ul><h1 id=memory-latency-hiding>Memory Latency Hiding</h1><ul><li>Weak consistency models</li><li>Prefetch</li><li>Multiple-context processors</li><li>Producer initiated communication</li></ul><h1 id=weak-consistency>Weak consistency</h1><ul><li>Does not require a strict synchronization access to shared variables unless explicitly required (explicit synchronization)</li><li>Release consistency: New instructions acquire and release</li><li>Fence instruction: Enforced finalization of unfinished instructions (waiting)</li></ul><h1 id=prefetch>Prefetch</h1><ul><li>Data moved to the process in advance</li><li>Binding prefetch</li><li>Data moved to the processor (register</li><li>Risk of consistency break</li><li>Non-binding prefetch</li><li>Data moved to the cache only</li><li>HW Prefetch</li><li>SW Prefetch</li><li>Special instruction prefetch-exclusive: read followed by a write</li><li>Remember ANDES?</li></ul><h1 id=multiple-context-processors>Multiple-context processors</h1><ul><li>Multithreading support</li><li>Requires:</li><li>Very fast context switch (1-2 ticks)</li><li>Very high number of registers (the full set per each thread)</li><li>Many experimental systems</li><li>HEP (seventies)</li><li>TERA</li><li>*T</li></ul><h1 id=producer-initiated-communication>Producer initiated communication</h1><ul><li>Analogy of invalidate and update in cache coherency</li><li>Specific use for message passing systems or block-copy (computers with shared memory)</li><li>Suitable for transfer of large blocks or for lock-based synchronization</li></ul><h1 id=synchronization-support>Synchronization support</h1><ul><li>Synchronization leads to “hot spots”</li><li>Basic synchronization primitives/protocols:</li><li>Mutual exclusion (mutex)</li><li>Dynamic load distribution</li><li>Events’ propagation</li><li>Global serialization (barriers)</li></ul><h1 id=mutual-exclusion-mutex>Mutual exclusion (mutex)</h1><ul><li>Access to a shared variable is granted to at most one process at any given time</li><li>Universal, but usually rather expensive</li><li>Synchronization constructs of higher programming languages (semaphores, monitors, critical sections)</li><li>Foundation – hardware support</li><li>test&set instruction</li><li>test-and-test&set instruction</li><li>Spin waiting protocol</li></ul><h1 id=testset>test&set</h1><ul><li>Properties:</li><li>An atomic instruction that reads the actual content and sets the content to 1 (CLOSED)</li><li>Busy (active) waiting till the read value is 0</li><li>Highly stressing cache coherent multiprocessor systems</li><li>Each “set” flushes all caches</li></ul><h1 id=test-and-testset>test-and-test&set</h1><ul><li>An instruction that reads the actual value and run the atomic test&set only if the read value is 0</li><li>Protocol responsive to the cache subsystem properties</li><li>First test over the shared (in cache) copy</li></ul><h1 id=use-of-queues>Use of queues</h1><ul><li>Improvement: Collision avoidance schemes</li><li>Queue on lock bit (QOLB) protocol</li><li>The most effective implementation</li><li>Processes lined up in a queue</li><li>After lock release the process at the queue head is activated</li><li>No data transfer among all waiting processes needed</li></ul><h1 id=locks-in-multiprocessors>Locks in multiprocessors</h1><ul><li>Related to the dynamic load distribution</li><li>Use of counter with atomic operation</li></ul><h1 id=events-propagation>Event’s propagation</h1><p>Global events/signals used as a tool:</p><ul><li>for producers to inform consumers that data are ready</li><li>to inform about a global change in a set of equivalent processes</li><li>Status change that must be announced to all processes</li></ul><h1 id=barriers>Barriers</h1><ul><li>A point of synchronization</li><li>To let all processes synchronize at the same point</li><li>No process can pass the barrier unless all processes reached it</li></ul></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.77414ca1a0d130043c129098d97cecf433ce369d23de8eaa91f5111f432729db1257c49a33b38203d4be241ef53dafecd99a1d2c350b75316b55a0bb6a2e150b.js integrity="sha512-d0FMoaDRMAQ8EpCY2Xzs9DPONp0j3o6qkfURH0MnKdsSV8SaM7OCA9S+JB71Pa/s2ZodLDULdTFrVaC7ai4VCw=="></script></body></html>