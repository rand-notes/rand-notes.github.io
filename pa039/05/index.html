<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="Parallel programming Data parallelism Identical instructions on different processors process different data In principle the SIMD model (Single Instruction Multiple Data) - e.g. loop parallelization Task parallelism MIMD model (Multiple Instruction Multiple Data) Independent blocks (functions, procedures, programs) run in parallel SPMD No synchronization at the level of individual instructions Equivalent to MIMD Message passing targets SPMD/MIMD Before MPI Many competing message passing libraries: Vendor specific/proprietary libraries Academic, narrow specific implementations Different communication models Difficult application development Need for “own” communication model to encapsulate the specific models MPI an attempt to define a standard set of communication calls Message Passing Interface Communication interface for parallel programs Defined through API Standardized Several independent implementations Programming model MPI designed originally for distributed memory architectures Currently supports hybrid models MPI Design Goals Portability Define standard APIs Define bindings for different languages Independent implementations Performance Independent hardware specific optimization Libraries, potential for changes in algorithms Functionality Goal to cover all aspects of inter-processor communication Library for message passing Designed for use on parallel computers, clusters and even Grids Make parallel hardware available for users library authors tools and app developers MPI Initialization Create an environment Specify that the program will use the MPI libraries No explicit work with processes Identity Any parallel (distributed) program needs to know How many processes are participating on the computation Identity of “own” process MPI Comm size and MPI Comm rank Work with messages Naive/primitive model Process A sends a message: operation send Process B receives a message: operation receive Lot of questions How to properly specify (define) the data?"><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pa039/05/><title>notes 5 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="notes 5"><meta itemprop=description content="Parallel programming Data parallelism Identical instructions on different processors process different data In principle the SIMD model (Single Instruction Multiple Data) - e.g. loop parallelization Task parallelism MIMD model (Multiple Instruction Multiple Data) Independent blocks (functions, procedures, programs) run in parallel SPMD No synchronization at the level of individual instructions Equivalent to MIMD Message passing targets SPMD/MIMD Before MPI Many competing message passing libraries: Vendor specific/proprietary libraries Academic, narrow specific implementations Different communication models Difficult application development Need for “own” communication model to encapsulate the specific models MPI an attempt to define a standard set of communication calls Message Passing Interface Communication interface for parallel programs Defined through API Standardized Several independent implementations Programming model MPI designed originally for distributed memory architectures Currently supports hybrid models MPI Design Goals Portability Define standard APIs Define bindings for different languages Independent implementations Performance Independent hardware specific optimization Libraries, potential for changes in algorithms Functionality Goal to cover all aspects of inter-processor communication Library for message passing Designed for use on parallel computers, clusters and even Grids Make parallel hardware available for users library authors tools and app developers MPI Initialization Create an environment Specify that the program will use the MPI libraries No explicit work with processes Identity Any parallel (distributed) program needs to know How many processes are participating on the computation Identity of “own” process MPI Comm size and MPI Comm rank Work with messages Naive/primitive model Process A sends a message: operation send Process B receives a message: operation receive Lot of questions How to properly specify (define) the data?"><meta itemprop=datePublished content="2022-05-05T00:00:00+00:00"><meta itemprop=dateModified content="2022-05-05T00:00:00+00:00"><meta itemprop=wordCount content="1100"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="notes 5"><meta name=twitter:description content="Parallel programming Data parallelism Identical instructions on different processors process different data In principle the SIMD model (Single Instruction Multiple Data) - e.g. loop parallelization Task parallelism MIMD model (Multiple Instruction Multiple Data) Independent blocks (functions, procedures, programs) run in parallel SPMD No synchronization at the level of individual instructions Equivalent to MIMD Message passing targets SPMD/MIMD Before MPI Many competing message passing libraries: Vendor specific/proprietary libraries Academic, narrow specific implementations Different communication models Difficult application development Need for “own” communication model to encapsulate the specific models MPI an attempt to define a standard set of communication calls Message Passing Interface Communication interface for parallel programs Defined through API Standardized Several independent implementations Programming model MPI designed originally for distributed memory architectures Currently supports hybrid models MPI Design Goals Portability Define standard APIs Define bindings for different languages Independent implementations Performance Independent hardware specific optimization Libraries, potential for changes in algorithms Functionality Goal to cover all aspects of inter-processor communication Library for message passing Designed for use on parallel computers, clusters and even Grids Make parallel hardware available for users library authors tools and app developers MPI Initialization Create an environment Specify that the program will use the MPI libraries No explicit work with processes Identity Any parallel (distributed) program needs to know How many processes are participating on the computation Identity of “own” process MPI Comm size and MPI Comm rank Work with messages Naive/primitive model Process A sends a message: operation send Process B receives a message: operation receive Lot of questions How to properly specify (define) the data?"><meta property="og:title" content="notes 5"><meta property="og:description" content="Parallel programming Data parallelism Identical instructions on different processors process different data In principle the SIMD model (Single Instruction Multiple Data) - e.g. loop parallelization Task parallelism MIMD model (Multiple Instruction Multiple Data) Independent blocks (functions, procedures, programs) run in parallel SPMD No synchronization at the level of individual instructions Equivalent to MIMD Message passing targets SPMD/MIMD Before MPI Many competing message passing libraries: Vendor specific/proprietary libraries Academic, narrow specific implementations Different communication models Difficult application development Need for “own” communication model to encapsulate the specific models MPI an attempt to define a standard set of communication calls Message Passing Interface Communication interface for parallel programs Defined through API Standardized Several independent implementations Programming model MPI designed originally for distributed memory architectures Currently supports hybrid models MPI Design Goals Portability Define standard APIs Define bindings for different languages Independent implementations Performance Independent hardware specific optimization Libraries, potential for changes in algorithms Functionality Goal to cover all aspects of inter-processor communication Library for message passing Designed for use on parallel computers, clusters and even Grids Make parallel hardware available for users library authors tools and app developers MPI Initialization Create an environment Specify that the program will use the MPI libraries No explicit work with processes Identity Any parallel (distributed) program needs to know How many processes are participating on the computation Identity of “own” process MPI Comm size and MPI Comm rank Work with messages Naive/primitive model Process A sends a message: operation send Process B receives a message: operation receive Lot of questions How to properly specify (define) the data?"><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pa039/05/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="article:published_time" content="2022-05-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-05T00:00:00+00:00"><meta property="og:site_name" content="idk"><meta property="article:published_time" content="2022-05-05 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pa039/05/>notes 5</a></h2><div class=post-content><h1 id=parallel-programming>Parallel programming</h1><ul><li>Data parallelism</li><li>Identical instructions on different processors process different data</li><li>In principle the SIMD model (Single Instruction Multiple Data) - e.g. loop parallelization</li><li>Task parallelism</li><li>MIMD model (Multiple Instruction Multiple Data)</li><li>Independent blocks (functions, procedures, programs) run in parallel</li><li>SPMD</li><li>No synchronization at the level of individual instructions</li><li>Equivalent to MIMD</li><li>Message passing targets SPMD/MIMD</li></ul><h1 id=before-mpi>Before MPI</h1><ul><li>Many competing message passing libraries:</li><li>Vendor specific/proprietary libraries</li><li>Academic, narrow specific implementations</li><li>Different communication models</li><li>Difficult application development</li><li>Need for “own” communication model to encapsulate the specific models</li><li>MPI an attempt to define a standard set of communication calls</li></ul><h1 id=message-passing-interface>Message Passing Interface</h1><ul><li>Communication interface for parallel programs</li><li>Defined through API</li><li>Standardized</li><li>Several independent implementations</li></ul><h1 id=programming-model>Programming model</h1><ul><li>MPI designed originally for distributed memory architectures</li><li>Currently supports hybrid models</li></ul><h1 id=mpi-design-goals>MPI Design Goals</h1><ul><li>Portability</li><li>Define standard APIs</li><li>Define bindings for different languages</li><li>Independent implementations</li><li>Performance</li><li>Independent hardware specific optimization</li><li>Libraries, potential for changes in algorithms</li><li>Functionality</li><li>Goal to cover all aspects of inter-processor communication</li><li>Library for message passing</li><li>Designed for use on parallel computers, clusters and even Grids</li><li>Make parallel hardware available for</li><li>users</li><li>library authors</li><li>tools and app developers</li></ul><h1 id=mpi-initialization>MPI Initialization</h1><ul><li>Create an environment</li><li>Specify that the program will use the MPI libraries</li><li>No explicit work with processes</li></ul><h1 id=identity>Identity</h1><ul><li>Any parallel (distributed) program needs to know</li><li>How many processes are participating on the computation</li><li>Identity of “own” process</li><li>MPI Comm size and MPI Comm rank</li></ul><h1 id=work-with-messages>Work with messages</h1><ul><li>Naive/primitive model</li><li>Process A sends a message: operation send</li><li>Process B receives a message: operation receive</li><li>Lot of questions</li><li>How to properly specify (define) the data?</li><li>How to specify (identify) process B (the receiver)?</li><li>How the receiver recognises that the data are for it?</li><li>How a successful completion is recognised?</li></ul><h1 id=classical-approach>Classical approach</h1><ul><li>We send data as a byte stream</li><li>It is left to sender and receiver to properly setup and recognize data</li><li>Each process has a unique identifier</li><li>We have to know identity of sender and receiver</li><li>Broadcast operation</li><li>We can specify some tag for the better recognitione.g. the message sequence number)</li><li>Synchronization</li><li>Explicit collaboration between a sender and a receiver</li><li>It defines order of messages</li><li>send and recv</li></ul><h1 id=deficiencies-of-the-classical-approach>Deficiencies of the classical approach</h1><ul><li>Insufficient level of data specification/definition</li><li>Heterogeneity between sender and receiver (incompatible representation)</li><li>Too many copies</li><li>Too much relies on a programmer</li><li>Tags are global</li><li>Complication when you want to write independent libraries</li><li>Collective operations</li><li>too many send/receive operations + not optimized, inefficient</li></ul><h1 id=mpi-extensions>MPI extensions</h1><ul><li>Processes are grouped</li><li>Each message is defined within a specific context (not only a tag)</li><li>Messages could be sent and received only within the same context</li><li>Group and context jointly define communicator</li><li>Tag is local to a specific communicator</li><li>Default communicator MPI COMM WORLD</li><li>Process identity (rank) is always defined within a specific context</li></ul><h1 id=data-types>Data types</h1><ul><li>Data are described by a triple (address, number, datatype)</li><li>MPI Datatype is recursively defined as:</li><li>Pre-defined data type of the used language</li><li>Continuous array of MPI datatypes</li><li>Strided array of MPI datatypes</li><li>Indexed array of datatype blocks</li><li>Arbitrary datatype structure</li><li>MPI provides functions to define own datatypes (row of a matrix which is stored column-wise)</li></ul><h1 id=tags>Tags</h1><ul><li>Each message has an associated tag</li><li>Simplifies message recognition by the receiver</li><li>Tag is always defined within the used context (it is scoped)</li><li>Receiver could specify which tag it expects</li><li>Alternatively it could ignore the tags (through MPI ANY TAG specification)</li></ul><h1 id=point-to-point-communication>Point-to-point Communication</h1><ul><li>Passing of a message between two processes</li><li>Blocking / Non-blocking call (transmission)</li><li>Blocking – the call waits till the operation is finished</li><li>Non-blocking – the call just initiates the operation but does not wait till completion; the state of the data transfer must be tested independently</li><li>Buffered / Un-buffered message passing</li><li>No buffer – message is passed directly without a buffer</li><li>MPI buffer – “transparent”, controlled directly by MPI</li><li>User buffer – controlled by the application (programmer)</li></ul><h1 id=communication-modes>Communication modes</h1><ul><li>Standard mode (Send)</li><li>Blocking call</li><li>MPI “decides”, if the MPI buffer is used</li><li>used → Send finishes when all data are in the buffer</li><li>not used → Send finishes when the data are accepted by the receiver</li><li>Synchronous mode</li><li>Blocking call</li><li>Send finishes when the data were accepted by the receiver (processes synchronization)</li><li>Buffered mode</li><li>Buffer provided by the application(programmer)</li><li>Blocking or non-blocking – the operation finishes when the data are in the user buffer</li><li>Ready mode</li><li>Receive must precede the actual send (Receive prepares the buffer), otherwise error</li></ul><h1 id=basic-send-operation>Basic send operation</h1><ul><li>Blocking send</li><li>MPI SEND</li><li>Triple (start, count, datatype) defines the message</li><li>dest identifies the receiver process, always relative to the used communicator comm</li><li>Finishing the operation successfully means</li><li>All data were accepted by the system</li><li>The buffer is available for re-use</li><li>The receiver may not yet receive the data</li></ul><h1 id=basic-receive-operation>Basic receive operation</h1><ul><li>Blocking operation</li><li>MPI RECV(start, count, datatype, source, tag, comm, status)</li><li>The operation waits till a message with a corresponding tuple (source, tag) is not received</li><li>source identifies the sending process, relative to the used communicator comm) or MPI ANY SOURCE</li><li>status contains info about the result of the operation</li><li>Reception of more than count block is an error</li></ul><h1 id=asynchronous-communications>Asynchronous communications</h1><ul><li>Non-blocking send operation</li><li>Buffer can be re-used only after the completion of the whole transfer</li><li>The send and receive operations create a request</li><li>Afterwards it is possible to check the status of the request</li></ul><h1 id=persistent-communication-channels>Persistent Communication Channels</h1><ul><li>Non-blocking</li><li>Created by combining two “half”-channels</li><li>Life cycle</li><li>Create (Start Complete)* Free</li><li>Creation, followed by repetitive use, destroyed afterwards</li><li>Channel destruction equivalent to the destruction of the corresponding request</li></ul><h1 id=collective-operations>Collective operations</h1><ul><li>Operation performed by all processes within a group</li><li>Broadcast: MPI BCAST</li><li>One process (root) will send data to all other processes</li><li>Reduction: MPI REDUCE</li><li>Joins data from all processes in a group (communicator) and makes it available (as an array) to the calling process</li><li>Often a group of send/receive operations can be replaced by a single bcast/reduce operation</li><li>Higher efficiency/performance: bcast/reduce optimized for a particular hardware</li><li>Other operations</li><li>alltoall: exchange of messages among all processes in a group</li><li>Special reduction: min, max, sum, or User defined additional collective operations</li></ul><h1 id=virtual-topology>Virtual topology</h1><ul><li>MPI can define communication patterns that directly corresponds to the application needs</li><li>These are (in a next step) mapped to the actual hardware ad its communication operations</li><li>Transparent</li><li>Higher efficiency when writing programs</li><li>Portability</li><li>Program is not directly associated with a concrete topology of used hardware</li><li>Potential for independent optimizations</li></ul><h1 id=datatype-constructions>Datatype constructions</h1><ul><li>Strided data types</li><li>They can include “holes</li><li>Implementation may optimize some datatypes</li><li>Example: every second element of a vector</li></ul><h1 id=operations-over-files>Operations over files</h1><ul><li>Support since MPI-2</li><li>File “parallelization”</li></ul><h1 id=mpi-and-optimizing-compilers>MPI and optimizing compilers</h1><ul><li>Asynchronous use of memory can lead to data changes (within arrays) that a complier knows nothing about</li></ul></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.e8a56c89d5ca304d6922b22e38d0af2db97fa70a6623ba17a492092da773dfa4b9aaffa0f682ecfd03d7c7964e89bdbef18f0d4183c698831978c7ca44959d10.js integrity="sha512-6KVsidXKME1pIrIuONCvLbl/pwpmI7oXpJIJLadz36S5qv+g9oLs/QPXx5ZOib2+8Y8NQYPGmIMZeMfKRJWdEA=="></script></body></html>