<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="CISC Size and speed of memory CISC directly supports compilers Rich addressing modes Complex instruction == microprogram Microinstructions: decomposition to simpler instructions
Disadvantages:
too complex instructions increasingly complex instruction analysis cross instruction relationships backward compatibility cost (within a family) Performance increase Clock cycles define processor’s performance Solution: parallelization
Pipelining five-stage pipelining:
Instruction Fetch instruction is loaded from a memory Instruction Decode instruction is decoded (recognized) Operand Fetch operands are ready (fetched from registers and/or memory) Execute instruction is executed Writeback results are written back Individual stages are processed in parallel, shifted by one stage"><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pa039/01/><title>notes 01 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="notes 01"><meta itemprop=description content="CISC Size and speed of memory CISC directly supports compilers Rich addressing modes Complex instruction == microprogram Microinstructions: decomposition to simpler instructions
Disadvantages:
too complex instructions increasingly complex instruction analysis cross instruction relationships backward compatibility cost (within a family) Performance increase Clock cycles define processor’s performance Solution: parallelization
Pipelining five-stage pipelining:
Instruction Fetch instruction is loaded from a memory Instruction Decode instruction is decoded (recognized) Operand Fetch operands are ready (fetched from registers and/or memory) Execute instruction is executed Writeback results are written back Individual stages are processed in parallel, shifted by one stage"><meta itemprop=datePublished content="2022-05-05T00:00:00+00:00"><meta itemprop=dateModified content="2022-05-05T00:00:00+00:00"><meta itemprop=wordCount content="940"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="notes 01"><meta name=twitter:description content="CISC Size and speed of memory CISC directly supports compilers Rich addressing modes Complex instruction == microprogram Microinstructions: decomposition to simpler instructions
Disadvantages:
too complex instructions increasingly complex instruction analysis cross instruction relationships backward compatibility cost (within a family) Performance increase Clock cycles define processor’s performance Solution: parallelization
Pipelining five-stage pipelining:
Instruction Fetch instruction is loaded from a memory Instruction Decode instruction is decoded (recognized) Operand Fetch operands are ready (fetched from registers and/or memory) Execute instruction is executed Writeback results are written back Individual stages are processed in parallel, shifted by one stage"><meta property="og:title" content="notes 01"><meta property="og:description" content="CISC Size and speed of memory CISC directly supports compilers Rich addressing modes Complex instruction == microprogram Microinstructions: decomposition to simpler instructions
Disadvantages:
too complex instructions increasingly complex instruction analysis cross instruction relationships backward compatibility cost (within a family) Performance increase Clock cycles define processor’s performance Solution: parallelization
Pipelining five-stage pipelining:
Instruction Fetch instruction is loaded from a memory Instruction Decode instruction is decoded (recognized) Operand Fetch operands are ready (fetched from registers and/or memory) Execute instruction is executed Writeback results are written back Individual stages are processed in parallel, shifted by one stage"><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pa039/01/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="article:published_time" content="2022-05-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-05T00:00:00+00:00"><meta property="og:site_name" content="idk"><meta property="article:published_time" content="2022-05-05 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pa039/01/>notes 01</a></h2><div class=post-content><h1 id=cisc>CISC</h1><p>Size and speed of memory
CISC directly supports compilers
Rich addressing modes
Complex instruction == microprogram
Microinstructions: decomposition to simpler instructions</p><p>Disadvantages:</p><ul><li>too complex instructions</li><li>increasingly complex</li><li>instruction analysis</li><li>cross instruction relationships</li><li>backward compatibility cost (within a family)</li></ul><h1 id=performance-increase>Performance increase</h1><p>Clock cycles define processor’s performance
Solution: parallelization</p><h1 id=pipelining>Pipelining</h1><p>five-stage pipelining:</p><ul><li>Instruction Fetch instruction is loaded from a memory</li><li>Instruction Decode instruction is decoded (recognized)</li><li>Operand Fetch operands are ready (fetched from registers and/or memory)</li><li>Execute instruction is executed</li><li>Writeback results are written back</li></ul><p>Individual stages are processed in parallel, shifted by one stage</p><h1 id=pipelines-and-memory>Pipelines and memory</h1><p>“Invisible” pipelines: Reading (writing) from (to) memory is moved ahead of the actual instruction that works with the data</p><p>“Visible” pipelines: Explicit instructions, with know number of cycles to complete e.g.g intel 80860</p><h1 id=risc>RISC</h1><p>Architecture removed the speed of memory access bottleneck</p><ul><li>Introduction of caches</li><li>Dramatic decrease in the memory cost paralleling increase of memory size</li><li>Better pipelining</li><li>Improved compilers</li><li>All instructions of the same size/length</li><li>Simple addressing</li><li>Load/Store architecture</li><li>Sufficient number of internal registers</li><li>“Delayed’ branches</li></ul><p>Problem: stall when waiting for a next instruction execution finalization (too complex relationship between instructions, microcode, &mldr;)</p><p>First generation RISC ideal: One instruction finished per each clock tick</p><p>Reality nowadays: Several instructions graduated in a single clock tick</p><h1 id=superscalar-processors>Superscalar processors</h1><ul><li>Multiple processing units: Arithmetic (ALU), Floating point (FPU) and other</li><li>Parallelism in a hardware: Sequential programs, “Automatic” parallelization, MADD (Multiply Add)</li></ul><h1 id=superpipeline>Superpipeline</h1><ul><li>Another circuits simplification</li><li>More extensive pipeline decomposition</li><li>Faster execution of individual stages</li><li>resulting in faster processing</li><li>A different form of parallelism</li><li>These pipelines also called deep pipelines</li><li>16 and more stages</li><li>instructions use only some of the whole set of stages</li></ul><h1 id=vliw>VLIW</h1><ul><li>Analogy of superscalar processors (many units)</li><li>Parallelization under compiler control</li><li>Increased complexity of compilers</li><li>Simplified hardware leads to higher performance</li><li>Decision which instructions can be run in parallel taken by the compiler</li><li>Advantages:</li><li>Simpler instructions</li><li>No complex control hardware needed</li><li>Lower energy consumption (at least a potential for it)</li></ul><h1 id=risc--additional-features>RISC – additional features</h1><ul><li>Register’s bypass</li><li>Register’s renaming</li><li>Branches</li><li>null operation</li><li>conditional assignment <code>(a = b&lt;c ? d : e;)</code></li><li>multiple pre-fetch from memory</li><li>buffer of potential branch targets</li><li>branch prediction: static (compiled) or dynamic</li></ul><h1 id=andes>ANDES</h1><p>Architecture with Non-sequential Dynamic Execution Scheduling</p><p>Also called out-of-order execution (OoOE) or dynamic execution</p><p>Foundations:</p><ul><li>Waiting for data causes a slowdown</li><li>Dynamic parallelism can help</li></ul><p>Multiple instructions queues:</p><ul><li>a queue for fixed point (arithmetic and logic) instructions</li><li>an address queue for Load/Store instructions</li><li>a queue for floating point instructions</li></ul><p>Independent pipeline for each queue</p><p>Features:</p><ul><li>readiness decides which instructions are executed</li><li>the original order of instructions in the program is not kept</li><li>instruction graduation guarantees restoration of the original order</li></ul><h2 id=additional-properties>Additional properties</h2><ul><li>Speculative branches</li><li>execution continues through predicted branch</li><li>does not wait for the result of the actual branch instruction</li><li>Non-blocking Load/Store instructions</li><li>Register renaming</li><li>Just part revealed (to a complier)</li><li>Multiple versions of the “same” register</li><li>Allows new data to be taken to a register “blocked” by a not yet finished execution of another instruction</li></ul><h2 id=summary>Summary</h2><ul><li>Instead of waiting for memory access, other instructions are executed</li><li>Internally changes the program order of instructions to the data order</li><li>instructions are executed when their operands are ready (in registers)</li><li>the original program order is not relevant</li></ul><p>Avoids (or at least reduces) stalling of execution</p><p>Speculates on branches or even takes both branches in parallel (Only one branch graduates)</p><p>Complex circuitry, higher power consumptions</p><h1 id=memory>Memory</h1><p>Memory organization:</p><ul><li>rows and columns (a 2D matrix)</li><li>address has two parts</li><li>page mode – a block or continuous bytes (the “row”) is read in one shot</li></ul><h2 id=memory-features>Memory Features</h2><ul><li>Memory access time - access row plus access column plus access (read or write) data</li><li>Memory cycle time - defines how often we can read data from a memory</li><li>Both depends on type of the memory (static or dynamic)</li></ul><h1 id=virtual-memory>Virtual Memory</h1><p>Physical vs. logical address: More address spaces</p><p>Translation Lookaside Buffer (TLB):</p><ul><li>translates logical addresses to their physical equivalent</li><li>a part of processor hardware</li><li>TLB can have misses like any other cache</li></ul><p>Virtual memory and supercomputers</p><ul><li>usually not used in specialized architectures</li><li>now common due to the synergic architecture (clusters)</li></ul><h1 id=cache>Cache</h1><ul><li>Size: several kB to tens of MB</li><li>Organization: fixed length rows (16–128 bytes)</li><li>Types: direct mapped, set-associative, fully-associative</li><li>Hit ratio</li></ul><h1 id=memory-architectures>Memory Architectures</h1><p>Harvard Memory Architecture: separated memory for data and instructions
Programmable cache: cache directly controlled at some superscalar processors</p><h1 id=direct-mapped-cache>Direct mapped cache</h1><ul><li>Static mapping: each cache row can keep only pre-defined memory rows</li><li>Fast</li><li>Simple circuits</li><li>Potentially inefficient: used data may map to a single cache row</li></ul><h1 id=fully-associative-cache>Fully associative cache</h1><p>Dynamic mapping</p><ul><li>associative memory</li><li>each row in a cache knows where data lie in the main memory</li><li>access to cache goes to all rows</li><li>need to select a row for invalidation
Very efficient
Very complex circuits – expensive</li></ul><h1 id=set-associative-cache>Set associative cache</h1><ul><li>Sets of direct addressable caches</li><li>Combination of positive properties of the previous cases</li><li>Not full associativity (cheaper)</li><li>Still options where a memory row can be stored</li></ul><h1 id=width-of-data-flow>Width of data flow</h1><p>Bandwidth = maximal throughput of a memory system Bps
Throughput is not the same among all components</p><ul><li>Processor – registers – cache – main memory – external memory</li></ul><p>Latency: Time between the request and the actual data delivery (Extremely important esp. when moving small data chunks)</p><h1 id=interleaved-memory>Interleaved Memory</h1><ul><li>The whole memory split to smaller blocks<ul><li>Consecutive addresses mapped to different blocks</li><li>Allows immediate access</li></ul></li><li>2–8 way interleaved memories common</li><li>Higher latency</li></ul><h1 id=re-ordering-of-memory-accesses>Re-ordering of memory accesses</h1><ul><li>ANDES predecessor</li><li>Minimization of consecutive accesses to the same memory bank</li><li>Run-time check on Load and Store interdependencies</li></ul><h1 id=multiprocessor-systems>Multiprocessor systems</h1><ul><li>Scaling ratio (number of sockets) for symmetric memory</li><li>Distributed memory</li><li>Clusters with huge number of multiprocessor nodes</li></ul></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.77414ca1a0d130043c129098d97cecf433ce369d23de8eaa91f5111f432729db1257c49a33b38203d4be241ef53dafecd99a1d2c350b75316b55a0bb6a2e150b.js integrity="sha512-d0FMoaDRMAQ8EpCY2Xzs9DPONp0j3o6qkfURH0MnKdsSV8SaM7OCA9S+JB71Pa/s2ZodLDULdTFrVaC7ai4VCw=="></script></body></html>