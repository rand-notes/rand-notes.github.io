<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="LeNet-5 used for MNIST in 98. Conv and subsampling
Bag Of Visual Words Extract a set of local descriptors and assign each descriptor the closest entry in a visual vocabulary
The general idea of bag of visual words (BOVW) is to represent an image as a set of features. Features consists of keypoints and descriptors. Keypoints are the “stand out” points in an image, so no matter the image is rotated, shrink, or expand, its keypoints will always be the same."><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pa228/2/><title>recap 2 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="recap 2"><meta itemprop=description content="LeNet-5 used for MNIST in 98. Conv and subsampling
Bag Of Visual Words Extract a set of local descriptors and assign each descriptor the closest entry in a visual vocabulary
The general idea of bag of visual words (BOVW) is to represent an image as a set of features. Features consists of keypoints and descriptors. Keypoints are the “stand out” points in an image, so no matter the image is rotated, shrink, or expand, its keypoints will always be the same."><meta itemprop=wordCount content="1605"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="recap 2"><meta name=twitter:description content="LeNet-5 used for MNIST in 98. Conv and subsampling
Bag Of Visual Words Extract a set of local descriptors and assign each descriptor the closest entry in a visual vocabulary
The general idea of bag of visual words (BOVW) is to represent an image as a set of features. Features consists of keypoints and descriptors. Keypoints are the “stand out” points in an image, so no matter the image is rotated, shrink, or expand, its keypoints will always be the same."><meta property="og:title" content="recap 2"><meta property="og:description" content="LeNet-5 used for MNIST in 98. Conv and subsampling
Bag Of Visual Words Extract a set of local descriptors and assign each descriptor the closest entry in a visual vocabulary
The general idea of bag of visual words (BOVW) is to represent an image as a set of features. Features consists of keypoints and descriptors. Keypoints are the “stand out” points in an image, so no matter the image is rotated, shrink, or expand, its keypoints will always be the same."><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pa228/2/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="og:site_name" content="idk"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pa228/2/>recap 2</a></h2><div class=post-content><h1 id=lenet-5>LeNet-5</h1><p>used for MNIST in 98. Conv and subsampling</p><h1 id=bag-of-visual-words>Bag Of Visual Words</h1><p>Extract a set of local descriptors and assign each descriptor the closest entry in a visual vocabulary</p><p>The general idea of bag of visual words (BOVW) is to represent an image as a set of features. Features consists of
keypoints and descriptors. Keypoints are the “stand out” points in an image, so no matter the image is rotated, shrink, or
expand, its keypoints will always be the same. And descriptor is the description of the keypoint. We use the keypoints and
descriptors to construct vocabularies and represent each image as a frequency histogram of features that are in the image.
From the frequency histogram, later, we can find another similar images or predict the category of the image.</p><p>We detect features, extract descriptors from each image in the dataset, and build a visual dictionary. Detecting features
and extracting descriptors in an image can be done by using feature extractor algorithms (for example, SIFT, KAZE, etc).</p><h1 id=2011-ilsvrc>2011 ILSVRC</h1><p>Fisher Vectors + SIFT</p><ul><li>Generalization of BOVW</li><li>Describe image by what makes it different from other images</li><li>Probabilistic visual dictionary modeled by a Gaussian mixture model (GMM)</li></ul><h2 id=filter-kernelvector>Filter kernel/vector</h2><p>The Fisher kernel can also be applied to image representation for classification or retrieval problems.
The Fisher kernel can result in a compact and dense representation, which is more desirable for image classification</p><p>The Fisher Vector (FV), a special, approximate, and improved case of the general Fisher kernel, is an image
representation obtained by pooling local image features. The FV encoding stores the mean and the covariance deviation
vectors per component k of the Gaussian-Mixture-Model (GMM).</p><h1 id=alexnet>AlexNet</h1><p>worked because ImageNet was introduced, optimized convolutions on GPU, ReLU, Batch Normalization, Pooling, data
augmentation, dropout, softmax at the end.</p><p>Batch Normalization (BN) within CNNs is a technique that standardizes and normalizes inputs by transforming a batch of
input data to have a mean of zero and a standard deviation of one. AlexNet architecture used a different method of
normalization within the network: Local Response Normalization (LRN).</p><p>LRN is a technique that maximizes the activation of neighbouring neurons. Neighbouring neurons describe neurons across several feature maps that share the same spatial position. By normalizing the activations of the neurons, neurons with high activations are highlighted; this essentially mimics the lateral inhibition that happens within neurobiology.</p><h1 id=zfnet>ZFNet</h1><p>Visualization of Features: the visualization of features, the authors use deconvolutional networks.</p><p>standard step in DL is to have a series of Conv > Rectification (Activation Function) > Pooling.</p><p>To visualize a deep layer feature, we need a set of decovnet techniques to reverse the above actions such that we can
visualize the feature in pixel domain.</p><p>Max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the
maxima within each pooling region, as in the figure above.</p><h2 id=layers>Layers</h2><p>The first layer takes out the most simplistic of features like lines.</p><p>The second layer is modelling the various corners and edge/colour combinations. Think of it as learning the curves in the
images. Curves are formed from little lines.</p><p>The third layer learns more complex patterns such as meshes. Think of it as learning the combination of those curves i.e.
mesh. Curves come together to create meshes (think basket).</p><p>The fourth layer learns class-specific features such as dog faces. Think of it as getting the baskets shaped and painted into different things. Meshes can be transformed to resemble faces and various complex objects.</p><p>The fifth layers learn whole objects with some pose variation (side, front and others). Think of it like arranging all
those baskets into resembling different objects. Meshes resembling smaller artefacts of bigger objects can be arranged
together to make the whole shape.</p><h1 id=vggnet>VGGNet</h1><p>Smaller kernels on more layers have the same effective receptive field as a large kernel, but fewer parameters.</p><p>Vertical stacking = small kernels & more layers.</p><h1 id=googlenet--inception-module>GoogLeNet + Inception Module</h1><p>Nine stacked inception modules with dimensionality reduction</p><p>Auxiliary classification outputs to inject additional gradient at lower layers &ndash; dropedout after training</p><p>principles:</p><ul><li>Highly performant deep neural networks need to be large</li><li>multi-scale convnet have the potential to learn more.</li><li>Consideration of the Hebbian Principle — neurons that fire together, wire together.</li></ul><h2 id=1-x-1-convolutions>1 x 1 Convolutions</h2><p>purpose: Reduce the dimensions of data passing through the network, which provides the additional benefit of an increase
in the width and depth of the network.</p><p>A 1x1 convolution takes the element-wise product of all pixel values of an image.</p><h2 id=3-x-3-and-5-x-5-convolutions>3 x 3 and 5 x 5 Convolutions</h2><p>purpose: Enables the network to learn various spatial patterns at different scales as a result of the varying conv filter sizes</p><p>1x1 learns patterns across the depth of the input
3x3 and 5x5 learns spatial patterns across all dimensional components (height, width and depth)of the input</p><p>Within an Inception module, we add padding(same) to the max-pooling layer to ensure it maintains the height and width as the other outputs(feature maps) of the convolutional layers within the same Inception module.</p><h2 id=naive-inception>Naive Inception</h2><ul><li>Concatenate all filter responses channel-wise</li><li>Apply parallel filter operations on the input from the previous layer</li><li>Multiple receptive feature sizes</li><li>Pooling operation</li></ul><p>previous layer -> (1x1, 3x3, 5x5 conv, 3x3 maxpool)</p><h2 id=inception-module-with-dimensionality-reduction>Inception module with dimensionality reduction</h2><p>previous layer -> (1x1, 1x1 -> 3x3, 1x1 -> 5x5 conv, 3x3 maxpool -> 1x1)</p><h1 id=resnet>ResNet</h1><p>Very deep network using residual connections</p><p>the problem is an optimization problem, deeper models are harder to optimize</p><p>A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping</p><p>Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping</p><p>ResConnection works because the gradient gets to every layer, with only a small number of layers in between it needs to differentiate through</p><p>It is a way to solve the vanishing gradient problem. And therefore models could be built even deeper.</p><h1 id=senet>SENet</h1><p>Squeeze and Excitation (SE) block</p><p>Adaptive recalibration of channel-wise feature responses</p><p>SE Block:</p><ul><li>The block has a convolutional block as an input.</li><li>Each channel is &ldquo;squeezed&rdquo; into a single numeric value using average pooling.</li><li>A dense layer followed by a ReLU adds non-linearity and output channel complexity is reduced by a ratio.</li><li>Another dense layer followed by a sigmoid gives each channel a smooth gating function.</li><li>Finally, we weight each feature map of the convolutional block based on the side network; the &ldquo;excitation&rdquo;.</li></ul><h1 id=efficientnet>EfficientNet</h1><p>Balancing of network depth, width, and resolution lead to better performance</p><p>Compound scaling = width + depth + resolution scaling</p><h1 id=object-detection>Object Detection</h1><h2 id=sliding-windows>Sliding Windows</h2><p>Utilizing both a sliding window and an image pyramid we are able to detect objects in images at various scales and locations.
We take window and just slide it over image.</p><h2 id=fully-convolutional-network>Fully Convolutional Network</h2><p>Direct idea: design a network with only convolutional layers without downsampling to make prediction for all pixels at once</p><p>Convolutions at the full image resolution are very expensive!</p><p>Reduce feature map sizes? But semantic segmentation requires the same output and input size => <strong>upsampling</strong>
Reduce the number of convolution operations? But how to keep a reasonable effective receptive field? => <strong>atrous convolution</strong></p><h1 id=invariance-vs-equivariance>Invariance vs Equivariance</h1><p>Invariance - applying transformation won&rsquo;t change output
Equivariance - Doesn&rsquo;t matter if transformation applied on input or output (of model)</p><p>Pooling layers are approximately shift invariant
Convolutional layers are (mostly) shift equivariant</p><p>Idea: design a network with downsampling and upsampling inside the network</p><p>Downsampling: Pooling or strided convolution
Upsampling: upsampling or upconvolution</p><h1 id=upsampling>Upsampling</h1><ol><li>Interpolate discrete values f_k by a continuous function</li><li>make the geometric transformation (e.g. scaling)</li><li>resample the continuous function at new sample positions</li></ol><h2 id=interpolation>Interpolation</h2><p>Continuous function f(x) can be created from discrete values f_k by convolving with a kernel o(x)</p><p>filters:</p><ul><li>Nearest neighbor: <code>_|-|_</code></li><li>linear: <code>_/\_</code></li><li>high orders: gaussian or whatever</li></ul><h2 id=interpolation-of-scattered-data>Interpolation of Scattered Data</h2><p>Many different methods</p><ul><li>Triangulation based</li><li>Inverse distance weighted</li><li>Radial basis functions</li></ul><h2 id=usage-in-cnns>Usage In CNNs?</h2><p>Direct up-sampling layers</p><ul><li>No learnable parameters
Learnable resampling layers</li><li>Strided convolutions</li><li>Transposed convolutions</li></ul><h2 id=direct-up-sampling-layers>Direct up-sampling layers</h2><ul><li>bed of nails: [1,2] => [0, 1, 0, 2]</li><li>nearest neighbor: [1, 2] => [1, 1, 2, 2]</li><li>Remember positions of maxima and Update activations at remembered positions</li></ul><p>Strided Convolution -> Learnable Downsampling; Recall: strides higher than 1 lead to downsampling
Transposed Convolution -> Learnable upsampling</p><h2 id=scattered-data>Scattered data</h2><p>Scattered data consists of a set of points X and corresponding values V , where the points have no structure or order
between their relative locations.</p><h1 id=transposed-convolution>Transposed Convolution</h1><p>TODO</p><h1 id=u-net>U-Net</h1><p>U-Net architecture is popular in biomedical imaging
V-shape architecture consisting of encoding and decoding part with skip connections between same levels.</p><p>Conv + MaxPool + Conv + MaxPool + Conv + UpConv + Conv + UpConv</p><h1 id=atrous-or-dilated-convolution>Atrous or Dilated Convolution</h1><p>Dense evaluation of layers lead to shift-equivariance</p><p>The key application the dilated convolution authors have in mind is dense prediction: vision applications where the
predicted object that has similar size and structure to the input image.</p><p>In many such applications one wants to integrate information from different spatial scales and balance two properties:</p><ul><li>local, pixel-level accuracy, such as precise detection of edges, and</li><li>integrating knowledge of the wider, global context</li></ul><p>it allows for very large receptive fields while only growing the number of parameters logarithmically.</p><p>networks using only diluted convolutions are fully equivariant under translation, which is great for dense prediction
applications.</p><p>Dense prediction = task of predicting a label for each pixel in the image</p><h1 id=full-drn-architecture>Full DRN Architecture</h1><p>Dilated Residual Networks: ResNet with dilated convolutions</p><p>Full DRN - 8 levels</p><p>The purpose of levels 7 and 8 is to reduce gridding artefacts</p><h1 id=deformable-convolutional-networks>Deformable Convolutional Networks</h1><p>Based on deformable convolution: Generalization of the dilated convolution</p></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.c7e937cd76fb1097985d5c8ae541eb44da0ddf0ecfc093bb5e2fe6be756ac425b77825c1c7d6f19ac4c0c4d18ae466ea8bf31c3f646c557dedb72d8a31bbd7e2.js integrity="sha512-x+k3zXb7EJeYXVyK5UHrRNoN3w7PwJO7Xi/mvnVqxCW3eCXBx9bxmsTAxNGK5Gbqi/McP2RsVX3tty2KMbvX4g=="></script></body></html>