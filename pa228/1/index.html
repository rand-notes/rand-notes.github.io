<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="Recap Is it static or dynamic? What are the objects of interest? What is our a priori knowledge? E.g., object appearance, position, pose. Can we control the scene? Scene adaptation may be significantly cheaper than development of a general image processing method!
Point Spread Function - PSF In fluorescence microscopy, the acquired image is always a blurred representation of the actual object under the microscope. This blurring is described by the so-called Point Spread Function (PSF)."><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pa228/1/><title>recap 0 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="recap 0"><meta itemprop=description content="Recap Is it static or dynamic? What are the objects of interest? What is our a priori knowledge? E.g., object appearance, position, pose. Can we control the scene? Scene adaptation may be significantly cheaper than development of a general image processing method!
Point Spread Function - PSF In fluorescence microscopy, the acquired image is always a blurred representation of the actual object under the microscope. This blurring is described by the so-called Point Spread Function (PSF)."><meta itemprop=wordCount content="2235"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="recap 0"><meta name=twitter:description content="Recap Is it static or dynamic? What are the objects of interest? What is our a priori knowledge? E.g., object appearance, position, pose. Can we control the scene? Scene adaptation may be significantly cheaper than development of a general image processing method!
Point Spread Function - PSF In fluorescence microscopy, the acquired image is always a blurred representation of the actual object under the microscope. This blurring is described by the so-called Point Spread Function (PSF)."><meta property="og:title" content="recap 0"><meta property="og:description" content="Recap Is it static or dynamic? What are the objects of interest? What is our a priori knowledge? E.g., object appearance, position, pose. Can we control the scene? Scene adaptation may be significantly cheaper than development of a general image processing method!
Point Spread Function - PSF In fluorescence microscopy, the acquired image is always a blurred representation of the actual object under the microscope. This blurring is described by the so-called Point Spread Function (PSF)."><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pa228/1/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="og:site_name" content="idk"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pa228/1/>recap 0</a></h2><div class=post-content><h1 id=recap>Recap</h1><ul><li>Is it static or dynamic?</li><li>What are the objects of interest?</li><li>What is our a priori knowledge? E.g., object appearance, position, pose.</li><li>Can we control the scene?</li></ul><p>Scene adaptation may be significantly cheaper than development of a general image processing method!</p><h1 id=point-spread-function---psf>Point Spread Function - PSF</h1><p>In fluorescence microscopy, the acquired image is always a blurred representation of the actual object under the microscope. This blurring is described by the so-called Point Spread Function (PSF). The PSF describes what a single point in the object looks like in the image.</p><p>basically: Response of the imaging system to an infinitesimal point source</p><p>In general, it is space variant and depends on the illumination source (e.g., light wavelength)</p><p>If the PSF is known, it can be used to bring the acquired microscopy image closer to the true object through a deconvolution.</p><p>Deconvolution reverses the imaging convolution in an iterative fashion: a model of the object is made and iteratively improved upon by convolving it with the PSF and comparing the result to the actual image.</p><p>At the end of the process, the model of the object is an accurate representation of the true object, with improved resolution and signal-to-noise ratio relative to the acquired image. Notably, deconvolution can re-separate neighboring particles with overlapping PSFs, which is not possible with simple deblurring operations.</p><p>A multi-channel image requires a multi-channel PSF for deconvolution as the PSFs often differ between channels.</p><h1 id=optical-transfer-function>Optical Transfer Function</h1><ul><li>Characterizes spatial frequencies transmitted through the optical system</li><li>Formally Fourier transform of the PSF (in general a complex function)</li><li>In real systems, it is finite, i.e., band-limited</li><li>The largest transmitted frequency is called cut-off frequency</li><li>is a complex-valued function describing the response of an imaging system as a function of spatial frequency</li><li></li></ul><h1 id=linear-shift-invariant-system>Linear Shift-Invariant System</h1><p>PSF &mdash;-fourier transform&mdash;> MTF = |OTF|</p><p>PSF and OTF are related – convolution theorem</p><p>LSI has two properties:</p><ul><li>Linearity</li><li>invariance: this means that if we shift the input in time (or shift the entries in a vector) then the output is shifted by the same amount</li></ul><p>LSI systems are characterized by their “impulse response”</p><h1 id=quantization>Quantization</h1><p>Process of reducing the number of values</p><p>e.g. grayscale image -> we can have 8bit or 4 or 1bit (or whatever), these are levels -> the lower we go the less values(memory) is needed but also image is getting degrading</p><h1 id=spatial-filters>Spatial filters</h1><p>we can apply gaussian or laplacian(gauss-like but sharper /). Applying Gaussian filter will blurr image. It is a widely
used effect in graphics software, typically to reduce image noise and reduce detail. The visual effect of this blurring technique is a smooth blur resembling that of viewing the image through a translucent screen, distinctly different from the bokeh effect produced by an out-of-focus lens or the shadow of an object under usual illumination.</p><p>Gaussian smoothing is also used as a pre-processing stage in computer vision algorithms in order to enhance image structures at different scales—see scale space representation and scale space implementation.</p><p>Gaussian blur is a low-pass filter, attenuating high frequency signals</p><p>There are also filters like LoG filtering (Laplacian of Gaussian).</p><p>Gaussian smoothing is commonly used with edge detection. Most edge-detection algorithms are sensitive to noise; the 2-D Laplacian filter, built from a discretization of the Laplace operator, is highly sensitive to noisy environments.</p><p>Using a Gaussian Blur filter before edge detection aims to reduce the level of noise in the image, which improves the result of the following edge-detection algorithm. This approach is commonly referred to as Laplacian of Gaussian, or LoG filtering</p><p>A high pass filter can be used for sharpening image.</p><p>A Band-pass filter passes only frequencies in certain range.</p><h1 id=steerable-filter>Steerable filter</h1><p>A steerable filter is an orientation-selective convolution kernel used for image enhancement and feature extraction that
can be expressed via a linear combination of a small set of rotated versions of itself.</p><p>Steerable filters may be designed as approximations of a given filter shape up to a desired error or computational complexity</p><p>The steerable filters refer to a class of arbitrary orientation filters that can be synthesized into a linear combination of base filters</p><p>Allow synthetization of filters of arbitrary orientation from basis oriented filters</p><h1 id=filter-banks>Filter banks</h1><p>In signal processing, a filter bank (or filterbank) is an array of bandpass filters that separates the input signal into multiple components, each one carrying a single frequency sub-band of the original signal</p><p>Several filters are applied and combined</p><ul><li>Maximum</li><li>Sum</li><li>Average</li></ul><h1 id=pyramids>Pyramids</h1><p>at the bottom highest resolution -blur and subsample -> lower resolution -> repeat</p><p>implementation of a multi-scale, multi-orientation band-pass filter bank used for applications including image
compression, texture synthesis, and object recognition. It can be thought of as an orientation selective version of a
Laplacian pyramid, in which a bank of steerable filters are used at each level of the pyramid instead of a single
Laplacian or Gaussian filter.</p><h1 id=adaptive-histogram-equalization>Adaptive Histogram Equalization</h1><ul><li>Histogram equalization in local windows.</li><li>Applied only from a certain contrast to avoid noise amplification</li><li>Popular normalization method in deep learning</li></ul><h1 id=transformations>Transformations</h1><p>Geometrical Transformations: Common for augmentation of the training datasets and camera calibration</p><p>Local Transforms (or Also Filters): Pixel values are changed based on a local region around each pixel</p><p>Shift Invariant Linear Filtering by Convolution: Relation to frequency domain filtering:</p><ul><li>Low-pass filters (smoothing)</li><li>High-pass filters (details)</li><li>Band-pass filters (difference filters, e.g., DoG)</li></ul><h1 id=regional-maxima>Regional Maxima</h1><p>A connected set of pixels M_t is a regional maximum of function g at level t, iff all external boundary pixels have a value
strictly smaller than t nad all pixels in M_t have a value equal to t</p><p>High Regional Maxima by Thresholding: Select only the high ones</p><h1 id=extrema-dynamics>Extrema Dynamics</h1><p>Dynamic of a regional maximum is the minimum descent we have to do to reach a regional maximum at higher elevation</p><h1 id=h-extrema>H-Extrema</h1><p>Extrema with height (depth) higher than or equal to h</p><p>Also called extrema with a high local contrast or dynamic</p><p>Regional maxima from which we can reach a regional maximum with a higher elevation only along paths descending more than
or equal to h.</p><p>Background Correction: Low-pass filter or white (or black) top-hat</p><p>Uneven Illumination Correction: Could be measured or estimated from images</p><h1 id=how-images-are-represented-in-the-frequency-domain>How images are represented in the frequency domain</h1><p>similarly as 1d signal, 2d image can be represented as a sum of rotated sines and cosines.
If represented in frequency domain, convolution can be easily computed, also for image compression.</p><p>Similar to Fourrier Transform, we got Gabor Transform and Wavelets.</p><p>Time series: I know where the signal is at each time but I don&rsquo;t know anything about frequencies
Fourier transform: a tool for converting signals/images from time or spatial domain to the frequency domain
Gabor Transform: compute spectogram that contain info when each frequencies get turn on and off but with small resolution
Wavelets: similar to gabor transforms, but we are using knowledge that most low frequencies last longer so we create
hierchical grading of time and frequencies information.</p><p>Wavelet Transform: Calculates the approximation and detail coefficients in the wavelet series expansion</p><h1 id=wavelet-scattering>Wavelet Scattering</h1><p>Wavelet scattering is an equivalent deep convolutional network
formed by cascade of wavelets, modulus non-linearities, and low-pass filters to enable you to derive, with minimal
configuration, low-variance features from real-valued time series and image data for use in machine learning applications</p><p>Wavelet Scattering Network: repeated wavelet blocks
wavelet block: Wavelet Convolution -> Non-linearity -> averaging</p><p>Key differences w.r.t. CNNs: 1. kernels are fixed not learned, 2. modulus and averaging instead of ReLU and pooling</p><p>why modulus?
absolute value of a complex value.
Kind of pooling – combines real and imaginary parts.
It calculates a lower frequency envelope.
Modulus is L1 norm, which is invariant to translations</p><h2 id=continuous-wavelet-transform>Continuous Wavelet Transform</h2><p>CWT measures similarity of the signal with wavelets of varying frequency and scale
Highly redundant function of two continuous variables–translation and scale</p><p>Wavelet scattering transform (WST) has architectural similarities with deep convolutional networks</p><p>The filters in WST are pre-defined and fixed
WST can produce robust representations of data for learning
Works well also for small data sets
The scattering features are invariant to translation, rotation, scaling, and small deformations</p><h1 id=mathematical-morphology>Mathematical Morphology</h1><p>Non-linear theory for analysis of spatial structures</p><p>Many useful operators</p><ul><li>Dilation, erosion, hit-or-miss</li><li>Opening, closing, granulometry</li><li>Reconstruction, connected filtering</li><li>Extrema extraction</li><li>Watershed</li></ul><p>dilatation: lines get thicker
erosion: lines get thinner
opening: first erosion then dilation
closing: first dilation then erosion</p><h1 id=gabor-filter>Gabor Filter</h1><p>is a linear filter used for texture analysis, which essentially means that it analyzes whether there is any specific frequency content in the image in specific directions in a localized region around the point or region of analysis</p><p>In the spatial domain, a 2-D Gabor filter is a Gaussian kernel function modulated by a sinusoidal plane wave (see Gabor transform).</p><p>Usually different scales and orientations</p><h1 id=connected-component-trees>Connected Component Trees</h1><p>Extremely useful image representation for connected filtering (attribute filters)</p><p>father wavelets are lowpass filters
mother wavelets are highpass filters</p><h1 id=discrete-wavelet-transform>Discrete Wavelet Transform</h1><p>Wavelet was originally meant for compression. With correct configuration we can have most of signal 0s so its easier to
compress. It could be lossless or for higher efficiency lossy.</p><p>we can decompose in multiple levels. Back we are doing reconstruction.</p><h1 id=undecimated-wavelet-transform>Undecimated Wavelet Transform</h1><ul><li>DWT is not translation/shift invariant</li><li>UWT does not incorporate down sampling</li><li>inherently redundant scheme</li><li>useful for feature extraction</li></ul><h1 id=feature-map>Feature Map</h1><p>A feature map, or activation map, is the output activations for a given filter</p><h1 id=generalization-problem>Generalization problem</h1><ul><li>Large memorization capacity of networks</li><li>Prone to overfitting and overlearning</li></ul><h1 id=adversarial-fragility-problem>Adversarial fragility problem</h1><p>The neural networks can be tricked into producing completely different outputs after the application of imperceptible perturbations to their inputs</p><h1 id=scale-space-theory>Scale-Space Theory</h1><p>The stack of images as a function of increasing inner scale is coined a linear ’scale-space’.</p><p>Formal theory for handling image structures at different scales</p><p>Scale-space method attempts to represent data at all scales simultaneously</p><p>Image represented as one-parameter family of smoothed images</p><p>Why Gaussian?</p><p>Comes from so-called scale-space axioms.
Non-enhancement of local extrema, linearity, shift-, rotational-, scale-invariance, semigroup structure</p><h1 id=feature-extraction-in-scale-space>Feature Extraction in Scale-Space</h1><p>Set of scale-space derivatives up to order N = N-jet</p><p>Features expressed as a polynomial combinations of normalized Gaussian derivatives</p><h1 id=image-pyramids>Image Pyramids</h1><p>What are they good for? improve search, precomputation, image blending etc.</p><p>Conceptually simple structure for representing images at more than one resolution</p><p>Originally designed for image compression applications</p><p>Used in computer graphics as well as image analysis</p><p>In the early days of computer vision, pyramids were used as the main type of multi-scale representation for computing
multi-scale image features from real-world image data. More recent techniques include scale-space representation, which
has been popular among some researchers due to its theoretical foundation.</p><h1 id=laplacian-pyramid>Laplacian Pyramid</h1><p>created from gaussian pyramid.</p><p>Take original image and substract it with gaussian image.</p><h1 id=smoothing-as-low-pass-filtering>Smoothing as low-pass filtering</h1><p>High frequencies lead to trouble with sampling
suppress high frequencies before sampling:
solution: truncate high frequencies in FT or convolve with a low-pass filter</p><h1 id=features-extraction>Features Extraction</h1><p>Local features and their descriptors, which are a compact vector representations of a local neighborhood</p><p>The general idea is to transform image content into local feature coordinates invariant to translation, rotation, scale, and other imaging parameters</p><p>algos like: SIFT</p><h1 id=scale-invariant-feature-transform-sift>Scale Invariant Feature Transform (SIFT)</h1><p>Scale space peak selection</p><ul><li>Potential locations for finding features
Key point localization</li><li>Accurately locating the feature key points
Orientation Assignment</li><li>Assigning orientation to the key points
Key point descriptor</li><li>Describing the key point as a high dimensional vector</li></ul><p>LoG acts as a blob detector which detects blobs in various sizes due to change in o, where o acts as a scaling parameter. It&rsquo;s often used DoG (approximation of Log) because it&rsquo;s quicker</p><p>key localization/scale/rotation, solution: Difference of Gaussians / scale-space pyramid / orientation assignment</p><h2 id=key-point-localization>Key Point Localization</h2><p>Precise peak localization by fitting a 3D quadratic function</p><ul><li>Location, scale, and principle curvatures</li></ul><p>Outlier rejection in case of:</p><ul><li>Small contrast</li><li>Large ratio or principle curvatures</li></ul><h2 id=orientation-assignment>Orientation Assignment</h2><ul><li>Local orientation is calculated for each key point based on the orientation histogram in a surrounding region from local image gradient directions</li><li>In ambiguous cases with multiple peaks multiple key points are created</li><li>The location, scale, and orientation define a local coordinate system for each feature</li></ul><h2 id=the-local-image-descriptor>The Local Image Descriptor</h2><ul><li>Feature vector with 4 x 4 x 8 = 128 elements</li><li>Gradient magnitude and orientation at each sample point</li><li>Samples are weighted by a Gaussian window and accumulated into orientation histograms summarizing content over 4 x 4 subregions in 8 directions</li></ul><h1 id=one-times-one-convolutions>One Times One Convolutions</h1><p>Used for changing depth -> “dimensionality” reduction</p><p>CWH format: 64x56x56 &ndash;1x1conv-> 32 filters (64x1x1) &mdash;> 32x1x1</p><p>A feature map, or activation map, is the output activations for a given filter, so the feature map has no channels!</p><h1 id=blur-pooling>Blur Pooling</h1><ul><li>applying a spatial low-pass filter before pooling operations</li><li>maxpool has heavy aliasing, by bluring before downsampling we reduce aliasing a bit.</li><li>maxblurpool = max(dense) + anti-aliasing filter + subsampling</li></ul><h1 id=surf--speeded-up-robust-features>SURF – Speeded Up Robust Features</h1><p>Feature Detection</p><ul><li>Basic Hessian matrix approximation by box filters</li><li>Faster calculation of the DoG(difference of gaussian) pyramid
Feature Description:</li><li>Identify reproducible orientation by Haar wavelet responses</li><li>Calculate wavelet responses dx and dy in local 4x4 subregion</li></ul><h1 id=orb--oriented-fast-and-rotated-brief>ORB – Oriented FAST and Rotated BRIEF</h1><ul><li>Fast and accurate orientation component to FAST</li><li>The efficient computation of oriented BRIEF</li><li>Analysis of variance and correlation of oriented BRIEF</li><li>A learning method for decorrelating BRIEF features under rotational invariance, leading to better performance in nearest-neighbor applications</li></ul><h1 id=maximally-stable-extremal-regions>Maximally Stable Extremal Regions</h1><ul><li>MSER features</li><li>Go through thresholds, grab regions which stay nearly the same through a wide range of thresholds</li></ul></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.e8a56c89d5ca304d6922b22e38d0af2db97fa70a6623ba17a492092da773dfa4b9aaffa0f682ecfd03d7c7964e89bdbef18f0d4183c698831978c7ca44959d10.js integrity="sha512-6KVsidXKME1pIrIuONCvLbl/pwpmI7oXpJIJLadz36S5qv+g9oLs/QPXx5ZOib2+8Y8NQYPGmIMZeMfKRJWdEA=="></script></body></html>