<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="Discriminative Model: p(y|x) Generative Model: p(x) Conditional Generative Model p(x|y)
Generative Models Explicit density vs Implicit Density
Autoregressive models Explicit and tractable(careful construction) density
with these models I cannot say what it should generate. It just generate picture useful for e.g. inpainting - I have a picture with blank square and I want to fill it.
PixelRNN - RNN that color pixel by pixel. Every pixel is dependent only on left and top neighbor."><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pa228/4/><title>recap 0 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="recap 0"><meta itemprop=description content="Discriminative Model: p(y|x) Generative Model: p(x) Conditional Generative Model p(x|y)
Generative Models Explicit density vs Implicit Density
Autoregressive models Explicit and tractable(careful construction) density
with these models I cannot say what it should generate. It just generate picture useful for e.g. inpainting - I have a picture with blank square and I want to fill it.
PixelRNN - RNN that color pixel by pixel. Every pixel is dependent only on left and top neighbor."><meta itemprop=wordCount content="1275"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="recap 0"><meta name=twitter:description content="Discriminative Model: p(y|x) Generative Model: p(x) Conditional Generative Model p(x|y)
Generative Models Explicit density vs Implicit Density
Autoregressive models Explicit and tractable(careful construction) density
with these models I cannot say what it should generate. It just generate picture useful for e.g. inpainting - I have a picture with blank square and I want to fill it.
PixelRNN - RNN that color pixel by pixel. Every pixel is dependent only on left and top neighbor."><meta property="og:title" content="recap 0"><meta property="og:description" content="Discriminative Model: p(y|x) Generative Model: p(x) Conditional Generative Model p(x|y)
Generative Models Explicit density vs Implicit Density
Autoregressive models Explicit and tractable(careful construction) density
with these models I cannot say what it should generate. It just generate picture useful for e.g. inpainting - I have a picture with blank square and I want to fill it.
PixelRNN - RNN that color pixel by pixel. Every pixel is dependent only on left and top neighbor."><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pa228/4/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="og:site_name" content="idk"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pa228/4/>recap 0</a></h2><div class=post-content><p>Discriminative Model: p(y|x)
Generative Model: p(x)
Conditional Generative Model p(x|y)</p><h1 id=generative-models>Generative Models</h1><p>Explicit density vs Implicit Density</p><h1 id=autoregressive-models>Autoregressive models</h1><p>Explicit and tractable(careful construction) density</p><p>with these models I cannot say what it should generate. It just generate picture
useful for e.g. inpainting - I have a picture with blank square and I want to fill it.</p><p>PixelRNN - RNN that color pixel by pixel. Every pixel is dependent only on left and top neighbor. Slow
PixelCNN - Quicker than PixelRNN but not so good. Instead of RNN, used CNN, every pixel is dependent on context region of filter.</p><p>Explicit Density: Autoregression:</p><p>p(x) = p(x_1) * p(x_2|x1) &mldr; = \mul p(x_t | x1, &mldr; , x_{t-1})</p><h1 id=autoencoders-non-variational>Autoencoders (non-variational)</h1><p>No labels needed just raw data.</p><p>Features need to be lower dimensional than the data</p><p>from input data we encode features (encoder), then using decoder we decode features into reconstructed input data. Then L2 loss over reconstructed input and input.</p><p>We are learning net to &ldquo;compress&rdquo; data</p><p>After training throw away decoder and use encoder for a downstream task e.g. classification</p><p>These encoders can be used to initialize a supervised model &ndash; training on final data will be quicker and there will be no need to have many data</p><h1 id=variational-encoders>Variational Encoders</h1><p>The difference from non-variational is that instead of decoding directly z, we decode \u and \Sigma (parameter to gaussian distribution from which we will drawn), this allows us to sample from latent space to obtain z (embedding), we can then use z as input to
decoder. Decoder again doesn&rsquo;t output exact y, but values with which we sample from y
distribution.</p><p>training process:</p><ol><li>trained by maximizing the variational lower bound:</li><li>encoder output should mathe the prior p(z)</li><li>sample code z from encoder output</li><li>run sampled code through decoder to get a distribtion over data sample</li><li>original input data should be likely under the distribution from (4)</li><li>can sample a reconstruction from (4)</li></ol><p>generating data process:</p><ol><li>sample z from prior(z)</li><li>run sampled z through decoder to get distribution over data x</li><li>sample from distribution in (2) to generate data</li></ol><p>edit image process:</p><ol><li>run input data through encoder to get a distribution over latent codes</li><li>sample code z from encoder output</li><li>modify some dimensions of sample code</li><li>run modified z through decoder to get a distribution over data samples</li><li>sample new data from (4)</li></ol><p>the idea is that the z is vector where each attribute mean something. e.g. while generating face
first attribute decides size of smile, second if person has beard etc.</p><h1 id=summary>Summary</h1><p>Probabilistic spin to traditional autoencoders => allows generating data
defines an intractable density => derive and optimize a (variational) lower bound</p><p>Pros:</p><ul><li>Principled approach to generative models</li><li>Allows inference of q(z|x) can be useful feature representation for other tasks
Cons:</li><li>Maximizes lower bound of likelihood: okay, but not as good evaluation as PixelRNN</li><li>Samples blurrier and lower quality compared to SOTA GANs</li></ul><h1 id=autoregressive-vs-variational>Autoregressive vs. Variational</h1><p>Autoregressive:</p><ul><li>directly maximize p(data)</li><li>high-quzlity generated images</li><li>slow to generate</li><li>no explicit latent codes</li></ul><p>Variational:</p><ul><li>Maximize lower-bound on p(data)</li><li>Very dast to generate images</li><li>Learn rich latent codes</li></ul><h1 id=vector-quantized-vae>Vector-Quantized VAE</h1><p>Combining VAE & Autoregressive</p><h1 id=generative-adversarial-networks>Generative Adversarial Networks</h1><p>give up on modeling p(x), but allow us to draw samples from p(x)</p><p>jointly train generator G and discriminator D with minmax game</p><p>discriminator wants D(x) = 1 for real data and D(x) = 0 for fake data
generator wants D(x) = 1 for fake data</p><p>we are not minimizing any overall loss. No training curves to look at.</p><p>at start of training, generator is very bad and discriminator easily tell apart real/fake, so the gradient is very small - vanishing.</p><p>solution: right now G is trained to minize log(1 - D(G(Z))). Instead, train G to minimize -log(D(G(z))). Then G gets strong gradients at start of training.</p><p>caveats:</p><ul><li>G and D are nets with fixed architecture. we dont know whether they can actually represent the optimal D and G.</li><li>This tells us nothing about convergence to the optimal solution</li></ul><p>DC-GAN - GAN with use of Deep nets</p><p>Conditional GAN that can generate MNIST handwritten digits conditioned on a given class. Such a model can have various useful applications:
cGANs are not strictly unsupervised learning algorithms because they require labeled data as input to the additional layer.</p><p>This is where the cGANs come in as we can add an additional input layer of one-hot-encoded image labels. This additional layer guides the generator in terms of which image to produce.</p><p>The input to the additional layer can be a feature vector derived either an image that encodes the class or a set of specific characteristics we expect from the image.</p><h1 id=attention>Attention</h1><p>Dynamic selection process
Assists in analyzing complex scenes</p><h1 id=attention-in-dl>Attention in DL</h1><p>RNNs to construct attention
explicit predition of important regions
implicit attention process
self-attention</p><p>Channel Attention e.g. SENet
Channel & Spatial Attention
Spation Attention
Spation & Temporal Attention
Temporal Attention
Branch Attention e.g. SKNet</p><h1 id=channel-attention-se-net>Channel Attention SE Net</h1><p>Squeeze & Excitation
Channels that often represent diifferent features are weighted
More importatnt features get higher weights
The general idea is always same: emphasize more important features</p><h1 id=spatial-attention>Spatial Attention</h1><p>RAM, Self-Attention</p><h1 id=ram---recurrent-attention-model>RAM - recurrent attention model</h1><ol><li>glimpse sensor: extraction of retina-like representation with multiresolution patches centered at l_{t-1}</li><li>glimpse network: glimpse network + MLPs</li><li>RNN model</li></ol><p>basically RNN model consisting of glimpse networks</p><p>idea:</p><p>The Recurrent Attention Model (RAM) is a neural network that processes inputs sequentially, attending to different locations within the image one at a time, and incrementally combining information from these fixations to build up a dynamic internal representation of the image.</p><h1 id=attention-layer>Attention Layer</h1><p>Inputs:</p><ul><li>Query vector Q</li><li>Input vectors X</li><li>Key matrix W_k</li><li>Value matrix W_v
Computation:</li><li>Key vectors K = X * W_k</li><li>Value vectors V = X * W_v</li><li>Similarities E = q_i * k_j / \sqrt(D_q)</li><li>Attention weights A = softmax(E)</li><li>Output vector Y = AV</li></ul><h1 id=self-attention>Self Attention</h1><p>Inputs:</p><ul><li>Input vectors X</li><li>Query matrix W_q</li><li>Key matrix W_k</li><li>Value matrix W_v</li></ul><p>Computation:</p><ul><li>Query Vectors Q = X * W_q</li><li>Key Vectors K = X * W_k</li><li>Value Vectors V = X * W_v</li><li>Similarities E = q_i * k_j / \sqrt(D_q)</li><li>Attention Weights A = softmax(E)</li><li>Output vector Y = A*V</li></ul><p>Self attention is permutation equivariant
Self attention layer works on sets of vectors</p><p>Self attention knows nothing about the order of vectors that is processing!</p><h1 id=masked-self-attention-layer>Masked Self-Attention Layer</h1><p>Don’t let vectors to “look ahead” in the sequence.</p><p>Used for language modelling to predict next word</p><h1 id=the-transformer>The Transformer</h1><p>Transformer Block:
Input: Set of vectors x
Output: Set of vectors y</p><p>Self-Attention is the only interaction between vectors</p><p>Layer norm and MLP work independently per vector</p><p>Highly scalable, highly parallelizable</p><p>Transformer Block:
X -> Self-Attention -> (skip connection) -> Layer Normalization -> multiple MLPs -> (skip) ->
Layer Normalization -> Y</p><h1 id=post-norm-vs-pre-norm-transformer>Post-Norm vs Pre-Norm Transformer</h1><p>Layer normalization is after residual connections
or
Layer normalization is inside residual connections (Gives more stable training, used in practice)</p><p>A transformer is a sequence of transformer blocks</p><p>A new neural network model that only uses attention</p><p>Usage:</p><p>Add Attention to Existing CNN
Replace Convolution with “Local Attention”</p><ul><li>Map center of receptive field to a query</li><li>Map each element in receptive field to key and value</li><li>Compute output by attention
Standard Transformer on Pixels
Standard Transformer on Patches</li></ul><h1 id=vision-transformer-vit>Vision Transformer (ViT)</h1><p>Vision Transformer (ViT) Computer vision model with no convolutions!</p><p>process:</p><ul><li>take patches</li><li>flatten patches</li><li>Produce lower-dimensional linear embeddings from the flattened patches</li><li>Add positional embeddings</li><li>pass to transformer encoder</li><li>MLP Head</li></ul><h1 id=summary-1>Summary</h1><p>Attention is inspired by human/animal vision</p><p>Vision transformers have been super hot topic in 1-2 last years</p><p>Very different architecture than CNNs</p><p>Application to many tasks</p><p>Vision transformers are evolution not revolution</p></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.77414ca1a0d130043c129098d97cecf433ce369d23de8eaa91f5111f432729db1257c49a33b38203d4be241ef53dafecd99a1d2c350b75316b55a0bb6a2e150b.js integrity="sha512-d0FMoaDRMAQ8EpCY2Xzs9DPONp0j3o6qkfURH0MnKdsSV8SaM7OCA9S+JB71Pa/s2ZodLDULdTFrVaC7ai4VCw=="></script></body></html>