<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>iv111 on idk</title><link>https://rand-notes.github.io/fi/iv111/</link><description>Recent content in iv111 on idk</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 02 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://rand-notes.github.io/fi/iv111/index.xml" rel="self" type="application/rss+xml"/><item><title>lec 05</title><link>https://rand-notes.github.io/fi/iv111/5/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/5/</guid><description/></item><item><title>lec 06</title><link>https://rand-notes.github.io/fi/iv111/6/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/6/</guid><description/></item><item><title>lec 04</title><link>https://rand-notes.github.io/fi/iv111/4/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/4/</guid><description>Covariance $$ E([X - E(X)][Y - E(Y)]) = \sum_{i, j} p_{x_i, y_j} [x_i - E(X)][y_j - E(Y)] $$
is called the covariance of X and Y and denoted Cov(X, Y)
Covariance measures linear dependence between two random variables. It is positive if the variables are correlated, and negative when anticorrelated.
e.g. when Y = aX, using E(Y) = aE(x) we have Cov(X, Y) = aVar(X)
we define the correlation coefficient \rho(X, Y) as the normalized covariance, i.</description></item><item><title>IV111 lec 03</title><link>https://rand-notes.github.io/fi/iv111/3/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/3/</guid><description>Expectation of Random Variables Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists.</description></item><item><title>0</title><link>https://rand-notes.github.io/fi/iv111/0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/0/</guid><description>Countable Addiitivity Axiom If \( A_1, A,2, &amp;hellip; \) is an infinite sequence of disjoint events, then \( P(A_1 \cup A_2 \cup A_3 \cup &amp;hellip;) = P(A_1) + P(A_2) + P(A_3) + &amp;hellip; \)
Addiitivity holds only for countable sequences of events
The unit square (similarly, the real line etc) is not countable (its elements cannot be arranged in a sequence)</description></item><item><title>exam</title><link>https://rand-notes.github.io/iv111/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/iv111/exam/</guid><description>Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).
Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 .</description></item></channel></rss>