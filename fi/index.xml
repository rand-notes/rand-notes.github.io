<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fis on idk</title><link>https://rand-notes.github.io/fi/</link><description>Recent content in Fis on idk</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 05 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://rand-notes.github.io/fi/index.xml" rel="self" type="application/rss+xml"/><item><title>notes 01</title><link>https://rand-notes.github.io/pa039/01/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/01/</guid><description>CISC Size and speed of memory CISC directly supports compilers Rich addressing modes Complex instruction == microprogram Microinstructions: decomposition to simpler instructions
Disadvantages:
too complex instructions increasingly complex instruction analysis cross instruction relationships backward compatibility cost (within a family) Performance increase Clock cycles define processor’s performance Solution: parallelization
Pipelining five-stage pipelining:
Instruction Fetch instruction is loaded from a memory Instruction Decode instruction is decoded (recognized) Operand Fetch operands are ready (fetched from registers and/or memory) Execute instruction is executed Writeback results are written back Individual stages are processed in parallel, shifted by one stage</description></item><item><title>notes 02</title><link>https://rand-notes.github.io/pa039/02/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/02/</guid><description>Parallel computers Small-scale multiprocessing
2–several hundreds of cores mostly SMP (shared memory systems) Large-scale multiprocessing from hundreds to millions of cores Most often distributed memory Architecture
Single Instruction Multiple Data, SIMD Multiple Instruction Multiple Data, MIMD Programming models
Single Program Multiple Data, SPMD
Multiple programs Multiple Data, MPMD
Concurrent: A single program with multiple tasks in progress
Parallel: A single program with multiple task closely cooperating
Distributed: Several programs (loosely) cooperating</description></item><item><title>notes 03</title><link>https://rand-notes.github.io/pa039/03/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/03/</guid><description>Optimizing Compiler Translation to the Intermediate language Optimization intra-procedural analysis cycle optimization global optimization (inter-process optimization) Code generation use of all superscalar units Intermediate Language Quadruple (generally n-tuple) Memory: accessible through temporary variables Branches: condition calculated separately Branches: jumps to absolute addresses Basic blocks Program is represented as a flow graph Block – a code segment without branches/jumps One entry and one exit point Block as a DAG (Directed Acyclic Graph) Optimization within blocks Removal of repeated (sub)expressions Removal of redundant variables Additional concepts Variables</description></item><item><title>notes 04</title><link>https://rand-notes.github.io/pa039/04/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/04/</guid><description>Measurement of processing/compute time Optimization impossible without knowledge what to optimize We need to know data about program run Processing/run time of the whole program: time command Processing/run time of individual program components: profiling Run time comparison: benchmarking time command User time: Processor time consumed by user’s processes System time: Processor time consumed by the kernel services Elapsed time: Total run time (wall clock time) Profiling Attempt to get timing information about program parts Emphasis on dynamic (run time) behavior: static analysis is a part of software engineering Profiling shows a result of an interaction between program and the computing system it runs on Time spent in individual blocks Time spent in individual commands Number of repetition of blocks/commands Primary interest on procedures Profile: graph X axis: individual procedures Y axis: run time Basic principles Uses software tools for collection of data needed for the profile construction Usually some operating system support access to information available to kernel only Examples of usual profiling tools: gprof, oprofile, valgrind, pin Profile collected during the run time – dynamic profile Performance Engineering is the name of the profile data analysis Types of data collected Call graph at the procedures and functions level Call graph at the basis blocks level Memory performance Events related to the architecture, e.</description></item><item><title>notes 5</title><link>https://rand-notes.github.io/pa039/05/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/05/</guid><description>Parallel programming Data parallelism Identical instructions on different processors process different data In principle the SIMD model (Single Instruction Multiple Data) - e.g. loop parallelization Task parallelism MIMD model (Multiple Instruction Multiple Data) Independent blocks (functions, procedures, programs) run in parallel SPMD No synchronization at the level of individual instructions Equivalent to MIMD Message passing targets SPMD/MIMD Before MPI Many competing message passing libraries: Vendor specific/proprietary libraries Academic, narrow specific implementations Different communication models Difficult application development Need for “own” communication model to encapsulate the specific models MPI an attempt to define a standard set of communication calls Message Passing Interface Communication interface for parallel programs Defined through API Standardized Several independent implementations Programming model MPI designed originally for distributed memory architectures Currently supports hybrid models MPI Design Goals Portability Define standard APIs Define bindings for different languages Independent implementations Performance Independent hardware specific optimization Libraries, potential for changes in algorithms Functionality Goal to cover all aspects of inter-processor communication Library for message passing Designed for use on parallel computers, clusters and even Grids Make parallel hardware available for users library authors tools and app developers MPI Initialization Create an environment Specify that the program will use the MPI libraries No explicit work with processes Identity Any parallel (distributed) program needs to know How many processes are participating on the computation Identity of “own” process MPI Comm size and MPI Comm rank Work with messages Naive/primitive model Process A sends a message: operation send Process B receives a message: operation receive Lot of questions How to properly specify (define) the data?</description></item><item><title>notes 6</title><link>https://rand-notes.github.io/pa039/06/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/06/</guid><description>What makes GPU powerful? Parallelism types Task parallelism the problem is decomposed to parallel tasks tasks are typically complex, they can perform different jobs complex synchronization best for lower number of high-performance processors/cores Data parallelism the parallelism on a level of data structures typically the same operation on multiple elements of a data structure can be executed on simpler processors Programmer point of view some problems are more task-parallel, some more data-parallel (tree traversal vs.</description></item><item><title/><link>https://rand-notes.github.io/fi/pa163/1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/pa163/1/</guid><description>Omezení Množina doménových proměnných Y = {y_1, &amp;hellip;, y_k} Konečná množina hodnota (doména) D = D_1 \cup D_2 \cup D-k
Omezení (podmínka) c na Y je podmnožina D_1 x &amp;hellip; x D_k tj. relace
příklad:
promenne: A, B domény: {0, 1} pro A; {1,2} pro B omezení: A != B
Problém splňování podmínek (CSP) konečná množina proměnných X = {x1,&amp;hellip;,xn} konečná množina hodnot (doména) D = D1 \cup &amp;hellip; \cup Dn konečná množina omezení C = {c1,&amp;hellip;,cm}</description></item><item><title>asd</title><link>https://rand-notes.github.io/pa163/exam2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa163/exam2/</guid><description>CSP je silně k-konzistentní právě tehdy, když je j-konzistentní pro každé j≤k
Cesta (V0, V1, &amp;hellip; , Vm) je konzistentní právě tehdy, když pro každou dvojici hodnot x∈D0 a y∈Dm splňující binární podmínky na hraně V0, Vm existuje ohodnocení proměnných V1, &amp;hellip; Vm−1 takové, že všechny binární podmínky mezi sousedy Vj, Vj+1 jsou splněny.
CSP je konzistentní po cestě, právě když jsou všechny cesty konzistentní.
šířka grafu je minimum z šířek všech jeho uspořádaných grafů.</description></item><item><title>exam PA163</title><link>https://rand-notes.github.io/pa163/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa163/exam/</guid><description>funfacts Hranová konzistence je směrová - konzistence hrany (Vi, Vj) nezaručuje konzistenci hrany (Vj, Vi) Použitím AC odstraníme mnoho nekompatibilních hodnot, ale nedostame reseni problemu ani nevime jestli reseni existuje. PC ⇒ AC, ale AC !⇒ PC CSP na binarni pomoci dualniho problemu vezmeme podminku C_i a prevedeme ji na V_i, ktera bude obsahovat n-tice hodnot, ktere splnuji podminku.
napr. C1 = C + E &amp;gt; 2, kde C ma domenu {1,2,3} a E ma domenu {0,1}.</description></item><item><title>PA163 lec 2</title><link>https://rand-notes.github.io/fi/pa163/2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/pa163/2/</guid><description>Grafová reprezentace CSP Reprezentace podmínek intenzionální (matematická/logická formule) extenzionální (výčet k-tic kompatibilních hodnot, 0-1 matice) Graf: vrcholy, hrany (hrana spojuje dva vrcholy) Hypergraf: vrcholy, hrany (hrana spojuje množinu vrcholů) Reprezentace CSP pomocí hypergrafu podmínek: vrchol = proměnná, hyperhrana = podmínka Hypergraf Hypergraf je pojem z teorie grafů. Jedná se o zobecnění pojmu graf. Rozdíl je v tom, že hrany hypergrafu (hyperhrany) mohou spojovat libovolný počet vrcholů, zatímco u grafu spojují hrany vždy dva vrcholy.</description></item><item><title>pa163 lec03</title><link>https://rand-notes.github.io/fi/pa163/3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/pa163/3/</guid><description>Konzistence po cestě (PC path consistency) PC-2 Postup Promazu domeny pomoci inicialnich relaci &amp;ndash; kdyz mam V_1 != V_2 vymazu v domene V1 vsechny dvojice, kde V_1 == V_2 do fronty si vlozim vsechny cesty [1] vezmeme z fronty a provedeme revizi pokud pri revizi neco zmenim pridam do queue podle [2] [1] - nejlepe se to dela ze vezmu ze jdu pres vsechny body. napr mame V_1, V_2, V_3 ve trojuhelniku &amp;ndash; do fronty vlozime (V_1, V_2, V_3),(V_1, V_3, V_2),(V_2, V_1, V_3).</description></item><item><title>pv021</title><link>https://rand-notes.github.io/pv021/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pv021/exam/</guid><description>Net separation if output is step function (1 or 0) the neuron divide space into halves. We can then combine these half spaces with and/or neurons etc.
Non linear separation Three layer nets are capable of &amp;ldquo;approximating&amp;rdquo; any &amp;ldquo;reasonable&amp;rdquo; subset A of the input space R^k. Each hypercube K can be separated using a two layer network NK. Hypercube is equivalent to square or cube in any dimension representing any shapes using nn:</description></item><item><title>recap 0</title><link>https://rand-notes.github.io/pa228/1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/1/</guid><description>Recap Is it static or dynamic? What are the objects of interest? What is our a priori knowledge? E.g., object appearance, position, pose. Can we control the scene? Scene adaptation may be significantly cheaper than development of a general image processing method!
Point Spread Function - PSF In fluorescence microscopy, the acquired image is always a blurred representation of the actual object under the microscope. This blurring is described by the so-called Point Spread Function (PSF).</description></item><item><title>recap 0</title><link>https://rand-notes.github.io/pa228/4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/4/</guid><description>Discriminative Model: p(y|x) Generative Model: p(x) Conditional Generative Model p(x|y)
Generative Models Explicit density vs Implicit Density
Autoregressive models Explicit and tractable(careful construction) density
with these models I cannot say what it should generate. It just generate picture useful for e.g. inpainting - I have a picture with blank square and I want to fill it.
PixelRNN - RNN that color pixel by pixel. Every pixel is dependent only on left and top neighbor.</description></item><item><title>recap 2</title><link>https://rand-notes.github.io/pa228/2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/2/</guid><description>LeNet-5 used for MNIST in 98. Conv and subsampling
Bag Of Visual Words Extract a set of local descriptors and assign each descriptor the closest entry in a visual vocabulary
The general idea of bag of visual words (BOVW) is to represent an image as a set of features. Features consists of keypoints and descriptors. Keypoints are the “stand out” points in an image, so no matter the image is rotated, shrink, or expand, its keypoints will always be the same.</description></item><item><title>recap 3</title><link>https://rand-notes.github.io/pa228/3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/3/</guid><description>Error function regularization function: technique used for tuning the function by adding an additional penalty term in the error function.
Loss function L – penalization of dissimilar results
Categorical Loss Functions Typical for classification problems
Cross entropy applied to Softmax is often called Softmax Loss
Focal Loss softmax for one object or sigmoid for multiple objects
Generalization of cross entropy: Designed for Retina-Net to alleviate class imbalance problem.
Focal loss (FL) adopts another approach to reduce loss for well-trained class.</description></item></channel></rss>