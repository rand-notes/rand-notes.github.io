<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="z method
f distribution
t test
F test
Mnohonasobne porovnavani
Tukeyho
Scheffeho metoda
Bartlettuv test
Kruskal-Wallisuv Test
Jednovyberovy Wilconxonuv test (signed rank test) Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)
dvojne trideni hypotezy ve dvojnem trideni
linearni modely Znam ́enkov ́y test (sign test) Parovy znamenkovy test ANOVA Postup testovani ve dvojnem trideni Dvojne trideni s interakcemi
Testovani v modelu s interakcemi Interpretace indexu determinace R-squared
Index determinace"><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/fi/ma012/1/><title>ma012 1 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="ma012 1"><meta itemprop=description content="z method
f distribution
t test
F test
Mnohonasobne porovnavani
Tukeyho
Scheffeho metoda
Bartlettuv test
Kruskal-Wallisuv Test
Jednovyberovy Wilconxonuv test (signed rank test) Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)
dvojne trideni hypotezy ve dvojnem trideni
linearni modely Znam ́enkov ́y test (sign test) Parovy znamenkovy test ANOVA Postup testovani ve dvojnem trideni Dvojne trideni s interakcemi
Testovani v modelu s interakcemi Interpretace indexu determinace R-squared
Index determinace"><meta itemprop=datePublished content="2021-10-02T00:00:00+00:00"><meta itemprop=dateModified content="2021-10-02T00:00:00+00:00"><meta itemprop=wordCount content="1688"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="ma012 1"><meta name=twitter:description content="z method
f distribution
t test
F test
Mnohonasobne porovnavani
Tukeyho
Scheffeho metoda
Bartlettuv test
Kruskal-Wallisuv Test
Jednovyberovy Wilconxonuv test (signed rank test) Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)
dvojne trideni hypotezy ve dvojnem trideni
linearni modely Znam ́enkov ́y test (sign test) Parovy znamenkovy test ANOVA Postup testovani ve dvojnem trideni Dvojne trideni s interakcemi
Testovani v modelu s interakcemi Interpretace indexu determinace R-squared
Index determinace"><meta property="og:title" content="ma012 1"><meta property="og:description" content="z method
f distribution
t test
F test
Mnohonasobne porovnavani
Tukeyho
Scheffeho metoda
Bartlettuv test
Kruskal-Wallisuv Test
Jednovyberovy Wilconxonuv test (signed rank test) Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)
dvojne trideni hypotezy ve dvojnem trideni
linearni modely Znam ́enkov ́y test (sign test) Parovy znamenkovy test ANOVA Postup testovani ve dvojnem trideni Dvojne trideni s interakcemi
Testovani v modelu s interakcemi Interpretace indexu determinace R-squared
Index determinace"><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/fi/ma012/1/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="article:published_time" content="2021-10-02T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-02T00:00:00+00:00"><meta property="og:site_name" content="idk"><meta property="article:published_time" content="2021-10-02 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/fi/ma012/1/>ma012 1</a></h2><div class=post-content><ul><li><p>z method</p></li><li><p>f distribution</p></li><li><p>t test</p></li><li><p>F test</p></li><li><p>Mnohonasobne porovnavani</p></li><li><p>Tukeyho</p></li><li><p>Scheffeho metoda</p></li><li><p>Bartlettuv test</p></li><li><p>Kruskal-Wallisuv Test</p></li><li><p>Jednovyberovy Wilconxonuv test (signed rank test)
Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)</p></li></ul><hr><p>dvojne trideni
hypotezy ve dvojnem trideni</p><p>linearni modely
Znam ́enkov ́y test (sign test)
Parovy znamenkovy test
ANOVA
Postup testovani ve dvojnem trideni
Dvojne trideni s interakcemi</p><p>Testovani v modelu s interakcemi
Interpretace indexu determinace R-squared</p><p>Index determinace</p><h1 id=empirical-distribution-function>Empirical distribution function</h1><img src=/images/edf.png alt=edf class=center><h1 id=error-vs-residual>Error vs Residual</h1><blockquote><p>Error (or disturbance) is theis the deviation of the observed value from the (unobservable) true value of quantity of interest</p></blockquote><blockquote><p>Residual is the difference between the observed value and the estimated value of the quantity of interest</p></blockquote><p>Residual = observed value - predicted value: \(e = y - \hat{y}\)</p><p>The distinction is most important in regression analysis, where the concepts are sometimes called the <strong>regression errors</strong> and <strong>regression residuals</strong> and were they lead to the concept of studentized
residuals.</p><h1 id=degrees-of-freedom>Degrees of freedom</h1><blockquote><p>Degrees of freedom is the number of values in the final calculation of a statistic that are free to vary</p></blockquote><h1 id=z-test>Z-test</h1><blockquote><p>A z-test is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large1.</p></blockquote><p>The test statistic is assumed to have a normal distribution, and nuisance parameters such as standard deviation should be known in order for an accurate z-test to be performed.</p><ul><li>is a hypothesis test in which the z-statistic follows a normal distribution</li><li>A <strong>z-statistic</strong>, or <strong>z-score</strong>, is a number representing the result from the z-test</li><li>are closely related to t-tests, but t-tests are best performed when an experiment has a small sample size.</li><li>assume the standard deviation is known, while t-tests assume it is unknown.</li></ul><h1 id=t-distribution>t-distribution</h1><p><em>or Student&rsquo;s t-distribution</em></p><blockquote><p>is any continuous probability distribution that arise when astimating the mean of a <strong>normally distributed</strong> population in situations where the sample size is small and the population&rsquo;s standard
deviation is unknown.</p></blockquote><h1 id=t-test>t-test</h1><p><em>or student&rsquo;s t-test</em></p><blockquote><p>is any statistical hypothesis test in which the test statistic follows a Student&rsquo;s t-dristribution under the null hypothesis.</p></blockquote><p>Used to determine wheter the means of two groups are equal to each other. The assumption for the test is that both groups are sampled from normal distributions with equal variances. The null hypothesis
is that the two means are equal, and the alternative is that they are not. It is known that under the null hypothesis, we can calculate a t-statistic that will follow a t-distribution with \( n1 + n2 -
2 \) degrees of freedom.</p><p>There is also a widely used modification of the t-test, known as <strong>Welch&rsquo;s t-test</strong> that adjusts the number of degrees of freedom when the variances are thought not to be equal to each other.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> rnorm(<span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> rnorm(<span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>t<span style=color:#f92672>.</span>test(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74> Welch Two Sample t-test
</span></span></span><span style=display:flex><span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74> data:  x and y
</span></span></span><span style=display:flex><span><span style=color:#e6db74> t = -1.6264, df = 16.666, p-value = 0.1226
</span></span></span><span style=display:flex><span><span style=color:#e6db74> alternative hypothesis: true difference in means is not equal to 0
</span></span></span><span style=display:flex><span><span style=color:#e6db74> 95 percent confidence interval:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  -1.4494854  0.1886137
</span></span></span><span style=display:flex><span><span style=color:#e6db74> sample estimates:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   mean of x   mean of y 
</span></span></span><span style=display:flex><span><span style=color:#e6db74> 0.007872345 0.638308171
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span></code></pre></div><h1 id=f-distribution>F-distribution</h1><p><em>or F-ratio also known as Snedecor&rsquo;s F distribution or the Fisher–Snedecor distribution</em></p><blockquote><p>is a continuous probability distribution that arises frequently as the null distribution of a test statistic, most notably in the ANOVA and other F-tests.</p></blockquote><p>The F-distribution with \(d_1\) and \(d_2\) degrees of freedom is the distribution of:</p><p>$$X = \frac{S_1/d_1}{S_2/d_2}$$</p><p>where \(S_1\) and \(S_2\) are independent random variables with chi-square distributions respective degrees of freedom \(d_1\) and \(d_2\)</p><h1 id=f-test>F-test</h1><blockquote><p>is any statistical test in which the test statistic has an F-distribution under the null hypothesis.</p></blockquote><p>It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exact &ldquo;F-tests&rdquo;
mainly arise when the models have been fitted to the data using least squares.</p><p>The test statistic can be obtained by computing the ration of the two variances \( S_A^2 \) and \( S_B^2 \)</p><p>$$F = \frac{S_A^2}{S_B^2}$$</p><p>The degrees of freedom are \( n_A - a \) (for the numerator) and \( n_B -1 \) (for the denominator)</p><h1 id=chi-square-distribution>Chi-square distribution</h1><p><em>also chi-square or \( \chi^2 \)-distribution</em></p><blockquote><p>with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.</p></blockquote><p>if \(Z_1, &mldr;, Z_k \) are independent, standard normal random variables, then sum of their squares:</p><p>$$Q=\sum^k_{i=1} Z_i^2$$</p><p>is distributed according to the chi-squared distribution with k degrees of freedom. This is usually denoted as:</p><p>$$Q \sim \chi^2 (k) \text{ or } Q \sim \chi^2_k $$</p><p>The chi-squared distribution has one parameter: a positive integer k that specifies the number of degrees of freedom (the number of random variables being summed, Zi s)</p><h1 id=multiple-comparisons-problem>Multiple Comparisons Problem</h1><p><em>or multiple comparisons, multiplicity or multiple testing problem</em></p><blockquote><p>occurs when one considers a set of statistical inferences simultaneously or inders a subset of parameters selected based on the observed values.</p></blockquote><h2 id=tukeys-range-test>Tukey&rsquo;s Range Test</h2><p><em>or Tukey&rsquo;s test, Tukey method &mldr;</em></p><blockquote><p>is a single-step multiple comparsion procedure and statistical test. It can be used to find means that are significantly different from each other.</p></blockquote><p>Tukey&rsquo;s test is based on a formula very similar to that of the t-test. In fact, Tukey&rsquo;s test is essentially a t-test, except that it corrects for family-wise error rate.</p><p>The formula for Tukey&rsquo;s test is:</p><p>$$q_s = \frac{Y_A - Y_B}{SE}$$</p><p>where Y_A is the larger of the two means being compared, Y_B is the smaller, and SE is the standard error of the sum of the means.</p><h2 id=scheffés-method>Scheffé&rsquo;s method</h2><blockquote><p>is a method for adjusting significance levels in a linear regression analysis to account for multiple comparsions.</p></blockquote><p>It is particularly useful in ANOVA and in constructing simultaneous confidence bands for regressions involving basis functions.</p><h1 id=bartletts-test>Bartlett&rsquo;s test</h1><blockquote><p>is used to test homoscedasticity, that is, if multiple samples are from populations with equal variances.</p></blockquote><p>Some statistical tests, such as the ANOVA assume that variances are equal across groups or samples, which can be verified with Bartlett&rsquo;s test.
If the corresponding p-value of the test statistic is less than some significance level (like α = .05) then we can reject the null hypothesis and conclude that not all groups have the same variance.</p><p>Sample output in R:</p><pre tabindex=0><code>	Bartlett test of homogeneity of variances

data:  clotting by diet
Bartlett&#39;s K-squared = 1.668, df = 3, p-value = 0.6441
</code></pre><p>If we assume significance level 0.05, we can reject null hypothesis (that clotting depends on diet).</p><h1 id=levenes-test>Levene&rsquo;s test</h1><blockquote><p>is an inferential statistic used to asses the equality of variances for a variable calculated for two or more groups.</p></blockquote><p>Levene&rsquo;s test is alternative to Bartlett but is often less sensitive. Rejecting hypothesis same as Bartlett&rsquo;s.</p><pre tabindex=0><code>Levene&#39;s Test for Homogeneity of Variance (center = median)
      Df F value Pr(&gt;F)
group  3  1.0109 0.4086
      20       
</code></pre><h1 id=anova>ANOVA</h1><p>We must met three assumptions: normality, equal variances, independence.</p><p>If p-value is smaller than significance level we reject the null hypothesis that the variances are equal across all groups.</p><h2 id=normality>Normality</h2><p>ANOVA assumes that each sample was drawn from a normally distributed population.</p><ul><li>Check the assumption visually using histograms or Q-Q plots.</li><li>Check the assumption using formal statistical tests like Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or D’Agostino-Pearson.</li></ul><h2 id=equal-variance>Equal Variance</h2><p>ANOVA assumes that the variances of the populations that the samples come from are equal.</p><ul><li>Check the assumption visually using boxplots.</li><li>Check the assumption using a formal statistical tests like Bartlett’s or Levene&rsquo;s Test.</li></ul><h2 id=independence>Independence</h2><p>The observations in each group are independent of the observations in every other group. The observations within each group were obtained by a random sample.</p><h1 id=sign-test>Sign Test</h1><blockquote><p>is a statistical method to test for consistent differences between pairs of observations</p></blockquote><h1 id=wilcoxon-signed-rank-test>Wilcoxon signed-rank test</h1><p><em>Jednovýběrový Wilcoxonův párový test</em></p><blockquote><p>is a non-parametric statistical hypothesis test used either to test the location of a set of samples or to compare the locations of two populations using a set of matched samples.</p></blockquote><h1 id=mannwhitney-u-test>Mann–Whitney U test</h1><p><em>Dvouvýběrový Wilcoxonův párový test or Wilcoxon rank-sum test</em></p><blockquote><p>is a non-parametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater
than X.</p></blockquote><h1 id=van-der-waerden-test>Van der Waerden test</h1><p>is a statistical test that k population distribution functions are equal. Van der Waerden test converts the ranks from a standard Kruskal-Wallis one-way analysis of variance to quantiles of the standard
normal distribution.</p><h1 id=kruskalwallis-test>Kruskal–Wallis test</h1><p><em>or Kruskal–Wallis one-way analysis of variance or one-way ANOVA on ranks</em></p><blockquote><p>is a non-parametric method for testing whether samples originate from the same distribution.</p><p>The parametric equivalent of the Kruskal–Wallis test is the ANOVA</p></blockquote><p>It used for comparing two or more independent samples of equal or different sample sizes. It exteds the <em>Mann-Whitney U test</em>, which is used for comparing only two groups.</p><h1 id=normality-tests>Normality tests</h1><p>normality tests are used to determine if a data set is well-modeled by a normal distribution and to compute how likely it is for a random variable underlying the data set to be normally distributed.</p><h2 id=kolmogorovsmirnov-test>Kolmogorov–Smirnov test</h2><p><em>or KS test</em></p><blockquote><p>is a nonparametric test of the equality of continuous, one dimensional probability distributions</p></blockquote><p>It can be used to compare a sample with a reference probability distribution (one-sample KS) or to comapre two-samples (two-sample KS).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=display:flex><span><span style=color:#a6e22e>ks.test</span>(X, <span style=color:#e6db74>&#34;pnorm&#34;</span>, mean<span style=color:#f92672>=</span><span style=color:#66d9ef>...</span>, sd<span style=color:#f92672>=</span><span style=color:#66d9ef>...</span>)
</span></span></code></pre></div><h2 id=pearsons-chi-squared-test>Pearson&rsquo;s chi-squared test</h2><p>Test dobré shody</p><p><em>\(\chi^2\)</em> test</p><blockquote><p>is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It is the most widely used of many chi-squared
tests (Yates, likelihood ratio, portmanteau test in time series, etc.)</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=display:flex><span><span style=color:#a6e22e>chisq.test</span>(X, p<span style=color:#f92672>=</span><span style=color:#66d9ef>...</span>)
</span></span></code></pre></div><h2 id=lilliefors-test>Lilliefors test</h2><blockquote><p>is a normality test based on the Kolmogorov–Smirnov test. It is used to test the null hypothesis that data come from a normally distributed population, when the null hypothesis does not specify which
normal distribution; i.e., it does not specify the expected value and variance of the distribution.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=display:flex><span><span style=color:#a6e22e>lillie.test</span>(X)
</span></span></code></pre></div><h2 id=other-normality-tests-in-r>Other Normality Tests in R</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=display:flex><span><span style=color:#75715e># Anderson–Darling test</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ad.test</span>(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Cramér–von Mises test</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>cvm.test</span>(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Shapiro–Wilk test</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>shapirotest</span>(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># D&#39;Agostino&#39;s K-squared test</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>agostino.test</span>(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Jarque–Bera test</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>jarque.test</span>(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Q-Q plot</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>qqnorm</span>(X)
</span></span><span style=display:flex><span><span style=color:#75715e># or</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>qqline</span>(X)
</span></span></code></pre></div></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.c7e937cd76fb1097985d5c8ae541eb44da0ddf0ecfc093bb5e2fe6be756ac425b77825c1c7d6f19ac4c0c4d18ae466ea8bf31c3f646c557dedb72d8a31bbd7e2.js integrity="sha512-x+k3zXb7EJeYXVyK5UHrRNoN3w7PwJO7Xi/mvnVqxCW3eCXBx9bxmsTAxNGK5Gbqi/McP2RsVX3tty2KMbvX4g=="></script></body></html>