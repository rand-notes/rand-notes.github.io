<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>idk</title><link>https://rand-notes.github.io/</link><description>Recent content on idk</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 05 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://rand-notes.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>notes 01</title><link>https://rand-notes.github.io/pa039/01/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/01/</guid><description>CISC Size and speed of memory CISC directly supports compilers Rich addressing modes Complex instruction == microprogram Microinstructions: decomposition to simpler instructions
Disadvantages:
too complex instructions increasingly complex instruction analysis cross instruction relationships backward compatibility cost (within a family) Performance increase Clock cycles define processor’s performance Solution: parallelization
Pipelining five-stage pipelining:
Instruction Fetch instruction is loaded from a memory Instruction Decode instruction is decoded (recognized) Operand Fetch operands are ready (fetched from registers and/or memory) Execute instruction is executed Writeback results are written back Individual stages are processed in parallel, shifted by one stage</description></item><item><title>notes 02</title><link>https://rand-notes.github.io/pa039/02/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/02/</guid><description>Parallel computers Small-scale multiprocessing
2–several hundreds of cores mostly SMP (shared memory systems) Large-scale multiprocessing from hundreds to millions of cores Most often distributed memory Architecture
Single Instruction Multiple Data, SIMD Multiple Instruction Multiple Data, MIMD Programming models
Single Program Multiple Data, SPMD
Multiple programs Multiple Data, MPMD
Concurrent: A single program with multiple tasks in progress
Parallel: A single program with multiple task closely cooperating
Distributed: Several programs (loosely) cooperating</description></item><item><title>notes 03</title><link>https://rand-notes.github.io/pa039/03/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/03/</guid><description>Optimizing Compiler Translation to the Intermediate language Optimization intra-procedural analysis cycle optimization global optimization (inter-process optimization) Code generation use of all superscalar units Intermediate Language Quadruple (generally n-tuple) Memory: accessible through temporary variables Branches: condition calculated separately Branches: jumps to absolute addresses Basic blocks Program is represented as a flow graph Block – a code segment without branches/jumps One entry and one exit point Block as a DAG (Directed Acyclic Graph) Optimization within blocks Removal of repeated (sub)expressions Removal of redundant variables Additional concepts Variables</description></item><item><title>notes 04</title><link>https://rand-notes.github.io/pa039/04/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa039/04/</guid><description>Measurement of processing/compute time Optimization impossible without knowledge what to optimize We need to know data about program run Processing/run time of the whole program: time command Processing/run time of individual program components: profiling Run time comparison: benchmarking time command User time: Processor time consumed by user’s processes System time: Processor time consumed by the kernel services Elapsed time: Total run time (wall clock time) Profiling Attempt to get timing information about program parts Emphasis on dynamic (run time) behavior: static analysis is a part of software engineering Profiling shows a result of an interaction between program and the computing system it runs on Time spent in individual blocks Time spent in individual commands Number of repetition of blocks/commands Primary interest on procedures Profile: graph X axis: individual procedures Y axis: run time Basic principles Uses software tools for collection of data needed for the profile construction Usually some operating system support access to information available to kernel only Examples of usual profiling tools: gprof, oprofile, valgrind, pin Profile collected during the run time – dynamic profile Performance Engineering is the name of the profile data analysis Types of data collected Call graph at the procedures and functions level Call graph at the basis blocks level Memory performance Events related to the architecture, e.</description></item><item><title>lec 05</title><link>https://rand-notes.github.io/fi/iv111/5/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/5/</guid><description/></item><item><title>lec 06</title><link>https://rand-notes.github.io/fi/iv111/6/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/6/</guid><description/></item><item><title>cheatsheet</title><link>https://rand-notes.github.io/fi/ma012/cs/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/cs/</guid><description>CPD - continuous probability distribution
T-distribution - CPD for astimating the mean of a normally distributed population with small size and unknown standard deviation T-test - t-distribution under null hypothesis
F-distribution - CPD used as null hypothesis in ANOVA and other F-tests. F-test &amp;ndash; F-distribution under null hypothesis
z-test - test whether two population means are different, when variances are known and sample size large
Tests Overview H_0 zamitneme pokud je p mensi nez alpha</description></item><item><title>lec 03</title><link>https://rand-notes.github.io/fi/iv126/3/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv126/3/</guid><description>Population-based Metaheuristics Genetic Algorithms Evolution Strategies Genetic Programming Ant Colony Optimization Particle Swarm Optimization Bee Colony Artificial Immune Systems Estimation of Distribution Algorithms general algo:
t = 0 while not end_condition: generate_popul(P_t) # generate new population P_t+1 = choose_new_popul() t += 1 Basic division:
Algorithms using evolution
Solution in population is choosed and reproduced with operators (mutation and crossovering).
e.g. genetics algorithms, evolution strategies
Algorithms using memory (blackboard)
solution in population is participating on construction of memory which is used to create new individuals.</description></item><item><title>lec 2</title><link>https://rand-notes.github.io/fi/iv126/2/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv126/2/</guid><description>Neighborhoods Definition: Function of neighborhood N is mapping \( N: S \rightarrow 2^S \), which assign every solution s from Z, set of solutions \( N(S) \subset S \)
Effective Algorithms for Searching in Very Large Neighborhoods space size: polynom of higher order of magnitude (n &amp;gt; 2)
main problem is indentification of improving neighborhoods or best neighbor without enumeration of whole neighborhood.
Ejection Chain e.g. Capacitated Vehicle Routing Problem</description></item><item><title>lec 1</title><link>https://rand-notes.github.io/fi/iv126/1/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv126/1/</guid><description>Approximate Methods is usually understood to give an approximate solution, with some kind of guarantee of performace. i.e. it solves TSP, and the total cost is never off by more than a factor of 2.
Exploration (Diversification) Terms Diversification and Intensification are being used mostly in conjunction with population-based optimization techniques
consists of probing a much larger portion of the search space with the hope of finding other promising solutions that are yet to be refined.</description></item><item><title>lec 04</title><link>https://rand-notes.github.io/fi/iv111/4/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/4/</guid><description>Covariance $$ E([X - E(X)][Y - E(Y)]) = \sum_{i, j} p_{x_i, y_j} [x_i - E(X)][y_j - E(Y)] $$
is called the covariance of X and Y and denoted Cov(X, Y)
Covariance measures linear dependence between two random variables. It is positive if the variables are correlated, and negative when anticorrelated.
e.g. when Y = aX, using E(Y) = aE(x) we have Cov(X, Y) = aVar(X)
we define the correlation coefficient \rho(X, Y) as the normalized covariance, i.</description></item><item><title>ma012 1</title><link>https://rand-notes.github.io/fi/ma012/1/</link><pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/1/</guid><description>z method
f distribution
t test
F test
Mnohonasobne porovnavani
Tukeyho
Scheffeho metoda
Bartlettuv test
Kruskal-Wallisuv Test
Jednovyberovy Wilconxonuv test (signed rank test) Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)
dvojne trideni hypotezy ve dvojnem trideni
linearni modely Znam ́enkov ́y test (sign test) Parovy znamenkovy test ANOVA Postup testovani ve dvojnem trideni Dvojne trideni s interakcemi
Testovani v modelu s interakcemi Interpretace indexu determinace R-squared
Index determinace</description></item><item><title>ma012 2</title><link>https://rand-notes.github.io/fi/ma012/2/</link><pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/2/</guid><description>Test (ne)korelovanosti Pearsonuv vyberovy korelacni koeficient \( \rho_{XY} \) je odhadem teoreticke korelace \( rho_{XY} \) mezi nahodnymi velicinami X a Y.
Test (ne)korelovanosti je vlastne testem vyznamnosti korelace \rho_(XY)
$$ H_0: \rho_{XY} \eq 0 H_1: \rho_{XY} \neq 0 $$
H_0 nezamitneme \( r_{XY} \approx 0 \): nekorelovanost = lineární nezávistlost X, Y. Pozor nekorelovanost neimplikuje stochastickou nezávistlost. H_0 zamitneme \( r_{XY} \approx 1 \): souhlasná lineární závislost X, Y H_0 zamitneme \( r_{XY} \approx -1 \):nesouhlasná lineární závislost X, Y</description></item><item><title>Confidence Intervals</title><link>https://rand-notes.github.io/fi/ma012/conf-intervals/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/conf-intervals/</guid><description>Statistical Inference is the process of drawing conclusions about populations or scientific truths from data
There are two types of statistical inferences; estimation and statistical (hypothesis) tests
Estimation Use information from the sample to estimate (or predict) the parameter of interest.
For instance, using the result of a poll about the president&amp;rsquo;s current approval rating to estimate (or predict) his or her true current approval rating nationwide.
Point Estimates
An estimate for a parameter that is one numerical value.</description></item><item><title>ma012 lec0</title><link>https://rand-notes.github.io/fi/ma012/0/</link><pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/0/</guid><description>A bell curve is a graph depicting the normal distribution
p-value A p-value is a measure of the probability that an observed difference could have occurred just by random chance. p-values are numbers in interval (0, 1) and commonly used threshold is 0.05. The lower the p-value, the greater the statistical significance of the observed difference. While a small p-value helps us decide if one group differs from another, it does not tell us how different they are.</description></item><item><title>IV111 lec 03</title><link>https://rand-notes.github.io/fi/iv111/3/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/3/</guid><description>Expectation of Random Variables Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists.</description></item><item><title>Environments, Agents, Optimizations</title><link>https://rand-notes.github.io/fi/iv126/0/</link><pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv126/0/</guid><description>Environments Fully Observable vs Partially Observable If an agent&amp;rsquo;s sensors give it access to the complete state of the environment at each point in time, then we say that he task environment is fully observable. If the agents has no sensors at all then the environment is unobservable
Single-Agent vs Multi-Agent Environments For example, an agent solving a crossword puzzle by itself is clearly in a single-agent enviroment, whereas an agent playing chess is in a two-agent enviroment.</description></item><item><title>cpp</title><link>https://rand-notes.github.io/cpp/2021-01-01-cpp/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/cpp/2021-01-01-cpp/</guid><description/></item><item><title>DL0</title><link>https://rand-notes.github.io/dl0/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl0/</guid><description>TOC Underflow
Overflow
Softmax Problems Condition number
Gradient-Based Optimization objective function; criterio; cost function; loss function; Gradient Descent; critical points; stationary points; local minumum; local maximum; Saddle points; global minimum; partial derivatives; gradient; directional derivative; method of steepest descent; gradient descent; Chain rule; Steepest descent proposes a new point; learning rate; line search; hill climbing; Jacobian matrix; second derivative; Hessian matrix; Jacobian; Hessian; second derivative test; Newton&amp;rsquo;s method; first-order optimization algorithms; second-order optimization algorithms; Lipschitz continous; Lipschitz constant; convex optimization;</description></item><item><title>DL1</title><link>https://rand-notes.github.io/dl1/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl1/</guid><description>TOC
self-information nats; bits; shannons;
Shannon entropy
KL Divergence
CrossEntropy
Factorization
Structured Probabilistic Model
Directed
Undirected The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions;
Self-information we rate information value much higher if information is less likely and independent.
I(x) = -log(P(x))
we are using natural logarithm with base e so I(x) is in units of nats. One nat is the amount fo information gained by observing an event of probability 1/e.</description></item><item><title>DL2</title><link>https://rand-notes.github.io/dl2/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl2/</guid><description>TOC Learning algorithms
The Task, T
Classification
Classification with missing inputs
Regression
Transcription
Machine translation
Structured output
Anomaly detection
Synthesis and sampling
Imputation of missing values
Denoising
Density estimation or probability mass function estimation
The Performance Measure, P accuracy; error rate;
The Experience, E supervised; unsupervised; Dataset; data points;
Unsupervised Learning Algorithm
Supervised Learning Algorithm label; target;
Overview semi-supervised; multi-instance; Reinforcement learning;
Dataset design matrix;
Capacity, Overfitting and Underfitting generalization; training error; generalization error; test error; test set; Statistical learning theory; data generating process; data generating distribution; underfitting; overfitting; capacity; hypothesis space; Representational capacity; effective capacity; Occam&amp;rsquo;s razor; Vapnik-Chervonenkis dimension; nonparametric models; Bayes error;</description></item><item><title/><link>https://rand-notes.github.io/fi/iv126/task/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv126/task/</guid><description>fyzicke stroje p \in P virtualni stroje v \in C
R(v, r) udava, kolik zdroje r vyuziva virtualni stroj v C(p, r) udava jakou kapacitu zdroje r je mozne na fyzickem stroji p naaalokavat SC(p, r) bezpecna kapacita zdroje r na fyzickem stroji p
plati: SC(p, r) &amp;lt; C(p, r)
MC(v) cena za migraci virtualniho stroje v PMC(p, p&amp;rsquo;) cena za migraci libovolneho virtualniho stroje z fyzickeho stroje p na fyzicky stroj p&amp;rsquo;.</description></item><item><title/><link>https://rand-notes.github.io/fi/ma012/cheatsheet2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/cheatsheet2/</guid><description># PCA # cor=TRUE =&amp;gt; pracuje s korelacni matici misto kovariancni, tj. standardizuje vsechny veliciny # scores=TRUE =&amp;gt; spocita i koeficienty pozorovani v hlavnich komponentach pca &amp;lt;- princomp(X, cor = TRUE, scores = TRUE) pca$scores Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables
vif(model) # VIF = variance inflation factors model &amp;lt;- lm(vydaje ~ ., data = domacnosti) # klasicky LRM # Stepwise regressiom # backward model.</description></item><item><title/><link>https://rand-notes.github.io/fi/pa163/1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/pa163/1/</guid><description>Omezení Množina doménových proměnných Y = {y_1, &amp;hellip;, y_k} Konečná množina hodnota (doména) D = D_1 \cup D_2 \cup D-k
Omezení (podmínka) c na Y je podmnožina D_1 x &amp;hellip; x D_k tj. relace
příklad:
promenne: A, B domény: {0, 1} pro A; {1,2} pro B omezení: A != B
Problém splňování podmínek (CSP) konečná množina proměnných X = {x1,&amp;hellip;,xn} konečná množina hodnot (doména) D = D1 \cup &amp;hellip; \cup Dn konečná množina omezení C = {c1,&amp;hellip;,cm}</description></item><item><title>0</title><link>https://rand-notes.github.io/fi/iv111/0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/0/</guid><description>Countable Addiitivity Axiom If \( A_1, A,2, &amp;hellip; \) is an infinite sequence of disjoint events, then \( P(A_1 \cup A_2 \cup A_3 \cup &amp;hellip;) = P(A_1) + P(A_2) + P(A_3) + &amp;hellip; \)
Addiitivity holds only for countable sequences of events
The unit square (similarly, the real line etc) is not countable (its elements cannot be arranged in a sequence)</description></item><item><title>adadas</title><link>https://rand-notes.github.io/ai/exam2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/ai/exam2/</guid><description>basic/pokrocile prohledavani == grafy
ejection chain; cyclic exchange
hill climbing simulovane zihani - akceptuje i horsi threshold accepting - akceptuje s max zhorsenim o prah potopa - zmenusujeme hladinu tabu seznam - muzeme overridnout pomoci aspiracniho kriteria
Multistart local search, Iterativní lokální prohledávání
VNS - variable neighborhood search VND - variable neighborhood descent
deterministicka verze VNS rozsirujeme okoli, po uspechu jedeme od zacatky GLS - guided local search
cena + penalizace + charakteristika (bool jestli je v reseni) upravena ucelova fce: f&amp;rsquo;(s) = f(s) + \lambda \sum p_i I_i(s) Výběr ruletovým kolem Pravděpodobnostní univerzální vzorkování (ruletove kolo se 4 ukazateli)</description></item><item><title>algos</title><link>https://rand-notes.github.io/ai/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/ai/exam/</guid><description>k-opt TODO
k-distance TODO
Ejection Chain řetězec odstranění: posloupnost přesunů zákazníka z jednoho okruhu na následující úspěšná změna: žádný vrchol není v řešení více než jednou řetězec odstranění nemusí být cyklický (stačí umístit konzistentně posledního zákazníka do posledního okruhu) postup každý řetězec odstranění zahrnuje k úrovní začínajících na okruhu 1 a končících na okruhu k vrchol je odstraněn z okruhu 1 a přesunut na okruh 2, vrchol z okruhu 2 přesunut na okruh 3, atd.</description></item><item><title>asd</title><link>https://rand-notes.github.io/pa163/exam2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa163/exam2/</guid><description>CSP je silně k-konzistentní právě tehdy, když je j-konzistentní pro každé j≤k
Cesta (V0, V1, &amp;hellip; , Vm) je konzistentní právě tehdy, když pro každou dvojici hodnot x∈D0 a y∈Dm splňující binární podmínky na hraně V0, Vm existuje ohodnocení proměnných V1, &amp;hellip; Vm−1 takové, že všechny binární podmínky mezi sousedy Vj, Vj+1 jsou splněny.
CSP je konzistentní po cestě, právě když jsou všechny cesty konzistentní.
šířka grafu je minimum z šířek všech jeho uspořádaných grafů.</description></item><item><title>cheatsheet1</title><link>https://rand-notes.github.io/exam1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/exam1/</guid><description>Anova Predpoklady faktor A (popr. B) musi mit 3 a vic urovni Normalita hodnot jednotlivých náhodných výběrů Stejný rozptyl hodnot ve všech srovnávaných skupinách Nezávislost pozorovaných hodnot (casto se bere automaticky) Hypotezy HA0: vsechny stredni hodnoty v radcich jsou stejne tj. faktor A nema vliv HA1: nektere dvojice strednich hodnot v radcich se lisi tj. faktor A ma vliv HB0: vsechny stredni hodnoty v sloupcich jsou stejne tj. faktor B nema vliv HB1: nektere dvojice strednich hodnot v sloupcich se lisi tj.</description></item><item><title>cheatsheet1</title><link>https://rand-notes.github.io/exam2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/exam2/</guid><description>Model vícenásobné lineárníj regrese Slouží k modelování jednorozměrné náhodné veličiny Y skupinou p náhodných veličin X1,&amp;hellip;,Xp. Vycházíme ze znalosti (p+1)-dimenzionálního náhodného výběru rozsahu n. Model mnohonásobné lineární regrese zapíšeme ve tvaru Y=X*Beta. Y je vektor (Y1,..Yn), Beta je vektor (B0,B1, &amp;hellip;,Bn), kde B1,..,Bn jsou lineární koeficienty náhodných veličin X1,&amp;hellip;,Xp. X je matice, která má první sloupec jednotkový, a další sloupce tvoří náhodné výběry rozsahu n jednotlivých náhodných veličin. Model řešíme odhadem parametrů metodou nejmenších čtverců.</description></item><item><title>exam</title><link>https://rand-notes.github.io/iv111/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/iv111/exam/</guid><description>Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).
Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 .</description></item><item><title>exam PA163</title><link>https://rand-notes.github.io/pa163/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa163/exam/</guid><description>funfacts Hranová konzistence je směrová - konzistence hrany (Vi, Vj) nezaručuje konzistenci hrany (Vj, Vi) Použitím AC odstraníme mnoho nekompatibilních hodnot, ale nedostame reseni problemu ani nevime jestli reseni existuje. PC ⇒ AC, ale AC !⇒ PC CSP na binarni pomoci dualniho problemu vezmeme podminku C_i a prevedeme ji na V_i, ktera bude obsahovat n-tice hodnot, ktere splnuji podminku.
napr. C1 = C + E &amp;gt; 2, kde C ma domenu {1,2,3} a E ma domenu {0,1}.</description></item><item><title>Examples</title><link>https://rand-notes.github.io/fi/ma012/ex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/ex/</guid><description>Ex 1 On significance level 0.05, test null hypothesis that mean values for X are same for all groups Y.
Steps:
Bartlett and Levene -&amp;gt; ANOVA -&amp;gt; Tukey and Scheffe Ex 2 Let&amp;rsquo;s have two random vectors X = (2, 4, 6, 8) a Y = (1, 3, 5, 7).
Compute Kendall \( \tau \) and number of concordant pairs:
res&amp;lt;-cor.test(X, Y, method=&amp;#34;kendall&amp;#34;) res The result will be:
Kendall&amp;#39;s rank correlation tau data: Y and X T = 6, p-value = 0.</description></item><item><title>iv111</title><link>https://rand-notes.github.io/iv111/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/iv111/exam/</guid><description>Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).
Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 .</description></item><item><title>math 0</title><link>https://rand-notes.github.io/math/0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/math/0/</guid><description>Pursuit Curve A curve of pursuit is a curve constructed by analogy to having a point or points representing pursuers and pursuees; the curve of pursuit is the curve traced by the pursuers. With the paths of the pursuer and pursuee parameterized in time, the pursuee is always on the pursuer&amp;rsquo;s tangent.
e.g. one plane chasing another. The plane that is being chased must have predetermined path (flying upwards, in circle etc.</description></item><item><title>overview 1</title><link>https://rand-notes.github.io/fi/ma012/cs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/cs/</guid><description>CPD - continuous probability distribution
T-distribution - CPD for astimating the mean of a normally distributed population with small size and unknown standard deviation T-test - t-distribution under null hypothesis
F-distribution - CPD used as null hypothesis in ANOVA and other F-tests. F-test &amp;ndash; F-distribution under null hypothesis
z-test - test whether two population means are different, when variances are known and sample size large
Tests Overview H_0 zamitneme pokud je p mensi nez alpha</description></item><item><title>overview 2</title><link>https://rand-notes.github.io/fi/ma012/overview2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/overview2/</guid><description>2 cviko Na hladine vyznamnosti testujte nulovou hypotezu, ze stredni hodnoty trzeb vsechno proddavacu se rovnaji. Spocitejte ze hmotnost brambor nesouvisi s odrudou.
reseni: Levene a Bartletts test pro overeni ze variance(rozptyl) je podobna mezi skupinami Shapiruv-Wilkeuv a Kolmogorovuv-Smirnovuv test pro overeni normality ANOVA pro testovani stredni hodnoty Tukey a Scheffe pro porovnani ktere dvojice se nejvic lisi
R:
leveneTest(tabulka$hmotnost ~ tabulka$odruda) # odruda musi byt factor bartlett.test(tabulka$hmotnost ~ tabulka$odruda)</description></item><item><title>PA163 lec 2</title><link>https://rand-notes.github.io/fi/pa163/2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/pa163/2/</guid><description>Grafová reprezentace CSP Reprezentace podmínek intenzionální (matematická/logická formule) extenzionální (výčet k-tic kompatibilních hodnot, 0-1 matice) Graf: vrcholy, hrany (hrana spojuje dva vrcholy) Hypergraf: vrcholy, hrany (hrana spojuje množinu vrcholů) Reprezentace CSP pomocí hypergrafu podmínek: vrchol = proměnná, hyperhrana = podmínka Hypergraf Hypergraf je pojem z teorie grafů. Jedná se o zobecnění pojmu graf. Rozdíl je v tom, že hrany hypergrafu (hyperhrany) mohou spojovat libovolný počet vrcholů, zatímco u grafu spojují hrany vždy dva vrcholy.</description></item><item><title>pa163 lec03</title><link>https://rand-notes.github.io/fi/pa163/3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/pa163/3/</guid><description>Konzistence po cestě (PC path consistency) PC-2 Postup Promazu domeny pomoci inicialnich relaci &amp;ndash; kdyz mam V_1 != V_2 vymazu v domene V1 vsechny dvojice, kde V_1 == V_2 do fronty si vlozim vsechny cesty [1] vezmeme z fronty a provedeme revizi pokud pri revizi neco zmenim pridam do queue podle [2] [1] - nejlepe se to dela ze vezmu ze jdu pres vsechny body. napr mame V_1, V_2, V_3 ve trojuhelniku &amp;ndash; do fronty vlozime (V_1, V_2, V_3),(V_1, V_3, V_2),(V_2, V_1, V_3).</description></item><item><title>pv021</title><link>https://rand-notes.github.io/pv021/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pv021/exam/</guid><description>Net separation if output is step function (1 or 0) the neuron divide space into halves. We can then combine these half spaces with and/or neurons etc.
Non linear separation Three layer nets are capable of &amp;ldquo;approximating&amp;rdquo; any &amp;ldquo;reasonable&amp;rdquo; subset A of the input space R^k. Each hypercube K can be separated using a two layer network NK. Hypercube is equivalent to square or cube in any dimension representing any shapes using nn:</description></item><item><title>recap 0</title><link>https://rand-notes.github.io/pa228/1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/1/</guid><description>Recap Is it static or dynamic? What are the objects of interest? What is our a priori knowledge? E.g., object appearance, position, pose. Can we control the scene? Scene adaptation may be significantly cheaper than development of a general image processing method!
Point Spread Function - PSF In fluorescence microscopy, the acquired image is always a blurred representation of the actual object under the microscope. This blurring is described by the so-called Point Spread Function (PSF).</description></item><item><title>recap 0</title><link>https://rand-notes.github.io/pa228/4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/4/</guid><description>Discriminative Model: p(y|x) Generative Model: p(x) Conditional Generative Model p(x|y)
Generative Models Explicit density vs Implicit Density
Autoregressive models Explicit and tractable(careful construction) density
with these models I cannot say what it should generate. It just generate picture useful for e.g. inpainting - I have a picture with blank square and I want to fill it.
PixelRNN - RNN that color pixel by pixel. Every pixel is dependent only on left and top neighbor.</description></item><item><title>recap 2</title><link>https://rand-notes.github.io/pa228/2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/2/</guid><description>LeNet-5 used for MNIST in 98. Conv and subsampling
Bag Of Visual Words Extract a set of local descriptors and assign each descriptor the closest entry in a visual vocabulary
The general idea of bag of visual words (BOVW) is to represent an image as a set of features. Features consists of keypoints and descriptors. Keypoints are the “stand out” points in an image, so no matter the image is rotated, shrink, or expand, its keypoints will always be the same.</description></item><item><title>recap 3</title><link>https://rand-notes.github.io/pa228/3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/pa228/3/</guid><description>Error function regularization function: technique used for tuning the function by adding an additional penalty term in the error function.
Loss function L – penalization of dissimilar results
Categorical Loss Functions Typical for classification problems
Cross entropy applied to Softmax is often called Softmax Loss
Focal Loss softmax for one object or sigmoid for multiple objects
Generalization of cross entropy: Designed for Retina-Net to alleviate class imbalance problem.
Focal loss (FL) adopts another approach to reduce loss for well-trained class.</description></item></channel></rss>