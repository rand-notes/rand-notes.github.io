<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>idk</title><link>https://rand-notes.github.io/</link><description>Recent content on idk</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 04 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://rand-notes.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>lec 04</title><link>https://rand-notes.github.io/fi/iv111/4/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/4/</guid><description>Covariance $$ E([X - E(X)][Y - E(Y)]) = \sum_{i, j} p_{x_i, y_j} [x_i - E(X)][y_j - E(Y)] $$
is called the covariance of X and Y and denoted Cov(X, Y)
Covariance measures linear dependence between two random variables. It is positive if the variables are correlated, and negative when anticorrelated.
e.g. when Y = aX, using E(Y) = aE(x) we have Cov(X, Y) = aVar(X)
we define the correlation coefficient \rho(X, Y) as the normalized covariance, i.</description></item><item><title>ma012 1</title><link>https://rand-notes.github.io/ma012/1/</link><pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/ma012/1/</guid><description>z method
f distribution
t test
F test
Mnohonasobne porovnavani
Tukeyho
Scheffeho metoda
Bartlettuv test
Kruskal-Wallisuv Test
Jednovyberovy Wilconxonuv test (signed rank test) Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)
dvojne trideni hypotezy ve dvojnem trideni
linearni modely Znam ́enkov ́y test (sign test) Parovy znamenkovy test ANOVA Postup testovani ve dvojnem trideni Dvojne trideni s interakcemi</description></item><item><title>Confidence Intervals</title><link>https://rand-notes.github.io/fi/ma012/conf-intervals/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/conf-intervals/</guid><description>Statistical Inference is the process of drawing conclusions about populations or scientific truths from data
There are two types of statistical inferences; estimation and statistical (hypothesis) tests
Estimation Use information from the sample to estimate (or predict) the parameter of interest.
For instance, using the result of a poll about the president&amp;rsquo;s current approval rating to estimate (or predict) his or her true current approval rating nationwide.
Point Estimates</description></item><item><title>ma012 lec0</title><link>https://rand-notes.github.io/fi/ma012/0/</link><pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/0/</guid><description>A bell curve is a graph depicting the normal distribution
p-value A p-value is a measure of the probability that an observed difference could have occurred just by random chance. p-values are numbers in interval (0, 1) and commonly used threshold is 0.05. The lower the p-value, the greater the statistical significance of the observed difference. While a small p-value helps us decide if one group differs from another, it does not tell us how different they are.</description></item><item><title>IV111 lec 03</title><link>https://rand-notes.github.io/fi/iv111/3/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/3/</guid><description>Expectation of Random Variables Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists.</description></item><item><title>ai 0</title><link>https://rand-notes.github.io/fi/iv126/0/</link><pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv126/0/</guid><description>Environments Fully Observable vs Partially Observable If an agent&amp;rsquo;s sensors give it access to the complete state of the environment at each point in time, then we say that he task environment is fully observable. If the agents has no sensors at all then the environment is unobservable
Single-Agent vs Multi-Agent Environments For example, an agent solving a crossword puzzle by itself is clearly in a single-agent enviroment, whereas an agent playing chess is in a two-agent enviroment.</description></item><item><title>cpp</title><link>https://rand-notes.github.io/cpp/2021-01-01-cpp/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/cpp/2021-01-01-cpp/</guid><description/></item><item><title>DL0</title><link>https://rand-notes.github.io/dl0/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl0/</guid><description>TOC Underflow
Overflow
Softmax
Problems Condition number
Gradient-Based Optimization objective function; criterio; cost function; loss function; Gradient Descent; critical points; stationary points; local minumum; local maximum; Saddle points; global minimum; partial derivatives; gradient; directional derivative; method of steepest descent; gradient descent; Chain rule; Steepest descent proposes a new point; learning rate; line search; hill climbing; Jacobian matrix; second derivative; Hessian matrix; Jacobian; Hessian; second derivative test; Newton&amp;rsquo;s method; first-order optimization algorithms; second-order optimization algorithms; Lipschitz continous; Lipschitz constant; convex optimization;</description></item><item><title>DL1</title><link>https://rand-notes.github.io/dl1/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl1/</guid><description>TOC
self-information nats; bits; shannons;
Shannon entropy
KL Divergence
CrossEntropy
Factorization
Structured Probabilistic Model
Directed
Undirected The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions;
Self-information we rate information value much higher if information is less likely and independent.</description></item><item><title>DL2</title><link>https://rand-notes.github.io/dl2/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl2/</guid><description>TOC Learning algorithms
The Task, T
Classification
Classification with missing inputs
Regression
Transcription
Machine translation
Structured output
Anomaly detection
Synthesis and sampling
Imputation of missing values
Denoising
Density estimation or probability mass function estimation
The Performance Measure, P accuracy; error rate;</description></item><item><title>0</title><link>https://rand-notes.github.io/fi/iv111/0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv111/0/</guid><description>Countable Addiitivity Axiom If \( A_1, A,2, &amp;hellip; \) is an infinite sequence of disjoint events, then \( P(A_1 \cup A_2 \cup A_3 \cup &amp;hellip;) = P(A_1) + P(A_2) + P(A_3) + &amp;hellip; \)
Addiitivity holds only for countable sequences of events
The unit square (similarly, the real line etc) is not countable (its elements cannot be arranged in a sequence)</description></item><item><title>lec 1</title><link>https://rand-notes.github.io/fi/iv126/1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/iv126/1/</guid><description>Christofides Algorithm</description></item></channel></rss>