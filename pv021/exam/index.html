<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="Net separation if output is step function (1 or 0) the neuron divide space into halves. We can then combine these half spaces with and/or neurons etc.
Non linear separation Three layer nets are capable of &amp;ldquo;approximating&amp;rdquo; any &amp;ldquo;reasonable&amp;rdquo; subset A of the input space R^k. Each hypercube K can be separated using a two layer network NK. Hypercube is equivalent to square or cube in any dimension representing any shapes using nn:"><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/pv021/exam/><title>pv021 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="pv021"><meta itemprop=description content="Net separation if output is step function (1 or 0) the neuron divide space into halves. We can then combine these half spaces with and/or neurons etc.
Non linear separation Three layer nets are capable of &ldquo;approximating&rdquo; any &ldquo;reasonable&rdquo; subset A of the input space R^k. Each hypercube K can be separated using a two layer network NK. Hypercube is equivalent to square or cube in any dimension representing any shapes using nn:"><meta itemprop=wordCount content="1016"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="pv021"><meta name=twitter:description content="Net separation if output is step function (1 or 0) the neuron divide space into halves. We can then combine these half spaces with and/or neurons etc.
Non linear separation Three layer nets are capable of &ldquo;approximating&rdquo; any &ldquo;reasonable&rdquo; subset A of the input space R^k. Each hypercube K can be separated using a two layer network NK. Hypercube is equivalent to square or cube in any dimension representing any shapes using nn:"><meta property="og:title" content="pv021"><meta property="og:description" content="Net separation if output is step function (1 or 0) the neuron divide space into halves. We can then combine these half spaces with and/or neurons etc.
Non linear separation Three layer nets are capable of &ldquo;approximating&rdquo; any &ldquo;reasonable&rdquo; subset A of the input space R^k. Each hypercube K can be separated using a two layer network NK. Hypercube is equivalent to square or cube in any dimension representing any shapes using nn:"><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/pv021/exam/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi"><meta property="og:site_name" content="idk"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/pv021/exam/>pv021</a></h2><div class=post-content><h1 id=net-separation>Net separation</h1><p>if output is step function (1 or 0) the neuron divide space into halves. We can then combine
these half spaces with and/or neurons etc.</p><h1 id=non-linear-separation>Non linear separation</h1><ul><li>Three layer nets are capable of &ldquo;approximating&rdquo; any &ldquo;reasonable&rdquo; subset A of the input space R^k.</li><li>Each hypercube K can be separated using a two layer network NK.</li><li>Hypercube is equivalent to square or cube in any dimension</li></ul><p>representing any shapes using nn:</p><ul><li>we can &ldquo;fill&rdquo; the shape with hypercubes (and combine them with or)</li><li>i.e. we need 3 layers &ndash; 2 to define square (hypercubes) and one to connect them.</li></ul><h2 id=theorem---cybenko>Theorem - Cybenko</h2><blockquote><p>two layers for any space (returns 1 if in space else 0)</p></blockquote><ul><li>for set A \in [0, 1]^n there is two layer network with sigmoidal activation and linear outputs such that: for most vectors v \in [0, 1]^n we have that v \in A iff the net output is <code>></code> 0 for the input v.</li></ul><h1 id=function-approximation>Function approximation</h1><blockquote><p>three layer net to approximate any continuous function using summing up the sigmoidal functions</p></blockquote><p>For every continuous function f: [0,1]^n -> [0,1] and \eps > 0 there is a three layer net
computing a function F: [0, 1]^n -> [0, 1]</p><ul><li>neurons have the logistic sigmoid as their activation, only outputs are linear.</li></ul><p>process:</p><ul><li>two layers to construct hill from two sigmoidal functions</li><li>one layer to sum the hills</li></ul><h2 id=theorem---cybenko-1>Theorem - Cybenko</h2><blockquote><p>two layer networks can approximate any continuous function</p></blockquote><p>For every continuous function and every \eps > 0 there is a function F computed by a two layer
network where each hidden neuron sigmoidal activation function and outputs are linear that
satisfies <code>|f(v) - F(v)| &lt; \eps</code> i.e. f and F are ~same</p><h1 id=computability>Computability</h1><p>we have a word w from {0, 1}+ alphabet. Encoding w into numbers as follows:
\( \sum_i=1^|w| (w(i)/2^i) + 1/2^{|w|+1} \)</p><p>A network recognize a language L \in {0, 1}+ if it computes a function F: A -> R (A \in R) such
that:</p><p>\( w \in L iff δ(w) \in A and F(δ(w)) > 0 \)</p><p>where δ(w) is the code of the word
eg. w = 11001 that gives us δ(w) = 0.110011 in binary form</p><ul><li>Recurrent networks with rational weights are equivalent to Turing machines</li><li>Recurrent networks are super-Turing powerful</li></ul><h1 id=computability-overview>Computability Overview</h1><ul><li>All Boolean functions can be expressed using two-layer networks.</li><li>Two-layer networks may approximate any continuous function.</li><li>Recurrent networks are at least as strong as Turing machines</li></ul><h1 id=online-algorithm-delta-rule-widrow-hoff-rule>Online algorithm (Delta-rule, Widrow-Hoff rule)</h1><ul><li>weights in ~w(0) initialized randomly close to 0</li></ul><h1 id=complexity-of-the-batch-algorithm>Complexity of the batch algorithm</h1><p>computation of E/wji stops in time linear in the size of the network + the size of the training
set</p><p>Proof sketch: The algorithm does the following p times:</p><ul><li>forward pass</li><li>backpropagation</li><li>gradient descent</li></ul><h1 id=gradient-descent-in-large-networks>Gradient Descent in Large Networks</h1><p>assume:</p><ul><li>activation functions: &ldquo;smooth&rdquo; ReLU (softplus)</li><li>inputs ~xk of Euclidean norm equal to 1, desired values dk satisfying |dk| ∈ O(1),</li><li>the number of hidden neurons per layer sufficiently large</li><li>the learning rate constant and sufficiently small</li></ul><p>/TODO</p><p>The gradient descent converges (with high probability) to a global
minimum with zero error at linear rate.</p><h1 id=issues-in-computing-the-gradient>Issues in computing the gradient</h1><p>inexact gradient computation:</p><ul><li>Minibatch gradient is only an estimate of the true gradient</li><li>Note that the variance of the estimate is (roughly) σ/√m where m is the size of the minibatch and σ is the variance of the gradient estimate for a single training example.</li></ul><h1 id=minibatch-size>Minibatch size</h1><ul><li>Larger batches provide a more accurate estimate</li><li>Multicore architectures are usually underutilized by small batches</li><li>It is common for power of 2 batch</li><li>Small batches can offer a regularizing effect, perhaps due to the noise they add to the learning process</li></ul><h1 id=generalization>Generalization</h1><p>Generalization = ability to cope with new unseen instances</p><p>more formally:
It is typically assumed that the training set has been generated as follows:</p><p>d_kj = g_j(x_k) + \theta_kj</p><p>where gj is the &ldquo;underlying&rdquo; function corresponding to the output neuron j ∈ Y and theta is random noise.</p><blockquote><p>Methods improving generalization are called regularization methods.</p></blockquote><h1 id=regularization>Regularization</h1><h2 id=early-stopping>Early stopping</h2><h2 id=size-of-the-network>Size of the network</h2><ul><li>Too small network is not able to capture intrinsic properties of the training set.</li><li>I Large networks overfit faster.</li></ul><h2 id=feature-extraction>Feature extraction</h2><p>basically reducing dimensionality like PCA</p><h2 id=ensemble-methods>Ensemble methods</h2><h2 id=dropout>Dropout</h2><h2 id=weight-decay-and-l2-regularization>Weight decay and L2 regularization</h2><p>Generalization can be improved by removing &ldquo;unimportant&rdquo; weights.</p><p>Penalising large weights gives stronger indication about their importance</p><p>Intuition: Unimportant weights will be pushed to 0, important weights will survive the decay.</p><p>E′(~w) = E(~w) + ζ/(2ε) (~w · ~w)</p><p>Here ζ/(2ε) (~w · ~w) is the L2 regularization that penalizes large weights.</p><h1 id=maximizing-input>Maximizing INput</h1><p>A maximum image computed using gradient ascent.
We can use backprop, but instead of weights we are changing inputs.
Gives the most &ldquo;representative&rdquo; image of the class c.</p><h1 id=image-specific-saliency-maps>Image specific saliency maps</h1><ul><li>Let us fix an output neuron i and an image I0.</li><li>Rank pixels in I0 based on their influence on the value y_i(I_0)</li><li>Note that we can approximate yi locally around I0 with the linear part of the Taylor series:</li><li>for every pixel we compute how much it affects output</li><li>Heuristics: The magnitude of the derivative indicates which pixels need to be changed the least to affect the score most</li><li>we can also take max gradient from saliency map and overlay it with default image, so we can clearly see which pixels are important.</li></ul><h1 id=smoothgrad>SmoothGrad</h1><ul><li>Make a several noisy copies of image and than average saliency maps of these noisy copies.</li></ul><h1 id=occlusion>Occlusion</h1><ul><li>Systematically cover parts of the input image.</li><li>Observe the effect on the output value.</li><li>Find regions with the largest effect</li></ul><h1 id=lime>LIME</h1><p>Let us fix an image I_0 to be explained.</p><p>Outline:</p><ul><li>Consider superpixels of I0 as interpretable components.</li><li>Construct a linear model approximating the network aroung the image I0 with weights corresponding to the superpixels.</li><li>Select the superpixels with weights of large magnitude as the important ones.</li></ul><p>superpixels:</p><ul><li>Denote by P1,&mldr;, P_l all superpixels of I0.</li><li>Consider binary vectors ~x = (x1, . . . , xl) ∈ {0, 1}^l.</li><li>Each such vector x determines a &ldquo;subimage&rdquo; I<input checked disabled type=checkbox> of I_0 obtained by removing all Pk with xk = 0.</li></ul></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.77414ca1a0d130043c129098d97cecf433ce369d23de8eaa91f5111f432729db1257c49a33b38203d4be241ef53dafecd99a1d2c350b75316b55a0bb6a2e150b.js integrity="sha512-d0FMoaDRMAQ8EpCY2Xzs9DPONp0j3o6qkfURH0MnKdsSV8SaM7OCA9S+JB71Pa/s2ZodLDULdTFrVaC7ai4VCw=="></script></body></html>