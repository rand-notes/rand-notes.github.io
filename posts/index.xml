<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on idk</title><link>https://rand-notes.github.io/posts/</link><description>Recent content in Posts on idk</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 30 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://rand-notes.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Confidence Intervals</title><link>https://rand-notes.github.io/posts/fi/ma012/confidence-intervals/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/posts/fi/ma012/confidence-intervals/</guid><description>Statistical Inference is the process of drawing conclusions about populations or scientific truths from data
There are two types of statistical inferences; estimation and statistical (hypothesis) tests
Estimation Use information from the sample to estimate (or predict) the parameter of interest.
For instance, using the result of a poll about the president&amp;rsquo;s current approval rating to estimate (or predict) his or her true current approval rating nationwide.
Point Estimates</description><content type="html"><![CDATA[<h1 id="statistical-inference">Statistical Inference</h1>
<p>is the process of drawing conclusions about populations or scientific truths from data</p>
<p>There are two types of statistical inferences; estimation and statistical (hypothesis) tests</p>
<h2 id="estimation">Estimation</h2>
<blockquote>
<p>Use information from the sample to estimate (or predict) the parameter of interest.</p>
<p>For instance, using the result of a poll about the president&rsquo;s current approval rating to estimate (or predict) his or her true current approval rating nationwide.</p>
</blockquote>
<p><strong>Point Estimates</strong></p>
<p>An estimate for a parameter that is one numerical value. An example of a point estimate is the sample mean or the sample proportion.</p>
<p><strong>Interval Estimates</strong></p>
<p>Interval estimates give an interval as the estimate for a parameter. This is a new concept which is the focus of this lesson. Such intervals are built around point estimates which us why understatning
point estimates is important to understaning interval estimates.</p>
<p><strong>Confidence Interval</strong></p>
<p>An interval of values computed from sample data that is likely to cover the true parameter of interest.</p>
<h2 id="properties-of-good-estimator">Properties of Good Estimator</h2>
<ul>
<li>The center of the sampling distribution for the estimate is the same as that of the population. When this property is true, the estimate is said to be unbiased. The most often-used measure of the center
is the mean.</li>
<li>The estimate has the smallest standard error when compare to other estimators. For example, in the normal distribution, the mean and median are essentially the same. However, the standard error of the
median is about 1.25 times that of the standard error of the mean. We know the standard error of the mean is \( \frac{\sigma}{\sqrt{n}} \). This is why the mean is a better estimator than the
median when the data is normal (or approximately normal).</li>
</ul>
<blockquote>
<p>We use estimated standard error because, typically, population is not known.</p>
</blockquote>
<h2 id="general-format-of-a-confidence-interval">General Format of a Confidence Interval</h2>
<p>In putting the two properties above together, the center of our interval should be the point estimate for the parameter of interest. With the estimated standard error of the point estimate, we can
include a measure of confidence to our estimate by forming a margin of error.</p>
<p>e.g. survey where 44% of respondens approved President’s reaction has +-3.5% margin of error. We known that interval would be from 40.5% to 47.5%</p>
<blockquote>
<p><strong>General Form of a confidence interval</strong>:<br>
sample statistic +- margin of error</p>
</blockquote>
<p>The margin of error will consist of two pieces. One is the standard error of the sample statistic. The other is some multiplier, , of this standard error, based on how confident we want to be in our
estimate. This multiplier will come from the same distribution as the sampling distribution of the point estimate; for example, as we will see with the sample proportion this multiplier will come from
the standard normal distribution.</p>
<blockquote>
<p><strong>General form of the margin of error</strong><br>
Margin of error = \( M \times S^{\wedge}E(estimate) \)</p>
</blockquote>
<p>the multiplier M depends on our level of confidence</p>
<h2 id="interpretation-of-a-confidence-interval">Interpretation of a Confidence Interval</h2>
<p>The interpretation of a confidence interval has the basic template of: &ldquo;We are &lsquo;some level of percent confident&rsquo; that the &lsquo;population of interest&rsquo; is from &lsquo;lower bound to upper bound&rsquo;. The phrases in
single quotes are replaced with the specific language of the proble. We will discuss more about the interpretation of a confidence interval after we provide a few more examples.</p>
<p>Most confidence levels use ranges from 90% confidence to 99% confidence, with 95% being the most widely used. In fact, when you read a report that includes a margin of error, you can usually assume this
has a 95% confidence attached to it unless otherwise stated.</p>
<h2 id="inference-for-the-popluation-proportion">Inference for the Popluation Proportion</h2>
<h3 id="point-estimate-for-the-population-proportion">Point Estimate for the Population Proportion</h3>
<blockquote>
<p>Point Estimate of the Population Proportion</p>
<p>\( \hat{p} = \# \) of successes in the sample of size n</p>
</blockquote>
<h3 id="confidence-interval-for-the-population-proportion">Confidence Interval for the Population Proportion</h3>
<p>Recall that: if np and n(1 - p) are greater than five, then \(\hat{p}\) is approximately normal with mean, p, standard error \( \sqrt{\frac{p(1-p)}{n}} \)</p>
<p>Under these conditions, the sampling ditribution of the sample proportion, pm is approximately Normal. The multiplier used in the confidence interval will come from the Standard Normal distribution.</p>
<h3 id="construct-and-interpret-the-ci">Construct and Interpret the CI</h3>
<p>Constructing a CI (Confidence Interval) is done in 3 steps.</p>
<h4 id="check-conditions">Check Conditions</h4>
<p>Before doing the actual computations of the interval or test, it&rsquo;s important to check whether these conditions are met, otherwise the calculations and conclusions may not be correct.</p>
<blockquote>
<p>The conditions we need for inderence on a mean are random, normal, independent</p>
</blockquote>
<h5 id="random-condition">Random Condition</h5>
<blockquote>
<p>A random sample or randomized experiment should be used to obtain the data.</p>
</blockquote>
<p>Random samples give us unbiased data from a population. When we don&rsquo;t use random selection, the resulting data usually has some form of bias, so using it to infer something about the population can be
risky.</p>
<p>More specifically, sample means are unbiased estimators of their population mean. For example, suppose we have a bag of ping pong balls individually numbered from  to 30, so the population mean
of the bag is 15. We could take random samples of balls from the bag and calculate the mean from each sample. Some samples would have a mean higher than 15 and some would be lower. But on
average, the mean of each sample will equal 15. We write this property as \( \mu_{\overline{x}} = \mu \)  which holds true as long as we are taking random samples.</p>
<h5 id="normal-condition">Normal Condition</h5>
<blockquote>
<p>The sampling distribution \( \overline{x} \) (the sample mean) need to be approximately normal. This is true if our parent population is normal or if our sample is reasonably large (n &gt;= 30).</p>
</blockquote>
<p>The sampling distribution of \( \overline{x} \) (a sample mean) is approximately normal in a few different cases. The shape of the sampling distribution of \( \overline{x} \) mostly dependes on the
shape of the parent population and the sample size n.</p>
<p><strong>case 1: parent population is normally distributed</strong></p>
<p>If the parent population is normally distributed, then the sampling distribution of \( \overline{x} \) is approximately normal regardless of sample size. So if we know that the parent population is normally distributed, we pass this condition even if the sample size is small. In practice, however, we usually don&rsquo;t know if the parent population is normally distributed.</p>
<p><strong>case 2: Not normal or unknown parent population; sample size is large (n &gt; 30)</strong></p>
<p>The sampling distribution of \( \overline{x} \) is approximately normal as long as the sample size is reasonably large. Because of the CLT, when n &gt; 30, we can treat the sampling distribution \(
\overline{x} \) as approximately normal.</p>
<p>There are a few rare cases where the parent population has such an unusual shape that the sampling distribution of the sample mean \( \overline{x} \) isn&rsquo;t quite normal for sample sizes near 30. These
cases are rate, so in practice, we are usually safe to assume approximately normality in the sampling distribution when n &gt;= 30.</p>
<p><strong>case 3: Not normal or unknown parent population; sample size is small (n &lt; 30)</strong></p>
<p>As long as the parent population doesn&rsquo;t have outliers or strong skew, even smaller samples will produce a sampling distribution of \( \overline{x} \) that is approximately normal. In
practice, we can&rsquo;t usually see the shape of the parent population, but we can try to infer shape based on the distribution of data in the sample. If the data in the sample shows skew or outliers, we
should doubt that the parent is approximately normal, and so the sampling distribution of \( \overline{x} \) may not be normal either. But if the sample data are roughly symmetric and don&rsquo;t show
outliers or strong skew, we can assume that the sampling distribution of \( \overline{x} \) will be approximately normal.</p>
<p>The big idea is that we need to graph our sample data when \( n\lt 30 \), is less than, 30 and then make a decision about the normal condition based on the appearance of the sample data.</p>
<h5 id="independent-condition">Independent Condition</h5>
<blockquote>
<p>Individual observations need to be independent. If sampling without replacement, our sample size shouldn&rsquo;t be more than 10% of the population.</p>
</blockquote>
<p>To use formula for stadard deviation of \( \overline{x} \), we need individual observations to be independent. In an experiment, good design usually takes care of independence between subjects
(control, different treatments, randomization).</p>
<p>In an observational study that involves sampling without replacement, individual observations aren&rsquo;t technically independent since removing each observation changes the population. However the 10%
condition says that if we sample 1-% or less of the population, we can treat individual observations as independent since removing each observation doesn&rsquo;t change the population all that much as we
sample. For instance, if our sample size is n = 30, there should to be at least N = 300 members in the population for the sample to meet independence condition.</p>
<p>Assuming independence between observations allows us to use this formula for standard deviation of \( \overline{x} \) when we&rsquo;re making confidence intervals or doing significance tests:</p>
<p>$$ \mu_{\overline{x}} = \frac{\mu}{\sqrt{n}} $$</p>
<p>We usually don&rsquo;t know the population standard deviation \( \mu \), so we substitute the sample standard deviation \(s_x\) as an estimate for \(\mu\). When we do this, we call it the <strong>standard
error</strong> of \(\overline{x}\) to distinguish it from the standard deviation.</p>
<p>So our formula for standard error of \(\overline{x}\) is:</p>
<p>$$\mu_{\overline{x}} \approx \frac{s_x}{\sqrt{n}}$$</p>
<h4 id="construct-the-general-formm">Construct The General Formm</h4>
<p>The general form of the condifence interval is point estimate \( \pm M \times \hat{SE}(estimate) \). The point estimate is the sample proportion \( \hat{p} \), and the estimated standard error is
\(  \hat{SE}(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n} } \)</p>
<p>\( 1 - \alpha 100% \) confidence interval for the population proportion, p</p>
<p>$$ \hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} $$</p>
<p>where z_{\alpha/2} represent a z-value with \( \alpha / 2 \) area to the right of it.</p>
<p>General notes about the confidence interval&hellip;
- The \( \pm \) in the formula above means &ldquo;plus or minus&rdquo;. It&rsquo;s shorthand way of writing \( ( \hat{p} - &hellip;, \hat{p} + &hellip; ) \)
- It&rsquo;s centered at the point estimate \( \hat{p} \)
- The width of the interval is determined by the margin of error.
- You must determine the multiplier</p>
<h4 id="interpret-the-confidence-interval">Interpret the Confidence Interval</h4>
<p>We can say we are \( (1 - \alpha)100% \) confident that the population proportion is in interval \( \hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} \)</p>
<h2 id="how-to-find-the-multiplier-using-the-standard-normal-distribution">How to find the multiplier using the Standard Normal Distribution</h2>
<p>\( z_{\alpha} \) is the z-value having a tail area of \( \alpha \) to its right. With some calculation, one can use the Standard Normal Cumulative Probability Table to find the value.</p>
<h2 id="interpreting-the-ci">Interpreting the CI</h2>
<p>In the graph below, we show 10 replications (for each replication, we sample 30 students and ask them whether they are Democrats) and compute an 80% Confidence Interval each time. We are lucky in this
set of 10 replications and get exactly 8 out of 10 intervals that contain the parameter. Due to the small number of replications (only 10), it is quite possible that we get 9 out of 10 or 7 out of 10
that contain the true parameter. On the other hand, if we try it 10,000 (a large number of) times, the percentage that contains the true proportions will be very close to 80%.</p>

    <img src="/images/confidence_interval.png"  alt="confidence interval"  class="left"  />


<p>If we repeatedly draw random samples of size n from the population where the proportion of success in the population is p and calculate the confidence interval each time, we would expect that \(100(1 -
\alpha)%\) of the intervals would contain the true parameter, p.</p>
<h2 id="sample-size-computation-for-the-population-proportion-confidence-interval">Sample Size Computation for the Population Proportion Confidence Interval</h2>
<p>An important part of obtaining desired results is to get a large enough sample size. We can use what we know about the margin of error and the desired level of confidence to determine an appropriate
sample size.</p>
<p>Recall that the margin of error, E, is half of the width of the confidence interval. Therefore for a one sample proportion,</p>
<p>$$
E = z_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}
$$</p>
<p><strong>Precision</strong> - The wider the interval, the poorer the precision. Note that the higher the confidence level, the wider the width (or equivalently, half width) of the interval and thus the poorer the
precision.</p>
<h2 id="statisticalhypothesis-tests">Statistical/Hypothesis Tests</h2>
<blockquote>
<p>Use information from the sample to determine whether a certain statement about the parameter of interest is true.</p>
<p>For instance, suppose a news station claims that the President’s current approval rating is more than 75%. We want to determine whether that statement is supported by the poll data.</p>
</blockquote>
]]></content></item><item><title>ma012 lec0</title><link>https://rand-notes.github.io/fi/ma012/0/</link><pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/fi/ma012/0/</guid><description>A bell curve is a graph depicting the normal distribution
p-value A p-value is a measure of the probability that an observed difference could have occurred just by random chance. p-values are numbers in interval (0, 1) and commonly used threshold is 0.05. The lower the p-value, the greater the statistical significance of the observed difference. While a small p-value helps us decide if one group differs from another, it does not tell us how different they are.</description><content type="html"><![CDATA[<blockquote>
<p>A bell curve is a graph depicting the normal distribution</p>
</blockquote>
<h1 id="p-value">p-value</h1>
<p>A p-value is a measure of the probability that an observed difference could have occurred just
by random chance. p-values are numbers in interval (0, 1) and commonly used threshold is 0.05.
The lower the p-value, the greater the statistical significance of the observed difference.
While a small p-value helps us decide if one group differs from another, it does not tell us how
different they are.</p>
<p>e.g.<br>
two groups (A: 73, B:125) and (A: 71, B: 127) results in p-value P=0.9. This means that the
difference is too small for us to be confident about it.</p>
<hr>
<p>two groups (A: 60, B:138) and (A: 84, B: 114) - (30% vs 42%) results in p-value P=0.01, thus we
can say that these groups differs.</p>
<p>However it might be just an coincidence, if we get a small p-value when there is no difference,
it&rsquo;s called a <strong>False Positive</strong></p>
<hr>
<p>If we need to be really sure the results are correct, we use smaller threshold. e.g. using a
threshold 0.00001 means that we would get a False Positive only once every 100 000 experiments.</p>
<h1 id="central-limit-theorem">Central Limit Theorem</h1>
<p>The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size gets larger, regardless of the population&rsquo;s distribution.</p>
<h1 id="population-distribution">Population Distribution</h1>
<p>The population is the whole set of values, we are interested in.</p>
<h1 id="sample-distribution">Sample Distribution</h1>
<p>The sample is a subset of the population, and the set of values you actually use in your
estimation.</p>
<h1 id="sampling-distribution">Sampling Distribution</h1>
<p>Researchers often use a sample to draw inferences about the population that sample is from. To
do that, they make use of a probability distribution that is very important in the world of
statistics: the sampling distribution.</p>
<h1 id="descriptive-statistics">Descriptive statistics</h1>
<ul>
<li>summarizes sample data via summarization techniques and indexes</li>
<li>frequency tables, moments (mean, variance, skewness, kurtoisis) and quantiles (median, quartiles, interquartiles spans), contingency tables, correlation coefficients</li>
</ul>
<h1 id="frequency-distribution-ie-frequency-listtable">Frequency distribution i.e. frequency list/table</h1>
<p>is a list, table or graph that display the frequency of various outcomes in a sample</p>
<h1 id="exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</h1>
<ul>
<li>analysis of sample data, usually via visualization methods</li>
<li>frequency plots, boxplots, histograms, scatter plots, PCA</li>
</ul>
<h1 id="statistical-inference">Statistical Inference</h1>
<ul>
<li>derives probability properties (arguments, distribution) of population based on analysis of sample data</li>
<li>requires a model - establishing a assumptions about population and data sample</li>
<li>point and interval parameter estimates (confidence intervals), testing statistics hypothesis, predictions, classifications, clustering</li>
</ul>
<h1 id="parametric-methods">Parametric methods</h1>
<ul>
<li>model establish probability distribution or some set, parameters are being examined via these probability distributions</li>
<li>most of classic methods like t-test, linear regression model, Multivariate regression</li>
</ul>
<h1 id="nonparametric-methods">Nonparametric methods</h1>
<p>or distribution-free statistical methods do not depend on having a normal distribution and can be used with skewed data or with categorical data (Spearman&rsquo;s rho, Mann-Whitney, Wilcoxon Test)</p>
<h1 id="semiparametric-methods">Semiparametric methods</h1>
<ul>
<li>combination of parametric and nonparametric mothods</li>
<li>Cox model of proportional risks in survival analysis</li>
</ul>
<h1 id="nominal-data-categorical">Nominal data (categorical)</h1>
<p>distinguishes different categories for qualitative variables;
dummy code: gender, ethnic background
R: factor</p>
<h1 id="ordinal-data">Ordinal data</h1>
<p>data exists in categories that are ordered but differences cannot be determined or they are meaningless. (Example: 1st, 2nd, 3rd)
R: ordered factor</p>
<h1 id="interval-data">Interval data</h1>
<p>Differences between values can be found, but there is no absolute 0. (Temp. and Time),
R: numeric</p>
<h1 id="ratio-data">Ratio Data</h1>
<p>data with an absolute 0. Ratios are meaningful. (Length, Width, Weight, Distance)</p>
<h1 id="null-hypothesis-h0">Null Hypothesis (H0)</h1>
<p>a statement that the performance of treatment groups is so similar that the groups must belong to the same population; a way of saying that the experimental manipulation had no important effect</p>
<ul>
<li>all level factors have same mean values</li>
</ul>
<h1 id="alternative-hypothesis-h1">Alternative hypothesis (H1)</h1>
<p>The hypothesis that states there is a difference between two or more sets of data.</p>
<ul>
<li>at least one pair of level factors differ in mean value</li>
</ul>
<h1 id="significance-level-alpha">significance level (alpha)</h1>
<p>The acceptable level of error selected by the researcher, usually set at 0.05;
what we compare our p-value to</p>
<h1 id="power-of-a-test">Power of a test</h1>
<ul>
<li>The probability of correctly detecting a false null hypothesis</li>
<li>could be increased by increasing sample data size</li>
</ul>
<h1 id="classical-statistical-hypothesis-testing-according-to-critical-field">classical statistical hypothesis testing; according to critical field</h1>
<ul>
<li>establish H0 and H1</li>
<li>establish model - i.e. statistics assumptions</li>
<li>choose test and testing statistics T with known probability distribution (under the validity of H0)</li>
<li>choose significance level (α) - usually 0.05</li>
<li>compute observed values <em>t</em> of testing statistics <em>T</em></li>
<li>establish critical region <em>W</em> according to <em>H1</em> and <em>t</em></li>
<li>H0 is rejected, if $$t \in W$$</li>
</ul>
<h1 id="statistical-hypothesis-testing-via-p-value">statistical hypothesis testing; via p-value</h1>
<ul>
<li>compute p-value <em>p</em></li>
<li>H0 is rejected, if $$p \lt \alpha$$</li>
</ul>
<h1 id="p-value-1">p-value</h1>
<p>The probability level which forms basis for deciding if results are statistically significant (not due to chance).</p>
<h1 id="t-test">t-test</h1>
<p>a statistical test used to evaluate the size and significance of the difference between two means</p>
<h1 id="multiple-testing">multiple testing</h1>
<p>the more statistical tests we do, the more likely we are to make an erroneous inference (unless we adjust our significance threshold).</p>
<h1 id="anova-analysis-of-variance">ANOVA (analysis of variance)</h1>
<p>-differences among MEANS of continuous (numerical) variables
-more flexible than t-tests&ndash;&gt;can analyze differences among MORE THAN 2 groups (even if diff sample sizes)</p>
<h1 id="multiple-comparison-tests">multiple comparison tests</h1>
<p>statistical tests used to make pairwise comparisons to find which means differ significantly from one another in a one-factor multilevel design</p>
<ul>
<li>Tukey or Scheffe method</li>
</ul>
<h1 id="tukeys-method">Tukey&rsquo;s method</h1>
<p>it&rsquo;s preferred especially in case that all partial random selections have approximately same range</p>
<h1 id="scheffe-method">Scheffe method</h1>
<p>it&rsquo;s recommended especially in case that partial random selections have distinctly different ranges.</p>
<h1 id="the-means--minimalni-nulovy-model">The means / minimalni (nulovy) model</h1>
<p>$$Y_{i,j} = \mu + \epsilon_{i,j}$$</p>
<ul>
<li>\mu (micro, u) - grand mean of Y, same for all levels of factor A</li>
<li>eps - stochaistic independent random variables with probability distribution N(0, \o^2)</li>
</ul>
<h1 id="the-effect-model--jednoducheho-trideni">The effect model / jednoducheho trideni</h1>
<p>$$Y_{i,j} = \mu + \alpha + \epsilon_{i,j}$$</p>
<ul>
<li>alpha - effect of i-th level of factor A</li>
</ul>
<h1 id="grand-mean--spolecna-stredni-hodnota">Grand mean / spolecna stredni hodnota</h1>
<p>$$M: \mu^{\wedge} = Y^{\wedge}$$</p>
<h1 id="effect-i-th-category-in-model-m">effect i-th category in model M</h1>
<p>$$\alpha_{i}^{\wedge} = \overline{Y}_{i.} \overline{Y}_{..}$$</p>
<h1 id="mean-value-of-i-th-category-in-model-m">mean value of i-th category in model M</h1>
<p>$$\mu_{i}^{\wedge} = \mu^{\wedge} + \alpha^{\wedge}_{i} = \overline{Y}_{i.}$$</p>
<h1 id="common-part-of-mean-value-in-submodel-m_0">common part of mean value in submodel M_0</h1>
<p>$$\mu^{\wedge} = \overline{Y}$$</p>
<h1 id="square-sums-describing-variability">Square sums describing variability</h1>
<ul>
<li>Total sum of squares , SS / celkovy soucet ctvercu</li>
<li>Between-groups SS / skupinovy soucet ctvercu</li>
<li>Within-groups/error/residual SS / rezidualni soucet ctvercu</li>
</ul>
<h1 id="parameters-estimates-in-models">Parameters estimates in models</h1>
<ul>
<li>Gran mean / spolecna stredni hodnota</li>
<li>effect i-th category in model M</li>
<li>mean value of i-th category in model M</li>
<li>common part of mean value in submodel M_0</li>
</ul>
]]></content></item><item><title>IV111 lec 03</title><link>https://rand-notes.github.io/iv111/0/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/iv111/0/</guid><description>Expectation of Random Variables Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists.</description><content type="html"><![CDATA[<h1 id="expectation-of-random-variables">Expectation of Random Variables</h1>
<ul>
<li>Often we need a shorter description than PMF or CDF - single number, or a few number.</li>
<li>First such characteristic describing a random variable is the <strong>expectation</strong>, also known as the <strong>mean value</strong></li>
</ul>
<p>Expectation of a random varialbe X is defined as:</p>
<p>$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$</p>
<p>provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists. In case the sum is not convergent, the expectation has
not meaning.</p>
<p>for continuous variable:</p>
<p>$$E(X) = \int x \cdot f_X (x) dx$$</p>
<h2 id="discrete-uniform-probability-distribution">Discrete Uniform Probability Distribution</h2>
<p>e.g. value on 6-sided die</p>
<p>Let
\(lm(X) = {x_1, x_2,&hellip;, x_n}\)</p>
<p>The expectation of X is:
$$E(X) = \sum_{x\in lm(X)} x\cdot P(X=x) = \sum_{x\in lm(X)} x\cdot\frac{1}{n} = \frac{1}{n} \sum^n_{i=1} x_i $$</p>
<h2 id="bernoulli-probability-distribution">Bernoulli Probability Distribution</h2>
<p>Bernoulli probability distribution has one parameter p and models single Bernoulli trial or (biased) coin toss.</p>
<p>the distribution is given by:</p>
<p>$$p_X(x) = P(X = 0) = 1 -p$$
$$p_X(x) = P(X = 0) = p$$</p>
<p>the expectation is:</p>
<p>$$E(X) = 0 \cdot (1 - p) + 1 \cdot p = p$$</p>
<h2 id="indicator-random-variable">Indicator Random Variable</h2>
<p>Indicator random variable of an event A.</p>
<p>the indicator of an event A is the random variable I_A defined by:</p>
<p>\( I_A(s) = 1\) if \(s \in A \)</p>
<p>and</p>
<p>\( I_A(s) = 0\)if \(s \notin A\)</p>
<p>The probability distribution is:</p>
<p>$$p_{I_A}(0) = P(\overline{A}) = 1 - P(A)$$
$$p_{I_A}(1) = P(A)</p>
<p>the distribution function reads</p>
<p>$$
F_{I_A}(x) =
\begin{cases}
\displaylines{0 \hskip 1em \text{for} x \lt 0 \\ P(\overline{A}) \hskip 1em \text{for} 0 \leq x \geq 1) \\ 1 \hskip 1em \text{for} x \geq 1}
\end{cases}
$$</p>
<p>It is a Bernoulli distribution, and so \( E(I_A) = P(A) \)</p>
<h2 id="constant-random-variable">Constant Random Variable</h2>
<p>For \(c \in \mathbb{R}\) the function defined for all \(s \in S \text{ by } X(s) = c\)</p>
<p>The probability distribution of this variable is:</p>
<p>$$
p_X(x) =
\begin{cases}
\displaylines{1 \hskip 1em \text{ if } x = c \\ 0 \hskip 1em \text{otherwise}}
\end{cases}
$$</p>
<p>The cumulative distribution function is:</p>
<p>$$
F_X(x) =
\begin{cases}
\displaylines{0 \hskip 1em \text{ for } x \lt c \\ 1 \hskip 1em \text{for} x \geq c}
\end{cases}
$$</p>
<p>The expectation:</p>
<p>$$E(X) = 1 \cdot c = c$$</p>
<h2 id="geometric-probability-distribution">Geometric Probability Distribution</h2>
<p>Geometric probability distribution has one parameter p and models the number of Bernoulli trials until the first &lsquo;success&rsquo; occurs.</p>
<p>The probability function</p>
<p>$$p(x) = P(X = x) = (1 - p)^{x-1}p$$</p>
<p>The probability distribution function:</p>
<p>$$F(x) = P(X \leq x) = 1 - (1 - p)^{x}p$$</p>
<p>The expectation is</p>
<p>$$E(X) = \sum_{x \in lm(X)} x \cdot P(X = x)$$</p>
<h2 id="binomial-probability-distribution">Binomial Probability Distribution</h2>
<p>Binomial random variable X, denoted by B(n, p)</p>
<p>The probability distribution is:</p>
<p>$$
p_X(x) = P(x = x) =
\begin{cases}
\displaylines{\binom{n}{x} p^x (1 - p)^{n-x} \hskip 1em \text{for } 0 \leq x \geq n \\ 0 \hskip 5em \text{otherwise}}
\end{cases}
$$</p>
<p>the expectation is</p>
<p>$$E(X) = \sum_{x \in lm(X)} x \cdot P(X = x) = &hellip; = np$$</p>
<p>or n independent Bernoulli trials = np. To use the easiest way, we need to define independence on random variables. Hence, we need more random variables.</p>
<h1 id="functions-of-random-variables">Functions of Random Variables</h1>
<h2 id="sum-of-independent-random-variables">Sum of Independent Random Variables</h2>
<p>Let X and U be independent random variables. Let \( Z = X + Y \) is a new random variable defined as sum of X and Y. If X and Y are non-negative integer values, the probability distribution of Z is</p>
<p>$$ p_Z(t) = p_{X+Y}(t) = \sum^t_{x=0} p_X(x) p_Y(t-x)$$</p>
<blockquote>
<p>Observe that</p>
<ul>
<li>Sum of two Bernoulli distributions with probability of success p is a binomial distribution B(2, p)</li>
<li>Sum of two binomial distributions B(n_1, p) and B(n_2, p) is a binomial distribution B(n_1 + n_2, p)</li>
</ul>
</blockquote>
<h3 id="sums-of-distributions">Sums of Distributions</h3>
<p>Bernoulli + Bernoulli = Binomial
Binomial + Binomial = Binomial
Geometric + Geometric = Negative Binomial
Negative Binomial + Negative Binomial = Negative Binomial
Poisson + Poisson = Poisson
Exponential + Exponential = Erlang
Erlang + Erlang = Erlang</p>
<h2 id="negative-binomial-distribution">Negative Binomial Distribution</h2>
<p>Negative binomial distribution has two parameters p and r and expresses the number of Bernoulli trials to the r th success. Hence, it is a convolution of r geometric distributions.</p>
<p>The probability function
$$
p_X(x) = P(X = x) =
\begin{cases}
\displaylines{\binom{x - 1}{r - 1} p^r (1 - p)^{x - r} \hskip 1em \text{ for } x \geq r \\ 0 \hskip 2em \text{otherwise}}
\end{cases}
$$</p>
<p>The expectation is
$$
E(X) = \sum_{x \in lm(x)} xP(X = x) =
$$</p>
<p>The probability distribution - pr for r successes, \( (1 −p)^{x −r} \) failures, the last is successs, hence, \( \binom{x −1}{r −1} \) stands for placement of x − 1 successes in r − 1 trials.</p>
<h2 id="linearity-of-expectation">Linearity of Expectation</h2>
<h3 id="theorem-expecation-of-sum">Theorem (Expecation of sum)</h3>
<p>Let X and Y be random variables. Then</p>
<p>$$E(X + Y) = E(X) + E(Y)$$</p>
<p>//TODO: proof</p>
<h3 id="theorem-multiplication-by-a-constant">Theorem (Multiplication by a constant)</h3>
<p>Let X be random variable and c be a real number. Then
$$E(cX) = cE(X)$$</p>
<p>In general, the above theorem implies the following result for any linear combination of n random variables i.e.</p>
<h3 id="corollary-linearity-of-expectation">Corollary (Linearity of expectation)</h3>
<p>Let \( X_1, X_2,&hellip;, X_n \) be random variables and \(c_1,c_2,&hellip;,c_n \in \mathbb{R} \). Then</p>
<p>$$
E(\sum^n_{i=1}c_i X_i) = \sum^n_{i=1} c_i E(X_i)
$$</p>
<h2 id="expectation-of-independent-random-variables">Expectation of Independent Random Variables</h2>
<h3 id="theorem">Theorem</h3>
<p>If X and Y are independent random variables, then:</p>
<p>$$
E(XY) = E(X) E(Y)
$$</p>
<p>//TODO: proof
//TODO: observation</p>
<h1 id="conditional-expectation">Conditional Expectation</h1>
<p>Using the derivation of conditional probability of two events, we can derive conditional probability of (a pair of) random variables.</p>
<h2 id="definition">Definition</h2>
<p>The conditional probability distribution of random variable X given random variable Y (where \( p_{X ,Y} (x , y) \) is their joint distribution) is</p>
<p>p_{X|Y}(x|y) = P(X = x|Y = y) = \frac{P(X = x,Y = y)}{P(Y = y)} = \frac{p_{X, Y}(x, y)}{p_Y(y)}</p>
<p>provided the marginal probability \( p_Y(y) \neq 0 \)</p>
<hr>
<p>We may consider Y|(X = x) to be a new random variable that is given by the conditional probability distribution \( p_{Y|X} \). therefore, we can define its mean.</p>
<h2 id="definition-1">Definition</h2>
<p>The <strong>conditional expectation</strong> of Y given X = x is defined</p>
<p>$$
E(Y|X = x) = \sum_y yP(Y = y|X = x) = \sum_y yp_{Y|X}(y|x)
$$</p>
<p>We can derive the expectation of Y from the conditional expectations.</p>
<h2 id="theorem-of-total-expectation">Theorem of total expectation</h2>
<p>Let X and Y be random variables, then</p>
<p>$$
E(Y) = \sum E(Y|X = x)p_X(x)
$$</p>
<p>//TODO random sums example</p>
<h1 id="markov-inequality">Markov Inequality</h1>
<p>It is important to derive as much information as possible even from a partial description of random variable. The mean value already gives more information than one might expect, as captured by Markov
inequality.</p>
<h2 id="theorem-markov-inequality">Theorem (Markov inequality)</h2>
<p>Let X be a nonnegative random variable with finite mean value E (X ). Then for all t &gt;0 it holds that</p>
<p>$$
P(X \geq t) \leq \frac{E(X)}{t}
$$</p>
<p>//TODO: examples</p>
<h2 id="proof">Proof</h2>
<p>Let t &gt; 0. We define a random variable \( Y_t \) (for fixed t ) as</p>
<p>$$
Y_t =
\begin{cases}
\displaylines{0 \hskip 1em \text{ if } X \lt t \\ t \hskip 1em X \geq t}
\end{cases}
$$</p>
<p>Then \( Y_t \) is a discrete random variable with probability distribution \( p_{Y_{y}}(x) = P(X \lt t), p_{Y_{t}}(t) = P(X \geq t) \) . We have</p>
<p>$$
E(Y_t) = tP(X \geq t)
$$</p>
<p>The observation \( X \geq Y_t \) gives</p>
<p>$$
E(X) \geq E(Y_y) = tP(X \geq t)
$$</p>
<p>what is the Markov Inequality</p>
]]></content></item><item><title>ai 0</title><link>https://rand-notes.github.io/iv126/0/</link><pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/iv126/0/</guid><description>Environments Fully Observable vs Partially Observable If an agent&amp;rsquo;s sensors give it access to the complete state of the environment at each point in time, then we say that he task environment is fully observable. If the agents has no sensors at all then the environment is unobservable
Single-Agent vs Multi-Agent Environments For example, an agent solving a crossword puzzle by itself is clearly in a single-agent enviroment, whereas an agent playing chess is in a two-agent enviroment.</description><content type="html"><![CDATA[<h1 id="environments">Environments</h1>
<h2 id="fully-observable-vs-partially-observable">Fully Observable vs Partially Observable</h2>
<p>If an agent&rsquo;s sensors give it access to the complete state of the environment at each point in time, then we say that he task environment is fully observable. If the agents has no sensors at all then the
environment is <strong>unobservable</strong></p>
<h2 id="single-agent-vs-multi-agent-environments">Single-Agent vs Multi-Agent Environments</h2>
<p>For example, an agent solving a crossword puzzle by itself is clearly in a single-agent enviroment, whereas an agent playing chess is in a two-agent enviroment. Sometimes the border is blurred. The key
is performance measure whose value depends on agent A behavior. For example in chess, the opponent V is trying to maximize its performance measure, which, by the rules of chess, minimizes agent A&rsquo;s
performance measure. This, chess is a <strong>competitive</strong> multiagent environment. On the other hand, in the taxi-driving environment, avoiding collisions maximizes the performance measure of all agents, so its
is a partially <strong>cooperative</strong> multiagent environment.</p>
<h2 id="deterministic-vs-nondeterministic">Deterministic vs Nondeterministic</h2>
<p>If the next state of the environment is completely determined by the current state and the action executed by the agent(s), then we say the environment is deterministic; otherwise, it is
nondeterministic.</p>
<p>If the environment is parially observable, however, then it could appear to be nondeterministic. Most real situations are so complex that it is impossible to keep track of all the unobserved aspects; for
practical purposes, they msut be treated as nondeterministic.</p>
<p><strong>We say that a model of the environment is stochastic if it explicitly deals with probabilities.</strong></p>
<blockquote>
<p>Stochastic != Nondeterministic</p>
</blockquote>
<h2 id="episodic-cs-sequential">Episodic cs Sequential</h2>
<p>In an episodic task environment, the agent&rsquo;s experience is divided into atomic eppisodes. In each episode the agent receives a percept and then performs a single action. Crucially, the next episode does not depend on the actions taken in previous episodes. In sequential environments, on the other hand, the current decision could affect all future decisions.</p>
<p>Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences. On the other hand, detecting defected item on assembly line doesn&rsquo;t affect whether the next part
is defective so it&rsquo;s episodic.</p>
<p>Episodic environments are much simpler than sequential environments because the agent does not need to think ahead.</p>
<h2 id="static-vs-dynamic">Static vs Dynamic</h2>
<p>If the environment can change while an agent is deliberating, then we say the environment is dynamic for that agent; otherwise, it is static. Static environments are easier to deal with because the agent
need not keep looking at the world while it is deciding on an action. Dynamic environments, on the other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet, that counts
as deciding to do nothing. If the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is <strong>semidynamic</strong>.</p>
<p>Taxi driving is clearly dynamic: the other cars and the taxi itself keep movingwhile the driving algorithm dithers about what to do next. Chess, when played with a clock,is semidynamic. Crossword puzzles are static.</p>
<h2 id="discrete-vs-continuous">Discrete vs Continuous</h2>
<p>The discrete/continuous distinction applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agent.</p>
<h2 id="known-vs-unknown">Known vs Unknown</h2>
<p>Strictly speaking, this distinction refers not to the environment itself but to the agent&rsquo;s (or designer&rsquo;s) state of knowledge about the &ldquo;laws of physics&rdquo; of the environment. In a known environment, the
outcomes (or outcome probabilities if the environment is nondeterministic) for all actions are given. Obviously, if the environment is unknown, the agent will have to learn how it works in order to make
good decisions.</p>
<p>The distinction between known and unknown environments is not the same as the one between fully and partially observable environments. It is quite possible for a known
environment to be partially observable—for example, in solitaire card games, I know the rules but am still unable to see the cards that have not yet been turned over.</p>
<p>Conversely, an unknown environment can be fully observable—in a new video game, the screen may show the entire game state but I still don’t know what the buttons do until I try them.</p>
<h1 id="the-structure-of-agents">The Structure of Agents</h1>
<p>The job of AI is to design an agent program that implements the agent function—the mapping from percepts to actions. We assume this program will run on some sort of computing device with physical sensors and actuators—we call this the agent architecture:</p>
<blockquote>
<p>agent = architecture + program</p>
</blockquote>
<h2 id="simple-reflex-agents">Simple Reflex Agents</h2>
<p>The simplest kind of agent that select actions on the basis of the current percept, ignoring the rest of the percept history.</p>
<p>Use <strong>condition-action rule</strong> &ndash; <code>**if** car-in-front-is-braking **then** initiate-braking</code></p>
<p>Sometimes we can create rules that will create infinite fail loop, in these cases we can <strong>randomize</strong> agents action.</p>
<h2 id="model-based-reflex-agents">Model-based Reflex Agents</h2>
<p>The most effective way to handle partial observability is for the agent to keep track of the part of the world it can&rsquo;t see now. That is, the agent should maintain some sort of <strong>internal state</strong> that
depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state.</p>
<p>Updating this internal state information as time goes by requires two kinds of knowledge to be encoded in the agent program in some form. First, we need some information about how the world changes over
time, which can be divided roughly into two parts: the effects of the agent’s actions and how the world evolves independently of the agent. i.e. if agent turns right the car goes right; if raining the
cameras get wet. This knowledge about world is called a <strong>transition model</strong>.</p>
<p>Second, we need some information about how the state of the world is reflected in the agent’s percepts. For example, when the car in front initiates braking, one or more illuminated red regions appear in
the forward-facing camera image. This kind of knowledge is called <strong>sensor model</strong>.</p>
<p>Together, the transition model and sensor model allow an agent to keep track of the state of the world—to the extent possible given the limitations of the agent’s sensors. An agent that uses such models
is called a <strong>model-based agent</strong>.</p>
<h2 id="goal-based-agents">Goal-based Agents</h2>
<p>Knowing something about the current state of the environment is not always enough to decide what to do. For example, at a road junction, the taxi can turn left, turn right, or go straight on. The correct
decision depends on where the taxi is trying to get to and for that agent needs some sort of <strong>goal</strong> information (i.e. being at final destination).</p>
<p>The agent program can combine this with the model (the same information as was used in the model-based reflex agent) to choose actions that achieve the goal.</p>
<p>Sometimes goal-based action selection is straightforward—for example, when goal satisfaction results immediately from a single action. Sometimes it will be more tricky—for example, when the agent has to
consider long sequences of twists and turns in order to find a way to achieve the goal. <strong>Search</strong> and <strong>Planning</strong> are the subfields of AI devoted to finding action sequences that  achieve the agent&rsquo;s
goals.</p>
<h2 id="utility-based-agents">Utility-based Agents</h2>
<p>Goals alone are not enough to generate high-quality behavior in most environments. For example, many action sequences will get the taxi to its destination (thereby achieving the goal), but some are
quicker, safer, more reliable, or cheaper than others. For this we use term <strong>utility</strong>-the quality of being useful; therefore how happy the agent is. An agent’s utility function is essentially an
internalization of the performance measure. Provided that the internal utility function and the external performance measure are in agreement, an agent that chooses actions to maximize its utility
will be rational according to the external performance measure.</p>
<p>Technically speaking, a rational utility-based agent chooses the action that maximizes the <strong>expected utility</strong> of the action outcomes—that is, the utility the agent expects to derive, on average, given the
probabilities and utilities of each outcome.</p>
<h1 id="summary">Summary</h1>
<ul>
<li>
<p>An agent is something that perceives and acts in an environment. The agent function for an agent specifies the action taken by the agent in response to any percept sequence.</p>
</li>
<li>
<p>The performance measure evaluates the behavior of the agent in an environment. A rational agent acts so as to maximize the expected value of the performance measure, given the percept sequence it has
seen so far.</p>
</li>
<li>
<p>A task environment specification includes the performance measure, the external environment, the actuators, and the sensors. In designing an agent, the first step must always be to specify the task
environment as fully as possible.</p>
</li>
<li>
<p>Task environments vary along several significant dimensions. They can be fully or partially observable, single-agent or multiagent, deterministic or nondeterministic, episodic or sequential, static or
dynamic, discrete or continuous, and known or unknown.</p>
</li>
<li>
<p>In cases where the performance measure is unknown or hard to specify correctly, there is a significant risk of the agent optimizing the wrong objective. In such cases the agent design should reflect
uncertainty about the true objective.</p>
</li>
<li>
<p>The agent program implements the agent function. There exists a variety of basic agent program designs reflecting the kind of information made explicit and used in the decision process. The designs
vary in efficiency, compactness, and flexibility. The appropriate design of the agent program depends on the nature of the environment.</p>
</li>
<li>
<p>Simple reflex agents respond directly to percepts, whereas model-based reflex agents maintain internal state to track aspects of the world that are not evident in the current percept. Goal-based agents
act to achieve their goals, and utility-based agents try to maximize their own expected “happiness.”</p>
</li>
<li>
<p>All agents can improve their performance through learning.</p>
</li>
</ul>

    <img src="/images/agents.png"  alt="Agents"  class="center"  />


]]></content></item><item><title>cpp</title><link>https://rand-notes.github.io/posts/cpp/2021-01-01-cpp/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/posts/cpp/2021-01-01-cpp/</guid><description/><content type="html"></content></item><item><title>DL0</title><link>https://rand-notes.github.io/dl0/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl0/</guid><description>TOC Underflow
Overflow
Softmax
Problems Condition number
Gradient-Based Optimization objective function; criterio; cost function; loss function; Gradient Descent; critical points; stationary points; local minumum; local maximum; Saddle points; global minimum; partial derivatives; gradient; directional derivative; method of steepest descent; gradient descent; Chain rule; Steepest descent proposes a new point; learning rate; line search; hill climbing; Jacobian matrix; second derivative; Hessian matrix; Jacobian; Hessian; second derivative test; Newton&amp;rsquo;s method; first-order optimization algorithms; second-order optimization algorithms; Lipschitz continous; Lipschitz constant; convex optimization;</description><content type="html"><![CDATA[<h1 id="toc">TOC</h1>
<ul>
<li>
<p><a href="#underflow">Underflow</a></p>
</li>
<li>
<p><a href="#overflow">Overflow</a></p>
</li>
<li>
<p><a href="#softmax-">Softmax </a></p>
<ul>
<li><a href="#problems">Problems</a></li>
</ul>
</li>
<li>
<p><a href="#condition-number">Condition number</a></p>
</li>
<li>
<p><a href="#gradient-based-optimization">Gradient-Based Optimization</a>
<strong>objective function</strong>; <strong>criterio</strong>; <strong>cost function</strong>; <strong>loss function</strong>; <strong>Gradient Descent</strong>; <strong>critical points</strong>; <strong>stationary points</strong>; <strong>local minumum</strong>; <strong>local maximum</strong>; <strong>Saddle points</strong>; <strong>global minimum</strong>; <strong>partial derivatives</strong>; <strong>gradient</strong>; <strong>directional derivative</strong>; <strong>method of steepest descent</strong>; <strong>gradient descent</strong>; <strong>Chain rule</strong>; <strong>Steepest descent proposes a new point</strong>; <strong>learning rate</strong>; <strong>line search</strong>; <strong>hill climbing</strong>; <strong>Jacobian matrix</strong>; <strong>second derivative</strong>; <strong>Hessian matrix</strong>; <strong>Jacobian</strong>; <strong>Hessian</strong>; <strong>second derivative test</strong>; <strong>Newton&rsquo;s method</strong>; <strong>first-order optimization algorithms</strong>; <strong>second-order optimization algorithms</strong>; <strong>Lipschitz continous</strong>; <strong>Lipschitz constant</strong>; <strong>convex optimization</strong>;</p>
</li>
<li>
<p><a href="#constrained-optimization">Constrained Optimization</a>
<strong>constrained optimization</strong>; <strong>feasible points</strong>; <strong>Karush-Kuhn-Tucker</strong>; <strong>generalized Lagrangian</strong>; <strong>equality constraints</strong>; <strong>inequalities constraints</strong>;</p>
</li>
</ul>
<h1 id="underflow">Underflow</h1>
<p>happens when small numbers are rounded to 0.</p>
<h1 id="overflow">Overflow</h1>
<p>happens when numbers with large magnitude are approximated as inf of -inf.</p>
<h1 id="softmax">Softmax</h1>
<p>also called softargmax or normalized exponential function.</p>
<p>basically it&rsquo;s function that turn real values into probabilities.
input is vector of K real numbers and ouput is a probability distribution of K probabilities.</p>
<p><code>softmax(x)_i = exp(x_i) / \sum^n_j=1(exp(x_j))</code></p>
<p>softmax is vulnerable to both underflow and overflow.</p>
<h2 id="problems">Problems</h2>
<p>if all x_i are equal to some constant c then output should be n probabilities with 1/n value. Numerically this may not occur when c has large magnitude.
If c is very negative then exp(c) will underflow and denominator will be 0.
If c is very large then exp(c) will overflow causing numerator to be inf.</p>
<p>Both of these difficulties can be resolved by instead evaluating softmax(z) where <code>z = x - max_i * x_i</code>. Normalization of vector results in the largest argument to exp being 0, which rules out the
possibility of overflow.</p>
<p>But there is still one small problem. Underflow in the numerator can still cause the expression as a whole to evaluate to zero and if we are using <code>log softmax(x)</code> we will get -inf.  So we need to
stabilize log max with same trick as above.</p>
<h1 id="condition-number">Condition number</h1>
<p>ratio of magnitude of the largest and smallest eigenvalue.</p>
<p><code>max_i,j |\lambda_i / \lambda_j|</code></p>
<p>When this number is large, matrix inversion is particularly sensitive to error in the input.
Conditioning refers to how rapidly a function changes with respect to small changes in its input.</p>
<h1 id="gradient-based-optimization">Gradient-Based Optimization</h1>
<p>Optimization refers to the task of either minimizing or maximizing some function f(x) by altering x.
We usually phrase most optimization problems in terms of minimizing. Maximization may be accomplished via a minimization by minimizing -f(x).</p>
<p>The function we want to minimize or maximize is called the <strong>objective function</strong> or <strong>criterio</strong>. When we are minimizing it, we may also call it the <strong>cost function</strong>, <strong>loss function</strong> or <strong>error
function</strong>.</p>
<p>We often denote the value that minimizes or maximizes a function with a superscript ∗ (asterisk) &ndash; it&rsquo;s not mutiplication operator * but slightly different star.
eg we may write: <code>x^* = arg min f(x)</code></p>
<p><strong>Gradient Descent</strong> is technique that move x in small steps with oposite sign of the derivate in order to find minimum of function.</p>
<p>Points with 0 derivation are known as <strong>critical points</strong> or <strong>stationary points</strong>. Gradient is the zero vector at a point if and only if it is a stationary point.</p>
<p>A <strong>local minumum</strong> is a point where f(x) is lower than at all neigboring points. A <strong>local maximum</strong> is opposite. Some critical points are neither maximum nor minimum, these are known as <strong>Saddle points</strong>
A point that obtains the absolute lowest value of f(x) is a <strong>global minimum</strong>. There could be one or multiple global minima.</p>
<p>Function that have multiple local minima and saddle points surroundedd by very flat regions makes optimization very difficult, especially when the input to the function is multidimensional. We therfore
usually settle for finding a value of f that is very low, but not neccessarily minimal in any formal sense.</p>
<p>For functions with multiple inputs we use <strong>partial derivatives</strong>. Partial derivative is derivation of one variable, whereas other variables are treated like constants.</p>
<p>The <strong>gradient</strong> generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of f is the vector containing all of the partial derivatives, denotes
<code>∇_x f(x)</code>. Element i of the gradient is the partial derivative of f with respect to x_i. In multiple dimensions, critical points are points where every element of the gradient is equal to zero.</p>
<p>where ∇ is nabla. Nabla can be interpreted as a vector of partial derivative operators.</p>
<p>Gradient: <code>grad f =  ∇ f</code>
Divergence: <code>div v =  ∇ * v</code>
Curl: <code>curl v =  ∇ x v</code></p>
<p><code>∇ = \sum e_i * ∂/∂x_i</code></p>
<p>∂ is symbol for partial derivative (it&rsquo;s stylized cursive of letter d).</p>
<hr>
<p>The <strong>directional derivative</strong> in direction u (a unit vector) is the slope of the function f in direction u.
To minimize f, we would like to find the direction in which f decreases the fastest. We can do this using the directional derivative:</p>
<p>Final form of equation obtained via chain rule: <code>min u^T ∇_x f(x) = min ||u||_2||∇_x f(x)||_2 cos θ</code>
where θ is angle between u and the gradient. Substituing in ||u||_2 = 1 and ingoring dactors that do not depend on u, this simplifies to <code>min_u cos θ</code>. This is minimized when u points in the oppostite
direction as the the gradient. In the other words, the gradient points directly uphill, and the negative gradient points directly downhill. We can decrease f by moving in the direction of the negative
gradient, This is known as the <strong>method of steepest descent</strong> or <strong>gradient descent</strong></p>
<p><strong>Chain rule</strong> - rule about derivation of composite function (f ∘ g).
<code>(f ∘ g)' = (f'∘ g) * g'</code></p>
<p>Alternatively, by letting <code>h = (f'∘ g)</code> which equivalent to <code>h(x) = f(g(x))</code> for all x, one can also write the chain rule in Lagrange&rsquo;s notation as follows:
<code>h'(x) = f'(g(x))g'(x) </code></p>
<p>The chain rule can be also rewritten in Leibniz&rsquo;s notation if a variable z depends on the variable y, which itself depends on the variable x, then z via the intermediate variable od y, depends on x as
well:
<code>dz   dz   dy -- = -- * -- dx   dy   dx</code></p>
<p><strong>Steepest descent proposes a new point</strong></p>
<p><code>x' = x - \eps grad_x f(x)</code></p>
<p>where \eps is the <strong>learning rate</strong>, a positive scalar determining the size of the step. We can set learning rate to small constant. Sometimes, we can solve for the step size that makes the directional
derivative vanish. Another approach is to evaluate <code>f(x - \eps grad_x f(x))</code> for several values of \eps and choose the one that results in the smallest objective function value. This last
strategy is called a <strong>line search</strong>.</p>
<p>In some cases, we may be able to avoid running this iterative algorithm, and just jump directly to the critical point by solving the equation <code>grad_x f(x) = 0</code> for x</p>
<p>Altough gradient descent is limited to optimization in continous paces, the general concept of repreatedly making a small move towards better configutaions can be generalized to discrete space.
Ascending an objective function of discrete parameters is called <strong>hill climbing</strong>.</p>
<p>Matrix of partial derivatives of function whose input and output are both vectors is known as a <strong>Jacobian matrix</strong>.</p>
<p>Derivative of derivative is called <strong>second derivative</strong>. We can think of the second derivative as measuring curvature. It could tell us how big step we should take.</p>

    <img src="/images/curvature.png"  alt="Curvatures"  class="center"  />


<p>When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the <strong>Hessian matrix</strong>.</p>
<p>determinant of Jacobi matrix is called <strong>Jacobian</strong> and determinant of Hessian matrix is called <strong>Hessian</strong>.</p>
<p>The eigenvalues of the Hessian is used to determine the scale of the learning rate.</p>
<p>Suppose we have critical point f'(x) = 0. When the second derivative f''(x) &gt; 0, then first derivate increase as we move to the right and decrease when moving left. This tells us that x is local minumum.
Simmilary <code>f''(x) &lt; 0</code> conclude local maximum. This is called <strong>second derivative test</strong>. When f'&lsquo;x = 0 the test is inconclusive and x may be saddle point or part of a flat region.</p>
<p>In multiple dimensions, we need to examine all of the second derivatives of the function. Using the eigendecomposition of the Hessian matrix, we can generalize the second derivative test to multiple
dimensions. At a critical point where <code>grad_x f(x) = 0</code> we can examine the eigenvalues of the Hessian to determine wheter the critical point is local minimum/maximum or saddle point. When the Hessian is
positive definite (all its eigenvalues are positive), the point is a local minimum. Likewise when the Hessian is negative definite (all its eigenvalues are negative), the point is a local maximum. The
test in multiple dimensions are inconclusive whener all of the non-zero eigenvalues have the same sign, but at least one eigen is zero. When at least one eigenvalue is positive and at least one is
negative we know that x is both a local minimum on one cross section and maximum on another cross section.</p>
<p><strong>Newton&rsquo;s method</strong> is iterative method that can find critical point even faster than gradient descent however ca be harmful near saddle points. Newton&rsquo;s method is only approriate when the nearby
critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient descent is not attracted to saddle points unless the gradient points toward them.</p>
<p>Optimization algorithms that use only the gradient, such as gradient descent, are called <strong>first-order optimization algorithms</strong>. Optimization algorithms that also use the Hessian matrix, such as Newton&rsquo;s
method, are called <strong>second-order optimization algorithms</strong>.</p>
<p>In DL we use algorithms that have almost no guarantees. Mostly because family of functions used in DL is quite complicated. In many other fields, the dominant approach to optimization is to design
optimization algorithms for a limited family of functions.</p>
<p>In the context of DL, we sometimes fain some guarantees by restricting ourselves to functions that are either <strong>Lipschitz continous</strong> or have Lipschitz continous derivatives. a Lipschitz continous
function is a function f whose rate of change is bounded by a <strong>Lipschitz constant</strong> often denoted as weird capital L:</p>
<p><code>∀x, ∀y, |f(x) - f(y)|&lt;=L ||x-y||_2</code>
where L is Lipschitz constant.</p>
<p>This property is useful because it allow us to assume that a small change in the input will have a small change in the output. Lipschitz continuity is also a fairly weak constraint, and many optimization
problems in DL can be made Lipschitz continuos with relatively minor modifications.</p>
<p>Perhaps the most sucesful field of specialized optimization is <strong>convex optimization</strong>. Convex optimization algorithms are able to provide many more guarantees by making stronger restrictions. Convex
optimization algorithms are applicable only to convex functions &ndash; functions for which the Hessian is positive semidefinite everywhere. Such functions are well-behaved because they lack saddle points and
all of their local minima are necessarily global minima. However most problems in DL are difficult to express in terms of convex optimization but some algorithms use it as subroutines. Ideas from the
analysis of convex optimization can be useful for proving the convergence of DL algorithms.</p>
<h1 id="constrained-optimization">Constrained Optimization</h1>
<p>Sometimes we may with to find the maximal or minimal value of f(x) for values of x just in some set S. This is known as <strong>constrained optimization</strong>. Points x that lie within the set S are called
<strong>feasible points</strong> in constrained optimization terminology.</p>
<p>There are few approaches to constrained optimization.</p>
<p>First approach is to modify gradient descent, with use of line search, we can search only over step sizes \eps that yields new x points that are feasible.</p>
<p>A more sophisticated is to design a different, unconstrained optimization problem whose solution can be converted into a solution to the original, constrained optimization problem. This approach requires
creativity; the transformation between optimization problems must be designed specifically for each case we encounter.</p>
<p>The <strong>Karush-Kuhn-Tucker</strong> (KKT) approach provides a very general solution to constrained optimization. With the KKT approach we introduce a new function called the <strong>generalized Lagrangian</strong> or <strong>generalized
Lagrange function</strong>.
To define the Lagrangian, we fist need to describe S in terms of equations and inequalities. We want a description of S in terms of m functions g^(i) and n functions h^(j) so that:
<code>S={x|∀i,g^(i)(x) = 0 and ∀j,h^(j)(x) &lt;= 0}</code></p>
<p>The equations involving g^(i) are called the <strong>equality constraints</strong> and the inequalities involving h^(j) are called <strong>inequalities constraints</strong>.</p>
<p>We introduce new variables \lambda_i and \alpha_j for each constrain, these are called the KKT multipliers. The generalized Lagrangian is then defined as
<code>L(x, \lambda, \alpha) = f(x) + \sum_i \lambda_i g^(i)(x) + \sum_j \alpha_j h^(j)(x)</code></p>
<p>We can now solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian. Observe that, so long as at least one feasible point exists and f(x) is not permitted
to have value inf, then</p>
<p><code>min_x max_\lambda max_(\alpha,\alpha&gt;=0) L(x, \lambda, \alpha)</code></p>
<p>has the same optimal objective function value and set of optimal points x as:
<code>min_(x∈S) = f(x)</code></p>
<p>This follows because any time the constains are satisfied:</p>
<p><code>max_\lambda max_(\alpha,\alpha&gt;=0) L(x, \lambda, \alpha) = f(x)</code></p>
<p>while any time a constraint is violated:</p>
<p><code>max_\lambda max_(\alpha,\alpha&gt;=0) L(x, \lambda, \alpha) = \inf</code></p>
<p>These properties guarantee that no infeasible point can be optimal, and that the optimum within the feasible points is unchanged.</p>
<p>We say that a constraint <code>h^(i)(x)</code> is active if <code>h^(i)(x*) = 0</code>. If a constraint is not active, then the solution to the problem found using would remain at least a local solution of that constraint
were removed. It&rsquo;s possible that an inactive constraint excludes other solutions. eg a convex problem with an entire region of globally optimal points (a wide, flat, region of equal const) could have a
subset of this region eliminated by constraints or a non-convex problem could have better local stationary points exluded by a constraint that is inactive at convergence.</p>
<p>We can say that either the solution is on the boundary imposed by the inequality and we must use its KKT multiplier to influence the solution to x, or the inequality has no influence on the solution and
we represent this by zeroing out its KKT multiplier.</p>
<p>A simple set of properties describe the optimal points of constrained optimization problems. These properties are called the KKT conditions. They are necessary conditions, but not always sufficient for a
point to be optimal. The conditions are:</p>
<ul>
<li>The gradient of the generalized Lagrangian is zero.</li>
<li>All constraints on both x and the KKT multipliers are satisfied</li>
<li>The inequality constrains exhibit &ldquo;complementary slackness&rdquo;: <code>\alpha ⊙ h(x) = 0</code></li>
</ul>
]]></content></item><item><title>DL1</title><link>https://rand-notes.github.io/dl1/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl1/</guid><description>TOC
self-information nats; bits; shannons;
Shannon entropy
KL Divergence
CrossEntropy
Factorization
Structured Probabilistic Model
Directed
Undirected The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions;
Self-information we rate information value much higher if information is less likely and independent.</description><content type="html"><![CDATA[<p>TOC</p>
<ul>
<li>
<p><a href="#self-information">self-information</a>
<strong>nats</strong>; <strong>bits</strong>; <strong>shannons</strong>;</p>
</li>
<li>
<p><a href="#shannon-entropy">Shannon entropy</a></p>
</li>
<li>
<p><a href="#kl-divergence">KL Divergence</a></p>
</li>
<li>
<p><a href="#crossentropy">CrossEntropy</a></p>
</li>
<li>
<p><a href="#factorization">Factorization</a></p>
</li>
<li>
<p><a href="#structured-probabilistic-model">Structured Probabilistic Model</a></p>
<ul>
<li>
<p><a href="#directed">Directed</a></p>
</li>
<li>
<p><a href="#undirected">Undirected</a>
<strong>The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions</strong>;</p>
</li>
</ul>
</li>
</ul>
<h1 id="self-information">Self-information</h1>
<p>we rate information value much higher if information is less likely and independent.</p>
<p><code>I(x) = -log(P(x))</code></p>
<p>we are using natural logarithm with base <code>e</code> so I(x) is in units of <strong>nats</strong>. One nat is the amount fo information gained by observing an event of probability <code>1/e</code>. Other texts use base-2 logarithms and
units called <strong>bits</strong> or <strong>shannons</strong>. Information measured in bits is just a rescaling of information measured in nats.</p>
<h1 id="shannon-entropy">Shannon entropy</h1>
<p><code>H(x) = E[I(x) = E[log P(x)]]</code></p>
<p>when x is continuous the Shannon entropy is known as the differential entropy.</p>
<h1 id="kl-divergence">KL Divergence</h1>
<p>we are basically measuring distance between two distributions.
<code>D_{KL}(P||Q) = E[log(P(x)/Q(x))]</code></p>
<p>important is that <code>D(P||Q) != D(Q||P)</code> and <code>D(P||Q) &gt;= 0</code>.</p>
<p>if both distributions are same than KL divergence is 0.</p>
<p>let&rsquo;s say we have distribution representing true labels and models predictions. We can rate model with KL divergence.</p>
<p><code>D(P||Q) = CrossEntropy(P, Q) - Entropy(P)</code></p>
<h1 id="crossentropy">CrossEntropy</h1>
<p>Similar to KL Divergence</p>
<p><code>H(P, Q) = H(P) + D_{KL}(P||Q)</code></p>
<h1 id="factorization">Factorization</h1>
<p>Factoring consists of writing a number or another mathematical object as a product of several factors, usually smaller or simpler objects of the same kind</p>
<h1 id="structured-probabilistic-model">Structured Probabilistic Model</h1>
<p>In ML we are working with a large number of random variables. Often these distributions are involved in direct interactions between relatively few variables. And instead difficulty crafting single
function we can split distribution into many factors that we multiply together. These factorization can greatly reduce the number of parameters needed to describe the distribution.</p>
<p>We can describe these kinds of factorizations using graphs that we call Structured Probabilistic Model or Graphical Model.
Could be directed or undirected. Each node represents random variable and edge represents direct interactions between two random variables.</p>
<h2 id="directed">Directed</h2>
<p>with directed edges; represent factorization into conditional probability distributions. A directed model contains one factor for every random variable x_i in the distribution and that factor consits of
the conditional distribution over x_i given the parents of x_i, denoted P_ag(x_i):</p>
<p>pi - capital Pi for multiplication</p>
<p><code>p(x) = pi(p(x_i | P_ag(x_i)))</code></p>

    <img src="/images/directed.png"  alt="Directed"  class="center"  />


<h2 id="undirected">Undirected</h2>
<p>with undirected edges; represent factorizations into a set of functions. Unlike in directed case, these functions are usually not probability distributions.
Any set of nodes that are all connected to each other i G is called clique. Each clique c_i is associated with a factor o_i. These factors are functions and the output of each factor must be
non-negative, but there is no constint that the factor must sum or integrate to 1 like probability distribution.</p>

    <img src="/images/undirected.png"  alt="undirected"  class="center"  />


<p>The probability of a configuration of random variables is proportional to the product od all these factors. Because we have no guarantee that the product will sum to 1, we divide by a normalizing
constant Z, defined to be the sum or integral over all states of the prodcut of the o fcuntions in order to obtain a normalized probability distribution:</p>
<p><code>p(x) = 1/Z * pi(o_i * (C_i))</code></p>
<p><strong>The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions</strong></p>
]]></content></item><item><title>DL2</title><link>https://rand-notes.github.io/dl2/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/dl2/</guid><description>TOC Learning algorithms
The Task, T
Classification
Classification with missing inputs
Regression
Transcription
Machine translation
Structured output
Anomaly detection
Synthesis and sampling
Imputation of missing values
Denoising
Density estimation or probability mass function estimation
The Performance Measure, P accuracy; error rate;</description><content type="html"><![CDATA[<h1 id="toc">TOC</h1>
<ul>
<li>
<p><a href="#learning-algorithms">Learning algorithms</a></p>
<ul>
<li>
<p><a href="#the-task,-t">The Task, T</a></p>
<ul>
<li>
<p><a href="#classification">Classification</a></p>
</li>
<li>
<p><a href="#classification-with-missing-inputs">Classification with missing inputs</a></p>
</li>
<li>
<p><a href="#regression">Regression</a></p>
</li>
<li>
<p><a href="#transcription">Transcription</a></p>
</li>
<li>
<p><a href="#machine-translation">Machine translation</a></p>
</li>
<li>
<p><a href="#structured-output">Structured output</a></p>
</li>
<li>
<p><a href="#anomaly-detection">Anomaly detection</a></p>
</li>
<li>
<p><a href="#synthesis-and-sampling">Synthesis and sampling</a></p>
</li>
<li>
<p><a href="#imputation-of-missing-values">Imputation of missing values</a></p>
</li>
<li>
<p><a href="#denoising">Denoising</a></p>
</li>
<li>
<p><a href="#density-estimation-or-probability-mass-function-estimation">Density estimation or probability mass function estimation</a></p>
</li>
</ul>
</li>
<li>
<p><a href="#the-performance-measure,-p">The Performance Measure, P</a>
<strong>accuracy</strong>; <strong>error rate</strong>;</p>
</li>
<li>
<p><a href="#the-experience,-e">The Experience, E</a>
<strong>supervised</strong>; <strong>unsupervised</strong>; <strong>Dataset</strong>; <strong>data points</strong>;</p>
<ul>
<li>
<p><a href="#unsupervised-learning-algorithm">Unsupervised Learning Algorithm</a></p>
</li>
<li>
<p><a href="#supervised-learning-algorithm">Supervised Learning Algorithm</a>
<strong>label</strong>; <strong>target</strong>;</p>
</li>
<li>
<p><a href="#overview">Overview</a>
<strong>semi-supervised</strong>; <strong>multi-instance</strong>; <strong>Reinforcement learning</strong>;</p>
</li>
<li>
<p><a href="#dataset">Dataset</a>
<strong>design matrix</strong>;</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#capacity,-overfitting-and-underfitting">Capacity, Overfitting and Underfitting</a>
<strong>generalization</strong>; <strong>training error</strong>; <strong>generalization error</strong>; <strong>test error</strong>; <strong>test set</strong>; <strong>Statistical learning theory</strong>; <strong>data generating process</strong>; <strong>data generating distribution</strong>; <strong>underfitting</strong>; <strong>overfitting</strong>; <strong>capacity</strong>; <strong>hypothesis space</strong>; <strong>Representational capacity</strong>; <strong>effective capacity</strong>; <strong>Occam&rsquo;s razor</strong>; <strong>Vapnik-Chervonenkis dimension</strong>; <strong>nonparametric models</strong>; <strong>Bayes error</strong>;</p>
<ul>
<li>
<p><a href="#free-lunch-theorem">Free lunch theorem</a></p>
</li>
<li>
<p><a href="#regularization">Regularization</a>
<strong>weight decay</strong>; <strong>regularizer</strong>;</p>
</li>
</ul>
</li>
<li>
<p><a href="#hyperparameters-and-validation-sets">Hyperparameters and Validation Sets</a>
<strong>validation set</strong>;</p>
<ul>
<li><a href="#cross-validation">Cross-validation</a></li>
</ul>
</li>
<li>
<p><a href="#estimators,-bias-and-variance">Estimators, Bias and Variance</a></p>
<ul>
<li>
<p><a href="#point-and-function-estimation">Point and Function Estimation</a>
<strong>point estimator</strong>; <strong>statisctic</strong>;</p>
</li>
<li>
<p><a href="#bias">Bias</a>
<strong>unbiased</strong>; <strong>asymptotically unbiased</strong>;</p>
</li>
<li>
<p><a href="#variance-and-standard-error">Variance and Standard Error</a></p>
</li>
<li>
<p><a href="#trading-off-bias-on-variance-to-minimize-mse">Trading off bias on variance to minimize MSE</a></p>
<ul>
<li><a href="#consistency">Consistency</a>
<strong>consistency</strong>; <strong>almost sure</strong>;</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></p>
<ul>
<li>
<p><a href="#conditional-log-likelihood-and-mse">Conditional Log-likelihood and MSE</a></p>
<ul>
<li>
<p><a href="#linear-regression-as-maximum-likelihood">Linear Regression as Maximum Likelihood</a></p>
</li>
<li>
<p><a href="#properties-of-maximum-likelihood">Properties of Maximum Likelihood</a>
<strong>statistical efficiency</strong>; <strong>parametric case</strong>;</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#bayesian-statistics">Bayesian Statistics</a>
<strong>frequentist statistics</strong>; <strong>prior probability distribution</strong>; <strong>the prior</strong>;</p>
<ul>
<li>
<p><a href="#bayesian-linear-regression">Bayesian Linear Regression</a>
<strong>posterior</strong>;</p>
</li>
<li>
<p><a href="#maximum-a-posteriori-(map)-estimation">Maximum a Posteriori (MAP) Estimation</a>
<strong>maximum a posteriori</strong>;</p>
</li>
</ul>
</li>
<li>
<p><a href="#supervised-learning-algorithms">Supervised Learning Algorithms</a></p>
<ul>
<li>
<p><a href="#probabilistic-supervised-learning">Probabilistic Supervised Learning</a></p>
<ul>
<li><a href="#support-vector-machines">Support Vector Machines</a>
<strong>kernel trick</strong>; <strong>Gaussian kernel</strong>; <strong>radial basis function</strong>; <strong>template matching</strong>; <strong>kernel machines</strong>; <strong>kernel methods</strong>; <strong>support vectors</strong>;</li>
</ul>
</li>
<li>
<p><a href="#other-simple-supervised-learning-algorithms">Other Simple Supervised Learning Algorithms</a></p>
</li>
</ul>
</li>
<li>
<p><a href="#unsupervised-learning-algorithms">Unsupervised Learning Algorithms</a></p>
<ul>
<li><a href="#principal-components-analysis">Principal Components Analysis</a></li>
</ul>
</li>
<li>
<p><a href="#stochaistic-gradient-descent">Stochaistic Gradient Descent</a>
<strong>minibatch</strong>;</p>
</li>
<li>
<p><a href="#building-a-machine-learning-algorithm">Building a Machine Learning Algorithm</a></p>
</li>
<li>
<p><a href="#challenges-motivating-deep-learning">Challenges Motivating Deep Learning</a></p>
<ul>
<li>
<p><a href="#the-curse-of-dimensionality">The Curse of Dimensionality</a>
<strong>curse pf dimensionality</strong>;</p>
</li>
<li>
<p><a href="#local-constancy-and-smoothness-regularization">Local Constancy and Smoothness Regularization</a>
<strong>smoothness prior</strong>; <strong>local constancy prior</strong>; <strong>local kernels</strong>;</p>
</li>
<li>
<p><a href="#manifold-learning">Manifold Learning</a>
<strong>manifold</strong>; <strong>Manifold Learnign</strong>; <strong>manifold hypothesis</strong>;</p>
</li>
</ul>
</li>
</ul>
<h1 id="learning-algorithms">Learning algorithms</h1>
<p>Basically algorithm that is able to learn from data.</p>
<blockquote>
<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks i T, as measured by P, improves with experience E.</p>
</blockquote>
<h2 id="the-task-t">The Task, T</h2>
<p>ML tasks are usually described in terms of how the machine learning system should process an example. An example is a collection of features that have been quantitatively measured from some object or
event that we want the ML system to process.</p>
<p>There are a lot of task types, some examples:</p>
<h3 id="classification">Classification</h3>
<p>In this type of thas the program is asked to specify which of k categories some input belongs to.
Algorithm usually produce a function <code>f: R^n -&gt; {1, ..., k}</code>.
When <code>y = f(x)</code>, the model assigns an input described by vector x to a category indentified by numeric y.
There are other variants of the classification task, eg where f outputs a probability distribution over classes.</p>
<h3 id="classification-with-missing-inputs">Classification with missing inputs</h3>
<p>Classification becomes more challenging if the program is not guaranteed that every measurement in its input vector will always be provided.
In order to solve the classification task, the learning algorithm only has to define a single function mapping from a vector input to a categorical ouput.
when some of the inputs may be missing, rather than providing a single function, the learning algorithm must learn a set of functions.
Each function corresponds to classifying x with a different subset of its inputs missing.
This often arises in medical diagnosis, because many tests are invasive or expensive.
One way to efficiently define such a large set of functions is to learn a probability distribution over all of the relevant variables, then solve the classification task by marginalizing out the missing
variables. With n input variables, we can now obrain all 2^n different classification functions needed for each possible set of missing inputs, but we only need to learn a single function describing the
joint probability distribution.</p>
<h3 id="regression">Regression</h3>
<p>program predict a numerical value given some input. To solve this task, the learning algorithm, is asked to output a function <code>f: R^n -&gt; R</code>.
eg. future prices of securities</p>
<h3 id="transcription">Transcription</h3>
<p>ML system is asked to observe a relatively unstructured representation of some kind of data and transcribe it into discrete, textual form.</p>
<p>eg. OCR, speech recognition</p>
<h3 id="machine-translation">Machine translation</h3>
<p>converting a sequence of symbols in some language into a sequence of symbols in another language.</p>
<p>eg. english -&gt; french</p>
<h3 id="structured-output">Structured output</h3>
<p>Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the different elements.
This is a broad category, and subsumes the transcription and translation tasks described above, but also many other tasks.
One examples is parsing, which is mapping a natural language into a tree that describe its grammatical structure  and tagging nodes as verbes, nouns, adverbs and so on.
The output need not have its form mirror the structure of the input as closely as in these annotation-style tasks.
These tasks are called structured output tasks because the program must output several values that are all tightly inter-related. eg. the words produced by an image captioning program must form a valid
sentence</p>
<h3 id="anomaly-detection">Anomaly detection</h3>
<p>In this type of task, the computer program sifts through a set of events or objects, and flags some of them as being unusual or atypical.
detecting card fraud etc.</p>
<h3 id="synthesis-and-sampling">Synthesis and sampling</h3>
<p>Algorithm generate new examples that are similar to those in the training data.
Synthesis and smapling via machine learning can be useful for media application &ndash; generating content like textures for large objects or landscape.
In speech synthesis task, we provide a written sentence and ask the program to emit an audio waveform containing a spoken version.
This is kind of structured ouput task, but with the added qualification that there is no single correct output for each input, and we explicitly desire a large amount of variation in the ouput, in order
for the output to seem more natural and realistic.</p>
<h3 id="imputation-of-missing-values">Imputation of missing values</h3>
<p>In this type of task, the ML algorithm is given a new example <code>x ∈ R^n</code>, but with some entries x_i of x missing. The algorithm must provide a prediction of the values of the missing entries.</p>
<h3 id="denoising">Denoising</h3>
<p>In this type of task, the ML algorithm is given in input a corrupted example <code>x' ∈ R^n</code> obtained by an uknown corruption process from a clean example x ∈ R^n. The learner must predict the clean example x
from its corrupted version x', or more generally predict the conditional probability distribution <code>p(x|x')</code></p>
<h3 id="density-estimation-or-probability-mass-function-estimation">Density estimation or probability mass function estimation</h3>
<p>In density estimation problem, the ML algorithm is asked to learn a function <code>p_model: R^n -&gt; R</code> where p_model(x) can be interpreted as a probability density function (if x is continuous) or a
probability mass function (if x is discrete) on the space that the examples were drawn from.</p>
<p>To do such tasks well the algorithm needs to learn the structure of the data it has seen. It must know where examples cluster tightly and where they are unlikely to occur.</p>
<p>Most of the tasks require to at least implicitly capture the structure of probability distribution. Density estimation allows us to explicitly capture that distribution. In principle, we can then perform
computations on that distribution in order to solve the other tasks as well. eg. ig we have performed density estimation to obtaion a probability distribution p(x), we can use that distribution to solve
the missing value imputation task. In practise, density estimation does not always allow us to solve all of these related tasks, because in many cases the required operations on p(x) are computationally
intractable.</p>
<h2 id="the-performance-measure-p">The Performance Measure, P</h2>
<p>In order to evaluate the abilities of ML algorithm on (ussualy) specific task T, we must design a quantitative measure of its performance.</p>
<p>For task such as classification or transcription, we often measure the <strong>accuracy</strong> of the model. Accuracy is just the proportion of examples for which the model produces the correct output.</p>
<p>We can also obtain equivalent information by measuring the <strong>error rate</strong>, the proportion of examples for which the model produces an incorrect output. We often refer to the error rate as the expected
0-1 loss (0 if correctly classiffied, 1 if mistakenly).</p>
<p>For tasks such as density estimation it doesn&rsquo;t make sense to use these metrics and instead we use metrics with continuous-valued score for each example. The most common approach is to report the average
log probability the model assigns to some examples.</p>
<p>Performance measures using a test set of data that is separate from the data used for training ML system.</p>
<p>Sometimes it may not be clear what should be measured. Should be measured whole result atomically or should be model awarded for partially correct result. Sometimes we must to approximate measuring
because real measuring is computationally difficult.</p>
<h2 id="the-experience-e">The Experience, E</h2>
<p>ML algorithms can be divided as <strong>supervised</strong> and <strong>unsupervised</strong> by what kind of experience they are allowed to have during learning process.</p>
<p><strong>Dataset</strong> is collection of many examples sometimes called <strong>data points</strong></p>
<h3 id="unsupervised-learning-algorithm">Unsupervised Learning Algorithm</h3>
<p>experience a dataset containing many features, then learn useful properties of the structure of this dataset. In DL context, we usually want to learn the entire probability distribution that generated a
dataset, wheter explicitly (density estimation) or implicitly (synthesis). Some other unsupervised learning algorithms perform other roles, like clustering, which constist of dividing the dataset into
clusters of similar examples.</p>
<h3 id="supervised-learning-algorithm">Supervised Learning Algorithm</h3>
<p>experience a dataset containing features, but each  example is also associated with a <strong>label</strong> or <strong>target</strong>.</p>
<h3 id="overview">Overview</h3>
<p>Basically, unsupervised learning involvesobserving several examples of a random vector x and attempting to learn the probability distribution p(x) or some interesting properties of that distribution.
On the other hand supervised learning involves observing several examples of a random vector x and an associated value or vector y, and learning to predict y from x, ussualy by estimating p(y|x).
The line between these two are often blurred.</p>
<p>eg. chain rule of probability, which decomposition says that we can solve ostensibly unsupervised problem of modeling p(x) by splitting it into n supervised learning problems.</p>
<p>Even though the line is blur, traditionally, people refer to regression, classification and structured output problems as supervised learning. Density estimation in support of other tasks is usually
considered unsupervised learning.</p>
<p>There are also other variants of learning paradigm.</p>
<p>In <strong>semi-supervised</strong>, some examples include a supervision target but others do not.
In <strong>multi-instance</strong> learning, an entire collection of examples is labeled as containin gor not containing an example of class, but he individual members of the collection are not labeled.
Some ML algorithms do not experience a fixed dataset. eg. <strong>Reinforcement learning</strong> algorithms interact with an environment, so there is a feedback loop between the learning system and its experience.</p>
<h3 id="dataset">Dataset</h3>
<p>Common way of describing a dataset is with a <strong>design matrix</strong>. A design matrix is a matrix containing a different example in each row. Each column corresponds to a different feature. If working with
heterogenous data we describe it as a set containing m elements. In supervised learning, design matrix of feature observations X, also contain a vector of labels y.</p>
<h1 id="capacity-overfitting-and-underfitting">Capacity, Overfitting and Underfitting</h1>
<p>The ability to perform well on previously unobserved inputs is called <strong>generalization</strong>.</p>
<p>When training we have access to train set so we can compute some error measure on the training set called the <strong>training error</strong> and we reduce this training error.</p>
<p>What separates ML from optimization is that we want the <strong>generalization error</strong>, also called the <strong>test error</strong>, to be low.
The generalization error is defined as the expected value of the error on a new input.</p>
<p>We typically estimate the generalization error of a ML model by measuring its performance on a <strong>test set</strong> of examples that were collected separately from the training set.</p>
<p><strong>Statistical learning theory</strong> can provide some answers on how can we affect performance on the test set we can observe only the training set.
If the sets weren&rsquo;t collected arbitrarily, we can make some assumptuions and make some progress.</p>
<p>The train and test data are generated by a probability distribution over datasets called the <strong>data generating process</strong>. We typically make a set of assumptions known collectively as the <strong>i.i.d.
assumptions</strong> (independent and identically distributed). This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is
then used to generate every train and test example. We call that shared underlying distribution the <strong>data generating distribution</strong>.</p>
<p>One immediate connection we can observe between the training and test erroris that the expected training error of a randomly selected model is equal to the expected test error of that model</p>
<p>However with ML algorithm, we do not fix parameters ahead of time, then sample both datasets. We sample training set, use it to choose parameters, then sample the test set. Under this process, the
expeted test error is greater than or equal to the expected value of training error.
The factors determining how well a machine learning algorithm will perform are its ability to:</p>
<ul>
<li>make the training error small</li>
<li>make the gap between training and test error small</li>
</ul>
<p>These two factors corresponds to the two central challenges in ML: <strong>underfitting</strong> <strong>overfitting</strong>.
Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set.
Overfitting occurs when the gap between the training error and test error is too large.</p>
<p>We can control whether a model is more likely to overfit or underfit by altering its <strong>capacity</strong></p>

    <img src="/images/capacity-error.png"  alt="capacity-error"  class="center"  />


<p>Informally, a model’s capacity is its ability to fit a wide variety of functions.
Models with low capacity may struggle to fit the training set.
Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set.</p>
<p>One way to control the capacity is by choosing its <strong>hypothesis space</strong>, the set of functions that the learning algorithm is allowed to select as being the solution.</p>
<p>For example, the linear regression algorithm has the set of all linear functions of its input as its hypothesis space.</p>
<p>If we generalize linear regression to include polynomials(eg quadratics or even more degree polynomials) we increase the models capacity. (By increasing degree of polynoms leads to different input but
output is still linear function).</p>
<p><strong>Representational capacity</strong> of model is changing according to function within family of functions of model. It&rsquo;s often very hard to find the best function withing this family. In practise we do not
find the best solution but rather use one that significantly reduce error. These additional limitations such as the imperfection fo the optimization, mean that the learning algorithm&rsquo;s <strong>effective capacity</strong>
may be less than the representational capacity of the model family.</p>
<p><strong>Occam&rsquo;s razor</strong> This principle state sthat among competing hypotheses that explain known ovservations equally well, one should choose the &ldquo;simplest one&rdquo;.</p>
<p><strong>Vapnik-Chervonenkis dimension</strong> or VC dimension measures the capacity of a binary classifier. The VC dimension is defined as being the largest possible value of m for which there exists a training set
of m different x points that the classifier can label arbitrarily.</p>
<p>To reach the most extreme case of arbitrarily high capacity, we introduce the concept of <strong>nonparametric models</strong>. Sometimes, nonparametric models are just theoretical abstractions (such as
an algorithm that searches over all possible probability distributions) that cannot be implemented in practice. However, we can also design practical nonparametric models by making their complexity a
function of the training set size. One example of such an algorithm is nearest neighbor regression, where the model looks up the nearest entry in the training set and returns the
associated regression target.</p>
<p>Finally, we can also create a nonparametric learning algorithm by wrapping a parametric learning algorithm inside another algorithm that increases the number
of parameters as needed. For example, we could imagine an outer loop of learning that changes the degree of the polynomial learned by linear regression on top of a polynomial expansion of the input.</p>
<p>The ideal model is an oracle that simply knows the true probability distribution that generates the data (which still can incur some error because of noise in distribution). The error incurred by an
oracle making predictions from the true distribution p(x, y) is called the <strong>Bayes error</strong>.</p>
<p>Training and generalization error vary as the size of the training set varies. Expected generalization error can never increase as the number of training examples
increases. For nonparametric models, more data yield better generalization until the best possible error is achieved. Any fixed parametric model with less than
optimal capacity will asymptote to an error value that exceeds the Bayes error.</p>
<p>Note that it is possible for the model to have optimal capacity and yet still have a large gap between training and generalization
errors. In this situation, we may be able to reduce this gap by gathering more training examples</p>
<h2 id="free-lunch-theorem">Free lunch theorem</h2>
<p>the no free lunch theorem for ML states that, averaged over alll possible data-generating distributions, every classification algorithm has the same error rate when classifying previously unobserved
points.</p>
<p>Basically no ML algorithm is better than random predictor on all unobserved possible data, but we are ussually interested only in real-worlds distributions.</p>
<h2 id="regularization">Regularization</h2>
<p>The no free lunch theorem implies that we must design our machine learning algorithms to perform well on a speciffic task.</p>
<p>For example, we can modify the training criterion for linear regression to include <strong>weight decay</strong> denoted as \lambda. with \lambda = 0, we impose no preference, larger \lambda forces the weights to
become smaller. This gives us solutions that have a smaller slope, or that put weight on fewer of the features.</p>
<p>More generally, we can regularize a model that learns a function f(x; θ) by adding a penalty called a <strong>regularizer</strong> to the cost function. In the case of weight deacay.
There are many other ways of expressing preferences for different solutions, both implicitly and explicitly. Together, these different approaches are known as regularization.</p>
<h1 id="hyperparameters-and-validation-sets">Hyperparameters and Validation Sets</h1>
<p>Most machine learning algorithms have hyperparameters, settings that we can  to control the algorithm’s behavior.</p>
<p>Sometimes hyperparameters does not learn because it&rsquo;s difficult. More frequently, it&rsquo;s not appropriate to learn them on training set. If learned on the training set, such hyperparameters would always
choose the maximum possible capacity.</p>
<p>To solve this problem, we need a <strong>validation set</strong> of examples that the training algorithm does not observe.</p>
<p>It is important that the test examples are not used in any way to make choices about the model, including its hyperparameters. For this reason, no example from the test set can be used in the validation set.
Therefore, we always construct the validation set from the training data. Speciffically, we split the training data into two disjoint subsets.</p>
<p>Validation set is used to estimate the generalization error during or after training, allowing for the hyperparameters to be updated accordingly.
Typically 80 - 20 &ndash; training - validation set</p>
<p>Since the validation set is used to “train” the hyperparameters, the validation set error will underestimate the generalization error, though typically by a smaller amount than the training error does.
After all hyperparameter optimization is complete, the generalization error may be estimated using the test set.</p>
<h2 id="cross-validation">Cross-validation</h2>
<p>If dataset is too small, there are procedures that are based on the idea of repeating the training and testing computation on different randomly chosen subset or splits of the original data.</p>
<p>Most common is k-fold cross validation, in which a partition of the dataset is formed by splitting it into k non-overlapping subsets.</p>
<p>On trial i, the i-th subset of the data is used as the test set and the rest as training set. The test error may then be estimated by taking the average test error across k trials.</p>
<h1 id="estimators-bias-and-variance">Estimators, Bias and Variance</h1>
<h2 id="point-and-function-estimation">Point and Function Estimation</h2>
<p>A <strong>point estimator</strong> or <strong>statisctic</strong> is any function of the data: <code>θ^^_m = g(x^(1), ..., x^(m))</code>, where θ is ground truth and θ^^ is estimation. Almost any function is estimator, but good estimator is
a function whose output is close to the true underlying θ that generated the training data.</p>
<p>A function estimation is basically point estimator in function space, denoted as: f^^.</p>
<h2 id="bias">Bias</h2>
<p>The bias of an estimator is defined as: <code>bias(θ^^_m) = E(θ^^_m) - θ</code>.</p>
<p>An estimator θ^_m is said to be <strong>unbiased</strong> if <code>bias(θ^^_m) = 0</code> which implies that <code>E(θ^^_m) = θ</code>
An estimator θ^_m is said to be <strong>asymptotically unbiased</strong> if <code>lim_(m-&gt;\inf) bias(θ^^_m) = 0</code>, which implies that: <code>lim_(m-&gt;\inf) E(θ^^_m) = θ</code></p>
<h2 id="variance-and-standard-error">Variance and Standard Error</h2>
<p>The variance of an estimator is simply the variance <code>Var(θ^^)</code>, where the random variable is the training set.</p>
<p>Alternately, the square root of the variance is called the standard error, denoted <code>SE(θ)</code></p>
<p>We often estimate the generalization error by computing the sample mean of the error on the test set.</p>
<h2 id="trading-off-bias-on-variance-to-minimize-mse">Trading off bias on variance to minimize MSE</h2>
<p>Bias and variance measure two different sources of error in an estimator. Bias measeures the expected deviation from the true value of the function or parameter.
Variance on the other hand, provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause.</p>
<p>Hot to choose between estimator with high bias and high variance? The most common way to negotiate this trade-off is to use cross-validation. Alternatively, we can also compare the mean squared error
(MSE) of the estimates.</p>

    <img src="/images/bias-var.png"  alt="bias-var"  class="center"  />


<h3 id="consistency">Consistency</h3>
<p>We would like for our estimator to converge to the true as our training dataset grow. More formally, we would like that</p>
<p><code>plim_(m-&gt;/inf) θ^^_m = θ</code>
where plim indicates convergence in probability.</p>
<p>This condition is known as <strong>consistency</strong>. It&rsquo;s sometimes referred to as weak consistency, with strong consistency referring to the <strong>almost sure</strong> convergence of θ^^ to θ. Almost sure convergence of a
sqeuemce of random variables x &hellip; x_n to value x occurs when <code>p(lim_(m-&gt;\inf) x_m = x) = 1</code></p>
<p>Consistency ensures that the bias induced by the estimator diminishes as the number of data examples grows. However, the reverse is not true - asymptotic unbiasedness does not imply consistency.</p>
<h1 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h1>
<p>MLE is most common principle for deriving specific functions that are good estimator for different models.</p>
<p>One way to interpret MLE is to view it as minimizing the dissimilarity between the emprirical distribution p_data defined bu the training set and the model distribution, with degree od dissimilarity the
two measured by the KL divergence.</p>
<p><code>D_KL(p^^_data||p_model) = E x∼p^^data [log p^^_data(x) − log p_model(x)]</code></p>
<p>The term on the left is a function only of the data-generating process, not the model. This means when we train the model to minimize the KL divergence, we need only minimize
<code>-E_(x~p^^_data) [log p_model(x)]</code></p>
<p>Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical
distribution deffined by the training set and the probability distribution deffined by model. e.g. MSE is the cross-entropy between the empirical distribution and a Gaussian model.</p>
<p>We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution p^^_match. Ideally, we would like to match the true data-generating distribution p_data.</p>
<p>While the optimal θ is the same regardless of whether we are maximizing the likelihood or minimizing the KL divergence, the values of the objective functions are different. In software, we often phrase
both as minimizing a cost function.</p>
<p>Maximum likelihood thus becomes minimization of the negative log-likelihood (NLL), or equivalently, minimization of the cross-entropy. The perspective of maximum likelihood as minimum KL divergence
becomes helpful in this case because the KL divergence has a known minimum value of zero. The negative log-likelihood can actually become negative when x is real-valued.</p>
<h2 id="conditional-log-likelihood-and-mse">Conditional Log-likelihood and MSE</h2>
<p>The MLE can readily be generalized to estimate a conditional probability P(y|x; θ) in order to predict y given x. This is actually the most common situation because it forms the basis for most supervised
learning. if X represents all our inputs and Y all our observed targets, then the conditional MLE is:</p>
<p><code>θ_ML = argmax_θ P(Y|X;θ)</code></p>
<p>If the examples are assumed to be i.i.d. then this can be decomposed into:</p>
<p><code>θ_ML = argmax_θ \sum^m_i=1 log P(y_i|x_i;θ)</code></p>
<h3 id="linear-regression-as-maximum-likelihood">Linear Regression as Maximum Likelihood</h3>
<p>Instead of producing a single prediction y^^, we now think of the model as producing a conditional distribution p(y|x). We can imagine that with an inffinitely large training set, we might see several
training examples with the same input value x but different values of y. The goal of the learning algorithm is now to fit the distribution p(y|x) to all those different y values that are all compatible
with x. derive the same linear regression algorithm we obtained before, we define p(y|x) = N(y;y^^(x;w), o^2), where o^2 is dispersion(σ). The function y^^(x;w)gives the prediction of the mean of the
Gaussian.</p>
<h3 id="properties-of-maximum-likelihood">Properties of Maximum Likelihood</h3>
<p>MLE is that it can be shown to be the best estimator asymptotically, as the number of examples goes to infinite.
Under appropriate conditions, the MLE has the property of consistency meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true
value of the parameter. These conditions are as follows:</p>
<ul>
<li>The true distribution p_data must lie within the model family p_model(.;θ). Otherwise, no estimator can recover p_data.</li>
<li>The true distribution p_data must correspond to exactly one value of θ. Otherwise, maximum likelihood can recover the correct p_data but will not be able to determine which value of θ was used by the
data-generating process.</li>
</ul>
<p>There are other inductive principles besides the MLE, many of which share the property of being consistent estimators. Consistent estimators can differ, however, in their <strong>statistical efficiency</strong>, meaning
that one consistent estimator may obtain lower generalization error for a fixed number of samples or equivalently, may require fewer examples to obtain a fixed level of generalization error.</p>
<p>Statistical efficiency is typically studied in the <strong>parametric case</strong> (as in linear regression), where our goal is to estimate the value of a parameter (assuming it is possible to identify the true
parameter), not the value of a function. A way to measure how close we are to the true parameter is by the expected MSE, computing the squared difference between the estimated and true parameter alues,
where the expectation is over m training samples from the data-generating distribution. That parametric mean squared error decreases as m increases, and for m large, the Cramer-Rao lower bound shows that
no consistent estimator has a lower MSE than MLE.</p>
<p>For these reasons (consistency and efficiency), maximum likelihood is often considered the preferred estimator to use for machine learning. When the number of examples is small enough to yield overffitting
behavior, regularization strategies such as weight decay may be used to obtain a biased version of maximum likelihood that has less variance when training data is limited.</p>
<h1 id="bayesian-statistics">Bayesian Statistics</h1>
<p>So far we have discussed <strong>frequentist statistics</strong> and approaches based on estimating single value of O, then making all predictions thereafter base on that one estimate. Another approach is to consider
all possible values of θ when making a prediction. Another approach is to cnsider all possible values of θ when making a prediction which is domain of Bayesian statistics.</p>
<p>The Bayesian uses probability to reflect degrees of certainty in states of knowledge. The dataset is directly observed and so is not random. On the other hand, the true paramater θ is unkown or uncertain
and thus is represented as a random variable.</p>
<p>Before observing the data, we represent our knowledge of θ using the <strong>prior probability distribution</strong> sometimes called just <strong>the prior</strong>, p(θ). Generally, the ML practitioner selects a prior
distribution that is quite broad (i.e. with high entropy) to reflet a high degree of uncertainty in the value of θ before observing any data. e.g. one might assume a priori that θ lies in some finite
range or volume, with a uniform distribution. Many priors instead reflect a preference for &ldquo;simplee&rdquo; solutions (such as smaller magnitude coefficients, or a function that is close to being constant)</p>
<p>Now consider that we have a set of data samples {x_1, &hellip;, x_n}. We can recover the effect of data on our belief about θ by combining the data likelihood p(x_1, &hellip;, x_m| θ) with the prior via Bayes
rule:</p>
<p><code>p(θ|x_1, ..., x_m) = p(x_1,...,x_m|θ)p(θ) / p(x_1,...,x_m)</code></p>
<p>In the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation of the data usually causes the
posterior to lose entropy and concentrate around a few highly likely values of the parameters.</p>
<blockquote>
<p>Unlike the maximum likelihood approach that makes predictions using a point estimate of O, the Bayesian approach is to make predictions using a full distribution over O.</p>
</blockquote>
<p>e.g., after observing m examples the predicted distribution over the next data sample, x_m+1, is given by:</p>
<p><code>p(x_m+1 | x_1,...,x_m) = \int{p(x_m+1 | θpθ | x_1, ... x_m)} dθ</code></p>
<p>Frequentist approach addresses the uncertainty in a given point estimate of θ by evaluating its variance. The variance of the estimator is an assessment of how the estimate might change with alternative
samplings of the observed data. The Bayesian answer to the question of how to deal with the uncertainty in the estimator is to simply integrate over it, which tends to protect well against overffitting.</p>
<p>Second difference between Bayesian and maximum likelihood approach is due to Bayesian prior contribution. The prior has an influence by shifting probability mass density ards regions of the parameter
space that are preferred a priori. In practice the prior often expresses a preference for models that are simpler or more smooth. Critics of the Bayesian approach identify the prior as a source of
subjective human judgment affecting the predictions.</p>
<p>Bayesian methods typically generalize much better when limited training data is available but typically suffer from high computational cost when the number of training examples is large.</p>
<h2 id="bayesian-linear-regression">Bayesian Linear Regression</h2>
<p>Expressed as a Gaussian conditional distribution on y_train, we have:</p>
<p><code>p(y_train | X_train, w) = N(y_train; X_train w, I) ∝ exp(-1/2(y_train - X_train w)^T (y_train - X_train w))</code></p>
<p>where we follow the standard MSE formulation in assuming that the Gaussian variance on y is one.</p>
<p>For real-valued parameters it is common to use a Gaussian as a prior distribution:</p>
<p><code>p(w) = N(w; µ_0, Λ_0)</code></p>
<p>where µ_0 and Λ_0 are the prior distribution mean vector and covariance matrix respectively.</p>
<p>With the prior thus specified, we can now proceed in determining the <strong>posterior</strong> distribution over the model paramater</p>
<p><code>p(w | X, y) ∝ p(y| X, w)p(w)</code></p>
<h2 id="maximum-a-posteriori-map-estimation">Maximum a Posteriori (MAP) Estimation</h2>
<p>Most principled approach is to use posterior distribution to make predictions but sometimes we also want a single point estimate. One common reason for desiring a point estimate is that most operations
involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation. Rather than simply returning to the maximum likelihood estimate, we
can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate. One rational way to do this is to choose the <strong>maximum a posteriori</strong>
point estimate. The MAP estimate chooses the point of maximal posterior probability (or maximal probability density in the more common case of continous θ:
<code>θ_MAP = arg max_θ p(θ|x) = arg max log p(x|θ) + log p(θ)</code>.</p>
<p>we recognize, on the righthand side, <code>log p(x|0)</code>, that is, the standard log-likelihood term, and <code>log p(θ)</code>, corresponding to the prior distribution. Becasuse</p>
<h1 id="supervised-learning-algorithms">Supervised Learning Algorithms</h1>
<p>Learning algorithms that learn to associate some input with some output, given a training set of examples of inputs x and outputs y. In many cases the outputs y must be provided by a human &ldquo;supervisor&rdquo;,
but the term still applies ven when the training set targets were collected automatically.</p>
<h2 id="probabilistic-supervised-learning">Probabilistic Supervised Learning</h2>
<p>A lot of supervised learning algorithms are based on estimating a probability distribution p(y|x). We can do this simply by using maximum likelihood estimation to find the best parameter vector θ for a
parametric family of distributions p(y|x;θ)</p>
<p>e.g. linear regression corresponds to te family
<code>p(y|x;θ) = N(y; θ^T x, I)</code></p>
<p>We can generalize linear regression to the classification scenario by defining a differentfamily of probability distributions. If we have two classes, class 0 and class 1, then we need only specify the
probability of one of these classes. The probability of class 1 determines the probability of class 0, because these two values must add up to 1.</p>
<p>The normal distribution over real-valued numbers that we used for linear regression is parametrized in terms of a mean. Any value we supply for this mean is valid. A distribution over a binary variable
is slighty more complicated, because its mean must always be between 0 and 1. One way to solve this problem is to use the logistic sigmoid function to squash the output of the linear function into the
interval (0, 1) and interpret that value as probability:</p>
<p><code>p(y = 1|x; θ) = σ(θ^T x)</code></p>
<p>This approach is known as logistic regression (a somewhat strange name since we use the model for classification ratger than regression).</p>
<p>In the case of linear regression, we were able to find the optimal weights by solving the normal equations. Logistic regression is somewhat more difficult. There is no closed-form solution for its optimal
weights. Instead, we must search for them by maximizing the log-likelihood. We can do this by minimizing the negative log-likelihood using gradient descent.</p>
<p>This same strategy can be applied to essentially any supervised learning problem, by writing down a parametric family of conditional probability distributions over the right kind of input and output
variables.</p>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<blockquote>
<p>This model is similar to logistic regression in that it&rsquo;s driven by a linear function <code>w^T x + b</code> but does not provide probabilities, but only outputs a class identity. The SVM predicts positive class when <code>w^T x + b</code> is positive otherwise it&rsquo;s negative class.</p>
</blockquote>
<p>One key innovation associated with SVM is the <strong>kernel trick</strong>.
The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples.
e.g. it can be shown that the linear function used by the support vector machine can be rewritten as:
<code>w^T x + b = b + \sum^m_i=1 \alpha_i x^T x_i</code>,
where x_i is a training example, and \alpha is a vector of coefficients. Rewritting the learning algorithm this way enables us to replace x with the output of a given feature function φ(x) and the dot
product with a function <code>k(x, x_i) = φ(x) \cdot φ(x_i)</code></p>
<p>After replacing dot products with kernel evaluations, we can make predictions using the function:
<code>f(x) = b + \sum_i \alpha_i k(x, x_i)</code></p>
<p>This function is nonlinear with respect to x, but the relationship between φ(x) and f(x) is linear. Also, the relationship between \alpha and f(x) is linear. The kernel-based function is exactly
equivalent to preprocessing the data by applying φ(x) to all inputs, then learning a linear model in the new transformed space.</p>
<p>The kernel trick is powerful for two reasons. First, it enables us to learn models that are nonlinear as a function of x using convex optimization techniques that are guaranteed to converge effictiently.
This is possible because we consider φ fixed and optimze only \alpha, that is, the optimization algorithm can view the decision function as being linear in different space. Second, the kernel function k
often admits an implementation that is significantly more computationally efficient than naively constructing two φ(x) vectors and explicitely taking their dot product.</p>
<p>In some cases φ(x) can even be infinite dimensional, which would result in an inffinite computational cost for the naive, explicit approach. In many cases, k(x, x') is a nonlinear, tractable function of x
even when φ(x) is intractable.</p>
<p>The most commonly used kernel is the <strong>Gaussian kernel</strong> also known as <strong>radial basis function</strong> (RBF) kernel, because its value decreases along lines in v space radiating outward from u.
<code>k(u, v) = N(u - v; 0, σ^2 I)</code></p>
<p>We can think of the Gaussian kernel as performing a kind of <strong>template matching</strong>. A training example x associated with training label y becomes a template for class y. When a test point x' is near x
accorfing to Euclidean distanc, the Gaussian kernel has a large response, indicating that x' is very similar to the x template. The model then puts a large weight on the associated training label y.
Overall, the prediction will combine many such training labels weighted by the similarity of the corresponding training examples.</p>
<p>SVM are not the only algorithm that can be enhanced using the kernel trick. Many other linear models can be enhanced in this way.</p>
<blockquote>
<p>The category of algorithms that employ the kernel trick is known as <strong>kernel machines</strong> or <strong>kernel methods</strong></p>
</blockquote>
<p>Training examples in kernel machines are known as <strong>support vectors</strong>.</p>
<p>Kernel machines also suffer from a high computational cost of training when the dataset is large. Kernel machines with generic kernels struggle to generalize well.</p>
<h2 id="other-simple-supervised-learning-algorithms">Other Simple Supervised Learning Algorithms</h2>
<p>We ussually think of k-nearest neighbors as not having any parameters and just implementing a simple function. We can turn it into a supervised learning algorithm by just taking from the training set
the nearest neigbours class. Problem is that it cannot learn that one feature is more discriminative than another.</p>
<p>Another type of learning algorithm that also breaks the input space into regions and has separate parameters for each region is the decision tree and its many variants.</p>
<h1 id="unsupervised-learning-algorithms">Unsupervised Learning Algorithms</h1>
<p>Unsupervised algorithms are those that experience only features but not a supervision signal.
A classic unsupervised learning task is to find the &ldquo;best&rdquo; rpresentation of the data. By best we can mean different things, but generally speaking we are looking for a representation that preserves as
much information about x as possible while eying some penalty or constraint aimed at keeping the representation simpler or more accessible than x itself.</p>
<p>There are multiple ways of defining a simpler representation. Three of the most common include lower-dimensional representations, sparse representations and independent representations.</p>
<p>Low-dimensional representations attempt to compress as much information about x as possible in a smaller representation. Often yield elements that have fewer or weaker endencies than the original
high-dimensional data. This is because one way to reduce the size of a representation is to find and remove redundancies. Identifying and removing more redundancy enables the dimensionality reduction
algorithm to hieve more compression while discarding less information.</p>
<p>Sparse representations embed the dataset into a representation whose entries are mostly zeros for most inputs. The use of sparse representations typically requires increasing the dimensionality of the
representation, so that the representation becoming mostly zeros does not discard too much information.</p>
<p>Independent representations attempt to disentangle the sources of variation underlying the data distribution such that the dimensions of the representation are statistically independent.</p>
<h2 id="principal-components-analysis">Principal Components Analysis</h2>
<p>PCA algorithm provides a means of compressing data. We can also view PCA as an unsupervised learning algorithm that learns a representation of data. This representation is based on two of the criteria
for a simple representation described above. PCA learns a representation that has lower dimensionality than the original input. It also learns a representation whose elements have no linear correlation
with each other. This is a first step toward the criterion of learning representations whose elements are statistically independent. To achieve full independence, a representation learning algorithm must
also remove the nonlinear relationships between variables.</p>
<p>PCA learns an orthogonal, linear transformation of the data that projects an input x to a representation z.</p>
<h1 id="stochaistic-gradient-descent">Stochaistic Gradient Descent</h1>
<p>Nearly all DL is powered by one very imporant algorithm: Stochaistic Gradient Descent (SGD). SGD is an extension of the gradient descent algorithm.</p>
<p>A recurring problem in machine learning is that large training sets are necessary for good generalization, but large training sets are also more computationally expensive.</p>
<p>The cost function used by a machine learning algorithm often decomposes as a sum over training examples of some per-example loss function. e.g. the negative conditional log-likelihood of the training
data can be written as:</p>
<p><code>J(θ) = E_(x,y~p^^_data) L(x, y, θ) = 1/m \sum_i=1^m L(x_i, y_i, θ)</code></p>
<p>where L is the per-example loss <code>L(x, y, θ) = -log p(y | x;θ)</code></p>
<p>The computational cost of this operation is O(m). As the training set size grows to billions of examples, the time to take a single gradient step becomes prohibitively long.</p>
<p>The insight of SGD is that the gradient is an expectation. The expectation may be approximately estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a
<strong>minibatch</strong> of examples drawn uniformly from the training set. The minibatch size is typically chosen to be a relatively small, ranging from one to a few hundred. The minibatch size is usually held
fixed as the training set size grows. We may fit a training set with billions of examples using updates computed on only a hundred examples.</p>
<p>The estimate of the gradient is formed as:</p>
<p><code>g = 1/m' ∇_θ \sum_i=1^m' L(x_i, y_i, θ)</code></p>
<p>where m' is minibatch size. The stochastic gradient descent algorithm then follows the estimated gradient downhill:</p>
<p><code>θ &lt;- θ - eg</code></p>
<p>where e is learning rate.</p>
<p>Gradient descent in general has often been regarded as slow or unreliable. In the past, the application of gradient descent to nonconvex optimization problems was regarded as foolhardy or unprincipled.
day, we know that the machine work very well when trained with gradient descent. The optimization algorithm may not be guaranteed to arrive at even a cal minimum in a reasonable amount of time, but it
often finds a very low value of the cost function quickly enough to be useful.</p>
<p>SGD has many important uses outside the context of DL. It is the main way to train large linear models on very large datasets. For a fixed model size, the cost per SGD update does not depend on the
training set size m. In practice, we often use a larger model as the training set size increases, but we are not forced to do so. The number of updates required to reach convergence usually increases
with training set size. However, as m approaches infinity, the model will eventually converge to its best possible test error before SGD has sampled every example in the training set. Increasing m
further will not extend the amount of training time needed to reach the model’s best possible test error. From this point of view, one can argue that the asymptotic cost of training a model with SGD is
O(1) as a function of m.</p>
<h1 id="building-a-machine-learning-algorithm">Building a Machine Learning Algorithm</h1>
<p>Nearly all deep learning algorithms can be described as particular instances of a fairly simple recipe: combine a speciffication of a dataset, a cost function, an optimization procedure and a mdoel.</p>
<p>By realizing that we can replace any of these components mostly independently from the others, we can obtain a wide range of algorithms.</p>
<p>The cost function typically includes at least one term that causes the learning process to perform statistical estimation. The most common cost function is the negative log-likelihood, so that minimizing
the cost function causes maximum likelihood estimation.</p>
<p>The cost function may also include additional terms, such as regularization terms.</p>
<p>If we change the model to be nonlinear, then most cost functions can no longer be optimized in closed form. This requires us to choose an iterative numerical optimization procedure, such as gradient descent.</p>
<p>The recipe for constructing a learning algorithm by combining models, costs, and optimization algorithms supports both supervised and unsupervised learning. The linear regression example shows how to
support supervised learning. Unsupervised learning can be supported by defining a dataset that contains only X and providing an appropriate unsupervised cost and model.</p>
<p>In some cases, the cost function may be a function that we cannot actually aluate, for computational reasons. In these cases, we can still approximately minimize it using iterative numerical
optimization, as long as we have some way of approximating its gradients.</p>
<p>Most machine learning algorithms make use of this recipe, though it may not be immediately obvious. If a machine learning algorithm seems especially unique or hand designed, it can usually be understood
as using a special-case optimizer. Some models, such as decision trees and k-means, require special-case optimizers because their cost functions have flat regions that make them inappropriate for
minimization by gradient-based optimizers. Recognizing that most machine learning algorithms can be described using this recipe helps to see the different algorithms as part of a taxonomy of methods for
doing related tasks that work for similar reasons, rather than as a long list of algorithms that each have separate justifications.</p>
<h1 id="challenges-motivating-deep-learning">Challenges Motivating Deep Learning</h1>
<p>The development of deep learning was motivated in part by the failure of traditional algorithms to generalize well on AI tasks such as speech/image recognition.</p>
<h2 id="the-curse-of-dimensionality">The Curse of Dimensionality</h2>
<p>Many machine learning problems become exceedingly difficult when the number of dimensions in the data is high. This phenomenon is known as the <strong>curse pf dimensionality</strong>. Of particular concern is that the
number of possible distinct configurations of a set of variables increases exponentially as the number of variables increses.</p>
<p>The curse of dimensionality arises in many places in computer science, especially in machine learning.
One challenge posed by the curse of dimensionality is a statistical challenge.</p>
<p>Statistical challenge arises because the number of possible configurations of x is much larger than the number of training examples.</p>
<p>To understand the issue, let us consider that the input space is organized into a grid. We can describe low-dimensional space with a small number of grid cells that are mostly occupied by the data. When
generalizing to a new data point, we can usually tell what to do simply by inspecting the training examples that lie in the same cell as the new input.</p>
<p>For example, if estimating the probability density at some point x, we can just return the number of training examples in the same unit volume cell as x, divided by the total number of training examples.
If we wish to classify an example, we can return the most common class of training examples in the same cell. If we are doing regression, we can average the target values observed over the examples in
that cell. But what about the cells for which we have seen no example? Because in high-dimensional spaces, the number of configurations is huge, much larger than our number of examples, a typical grid
cell has no training example associated with it. How could we possibly say something meaningful about these new configurations? Many traditional machine learning algorithms simply assume that the output
at a new point should be approximately the same as the output at the nearest training point.</p>
<h2 id="local-constancy-and-smoothness-regularization">Local Constancy and Smoothness Regularization</h2>
<p>To generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn. We have seen these priors incorporated as explicit beliefs in the form of
probability distributions over parameters of the model. More informally, we may also discuss prior beliefs as directly influencing the function itself and influencing the parameters only indirectly, as a
result of the relationship between the parameters and the funtion.
Additionally, we informally discuss prior beliefs as being expressed implicitly by choosing algorithms that are biased toward choosing some class of functions over another, even though these biases may
not be expressed (or even be possible to express) in terms of a probability distribution representing our degree of belief in various functions.</p>
<blockquote>
<p>Among the most widely used of these implicit “priors” is the <strong>smoothness prior</strong> or <strong>local constancy prior</strong>. This prior states that the function we learn should not change very much within small region.</p>
</blockquote>
<p>Many simpler algorithms rely exclusively on this prior to generalize well, and as a result, they fail to scale to the statistical challenges involved in solving AI level tasks.</p>
<p>There are many different ways to implicitly or explicitly express a prior belief that the learned function should be smooth or locally constant. All these different methods are designed to encourage the
learning process to learn a function f∗ that satisfies the condition.</p>
<p><code>f^*(x) ≈ f^x(x + \eps)</code> for most configurations x and small change \eps (learning rate).</p>
<p>In other words, if we know a good answer for an input x (e.g. x is a labeled training example), then that answer is probably good in the neighborhood of x.  If we have several good answers in some
neighborhood, we would combine them (by some form of averaging or interpolation) to produce an answer that agrees with as many of them as much as possible.</p>
<p>An extreme example of the local constancy approach is the k-nearest neighbors family of learning algorithms. These predictors are literally constant over each region containing all the points x that have
the same set of k nearest neighbors in the training set. For k=1, the number of distinguishable regions cannot be more than the number of training examples.</p>
<p>While the k-nearest neighbors algorithm copies the output from nearby training examples, most kernel machines interpolate between training set outputs associated with nearby training examples.</p>
<p>An important class of kernels is the family of <strong>local kernels</strong>, where k(u, v) is large when u=v and decreses as u and v grown further apart from each other (both u and v are vectors). A local kernel
can be thought of as a similarity function that performs template matching, by measuring how closely a test example x resembles each training example x_i. Much of the modern motivation for deep learning
is derived from studying the limitations of local template matching and how deep models are able to succeed in cases where local template matching fails.</p>
<p>Decision trees also suffer from the limitations of exclusively smoothness-based learning, because they break the input space into as many regions as there are leaves and use a separate parameter (or
sometimes many parameters for extensions of decision trees) in each region. If the target function requires a tree with at least n leaves to be represented accurately, then at least n training examples
are required to fit the tree. A multiple of n is needed to achieve some level of statistical confidence in the predicted output.</p>
<p>In general, to distinguish O(k) regions in input space, all these methods require O(k) examples. Typically there are O(k) parameters, with O(1) parameters associated with each of the O(k) regions.</p>
<p>Is there a way to represent a complex function that has many more regions to be distinguished than the number of training examples? Clearly, assuming only
smoothness of the underlying function will not allow a learner to do that. For example, imagine that the target function is a kind of checkerboard. A checkerboard
contains many variations, but there is a simple structure to them. Imagine what happens when the number of training examples is substantially smaller than the
number of black and white squares on the checkerboard. Based on only local generalization and the smoothness or local constancy prior, the learner would be
guaranteed to correctly guess the color of a new point if it lay within the same checkerboard square as a training example. There is no guarantee, however, that
the learner could correctly extend the checkerboard pattern to points lying in squares that do not contain training examples. With this prior alone, the only
information that an example tells us is the color of its square, and the only way to get the colors of the entire checkerboard right is to cover each of its cells with at least one example.</p>
<p>The smoothness assumption and the associated nonparametric learning algorithms work extremely well as long as there are enough examples for the learning
algorithm to observe high points on most peaks and low points on most valleys of the true underlying function to be learned. This is generally true when the
function to be learned is smooth enough and varies in few enough dimensions. In high dimensions, even a very smooth function can change smoothly but in a
different way along each dimension. If the function additionally behaves differently in various regions, it can become extremely complicated to describe with a set of
training examples. If the function is complicated (we want to distinguish a huge number of regions compared to the number of examples), is there any hope to generalize well?</p>
<p>The answer to both of these questions—whether it is possible to represent a complicated function eﬃciently, and whether it is possible for the estimated function to generalize well to new inputs—is yes.
The key insight is that a very large number of regions, such as O(2^k), can be defined with O(k) examples, so long as we duce some dependencies between the regions through additional assumptions about
the underlying data-generating distribution. In this way, we can actually generalize nonlocally. Many different deep learning algorithms provide implicit or explicit assumptions that are reasonable for a
broad range of AI tasks in order to capture these advantages.</p>
<p>Other approaches to machine learning often make stronger, task-specific assumptions. For example, we could easily solve the checkerboard task by providing
the assumption that the target function is periodic. Usually we do not include such strong, task-specific assumptions in neural networks so that they can generalize
to a much wider variety of structures. AI tasks have structure that is much too complex to be limited to simple, manually specified properties such as periodicity, so we want learning algorithms that
embody more general-purpose assumptions.
The core idea in deep learning is that we assume that the data was generated by the composition of factors, or features, potentially at multiple levels in a hierarchy. Many other similarly generic
assumptions can further improve deep learning algorithms. These apparently mild assumptions allow an exponential gain in the relationship between the number of examples and the number of regions that can
be distinguished.</p>
<p>The exponential advantages conferred by the use of deep distributed representations counter the exponential challenges posed by the curse of dimensionality.</p>
<h2 id="manifold-learning">Manifold Learning</h2>
<p>An important concept underlying many ideas in machine learning is that of a manifold. A <strong>manifold</strong> is connected region.  From any given point, the manifold locally appears to be a Euclidean space. In
everyday life, we experience the surface of the world as a 2-D plane, but it is in fact a spherical manifold in 3-D space.
The concept of a neighborhood surrounding each point implies the existence of transformations that can be applied to move on the manifold from one position to a neighboring one. In the example of the
world’s surface as a manifold, one can walk north, south, east, or west.</p>
<p>Although there is a formal mathematical meaning to the term “manifold,” in machine learning it tends to be used more loosely to designate a connected set of points that can be approximated well by
considering only a small number of degrees of freedom, or dimensions, embedded in a higher-dimensional space. Each dimension corresponds to a local direction of variation.
In the context of machine learning, we allow the dimensionality of the manifold to vary from one point to another. This often happens when a
manifold intersects itself. For example, a figure eight is a manifold that has a single dimension in most places but two dimensions at the intersection at the center.</p>
<p>Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn functions with interesting variations across all of R^n.
<strong>Manifold Learnign</strong> algorithms surmount this obstacle by assuming that most of R^n consists of invalid inputs, and that interesting inputs occur only along a collection of manifolds containing a small
subset of points, with interesting variations in the output of the learned function occurring only along directions that lie on the manifold, or with interesting variations happening only when we move
from one manifold to another. Manifold learning was introduced in the case of continuous-valued data and in the unsupervised learning setting, although this probability concentration idea can be
generalized to both discrete data and the supervised learning setting: the key assumption remains that probability mass is highly concentrated.</p>
<p>The assumption that the data lies along a low-dimensional manifold may not always be correct or useful. We argue that in the context of AI tasks, such as those that involve processing images, sounds, or
text, the manifold assumption is at least approximately correct. The evidence in favor of this assumption consists of two categories of observations.</p>
<p>The first observation in favor of the <strong>manifold hypothesis</strong> is that the probability distribution over images, text strings, and sounds that occur in real life is highly concentrated. Uniform noise
essentially never resembles structured inputs from these domains. Similarly, if you generate a document by picking letters uniformly at random, what is the probability that you will get a meaningful
English-language text? Almost zero, again, because most of the long sequences of letters do not correspond to a natural language sequence: the distribution of natural language sequences occupies a very
little volume in the total space of sequences of letters.</p>
<p>Of course, concentrated probability distributions are not suﬃcient to show that the data lies on a reasonably small number of manifolds. We must also establish that the examples we encounter are
connected to each other by other examples with each example surrounded by other highly similar examples that can be reached y applying transformations to traverse the manifold. The second argument in favor
of the manifold hypothesis is that we can imagine such neighborhoods and transformations, at least informally.
In the case of images, we can certainly think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate
objects in the image, gradually alter the colors on the surfaces of objects, and so forth.
Multiple manifolds are likely involved in most applications. For example, the manifold of human face images may not be connected to the manifold of cat face images.</p>
<p>When the data lies on a low-dimensional manifold, it can be most natural for machine learning algorithms to represent the data in terms of coordinates on the
manifold, rather than in terms of coordinates in R^n. In everyday life, we can think of roads as 1-D manifolds embedded in 3-D space. We give directions to specific
addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space.</p>
<p>Extracting these manifold coordinates is challenging but holds the promise of improving many machine learning algorithms. This general principle is applied in many contexts.</p>

    <img src="/images/manifold.png"  alt="manifold"  class="center"  />


]]></content></item><item><title/><link>https://rand-notes.github.io/posts/fi/iv126/ai1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/posts/fi/iv126/ai1/</guid><description/><content type="html"></content></item><item><title/><link>https://rand-notes.github.io/posts/fi/ma012/ma012-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rand-notes.github.io/posts/fi/ma012/ma012-1/</guid><description>z method t test f distribution
Mnohonasobne porovnavani Tukeyho Scheffeho metoda Bartlettuv test
dvojne trideni hypotezy ve dvojnem trideni
linearni modely ANOVA Znam ́enkov ́y test (sign test) Parovy znamenkovy test Jednovyberovy Wilconxonuv test (signed rank test) Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test) Postup testovani ve dvojnem trideni Mnohonasobne porovnavani Dvojne trideni s interakcemi
Testovani v modelu s interakcemi Interpretace indexu determinace R-squared
F test Index determinace
Scheffeho metoda Kruskal-Wallisuv Test</description><content type="html"><![CDATA[<p>z method
t test
f distribution</p>
<p>Mnohonasobne porovnavani
Tukeyho
Scheffeho metoda
Bartlettuv test</p>
<p>dvojne trideni
hypotezy ve dvojnem trideni</p>
<p>linearni modely
ANOVA
Znam ́enkov ́y test (sign test)
Parovy znamenkovy test
Jednovyberovy Wilconxonuv test (signed rank test)
Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)
Postup testovani ve dvojnem trideni
Mnohonasobne porovnavani
Dvojne trideni s interakcemi</p>
<p>Testovani v modelu s interakcemi
Interpretace indexu determinace R-squared</p>
<p>F test
Index determinace</p>
<p>Scheffeho metoda
Kruskal-Wallisuv Test</p>
<h1 id="multiple-comparsion-test">Multiple Comparsion Test</h1>
]]></content></item></channel></rss>