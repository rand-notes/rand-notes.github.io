<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="TOC
self-information nats; bits; shannons;
Shannon entropy
KL Divergence
CrossEntropy
Factorization
Structured Probabilistic Model
Directed
Undirected The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions;
Self-information we rate information value much higher if information is less likely and independent.
I(x) = -log(P(x))
we are using natural logarithm with base e so I(x) is in units of nats. One nat is the amount fo information gained by observing an event of probability 1/e."><meta name=keywords content><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rand-notes.github.io/dl1/><title>DL1 :: idk</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css><meta itemprop=name content="DL1"><meta itemprop=description content="TOC
self-information nats; bits; shannons;
Shannon entropy
KL Divergence
CrossEntropy
Factorization
Structured Probabilistic Model
Directed
Undirected The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions;
Self-information we rate information value much higher if information is less likely and independent.
I(x) = -log(P(x))
we are using natural logarithm with base e so I(x) is in units of nats. One nat is the amount fo information gained by observing an event of probability 1/e."><meta itemprop=datePublished content="2021-01-01T00:00:00+00:00"><meta itemprop=dateModified content="2021-01-01T00:00:00+00:00"><meta itemprop=wordCount content="534"><meta itemprop=image content="https://rand-notes.github.io"><meta itemprop=keywords content><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rand-notes.github.io"><meta name=twitter:title content="DL1"><meta name=twitter:description content="TOC
self-information nats; bits; shannons;
Shannon entropy
KL Divergence
CrossEntropy
Factorization
Structured Probabilistic Model
Directed
Undirected The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions;
Self-information we rate information value much higher if information is less likely and independent.
I(x) = -log(P(x))
we are using natural logarithm with base e so I(x) is in units of nats. One nat is the amount fo information gained by observing an event of probability 1/e."><meta property="og:title" content="DL1"><meta property="og:description" content="TOC
self-information nats; bits; shannons;
Shannon entropy
KL Divergence
CrossEntropy
Factorization
Structured Probabilistic Model
Directed
Undirected The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions;
Self-information we rate information value much higher if information is less likely and independent.
I(x) = -log(P(x))
we are using natural logarithm with base e so I(x) is in units of nats. One nat is the amount fo information gained by observing an event of probability 1/e."><meta property="og:type" content="article"><meta property="og:url" content="https://rand-notes.github.io/dl1/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="dl"><meta property="article:published_time" content="2021-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2021-01-01T00:00:00+00:00"><meta property="og:site_name" content="idk"><meta property="article:published_time" content="2021-01-01 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo>title</div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://rand-notes.github.io/dl1/>DL1</a></h2><div class=post-content><p>TOC</p><ul><li><p><a href=#self-information>self-information</a>
<strong>nats</strong>; <strong>bits</strong>; <strong>shannons</strong>;</p></li><li><p><a href=#shannon-entropy>Shannon entropy</a></p></li><li><p><a href=#kl-divergence>KL Divergence</a></p></li><li><p><a href=#crossentropy>CrossEntropy</a></p></li><li><p><a href=#factorization>Factorization</a></p></li><li><p><a href=#structured-probabilistic-model>Structured Probabilistic Model</a></p><ul><li><p><a href=#directed>Directed</a></p></li><li><p><a href=#undirected>Undirected</a>
<strong>The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions</strong>;</p></li></ul></li></ul><h1 id=self-information>Self-information</h1><p>we rate information value much higher if information is less likely and independent.</p><p><code>I(x) = -log(P(x))</code></p><p>we are using natural logarithm with base <code>e</code> so I(x) is in units of <strong>nats</strong>. One nat is the amount fo information gained by observing an event of probability <code>1/e</code>. Other texts use base-2 logarithms and
units called <strong>bits</strong> or <strong>shannons</strong>. Information measured in bits is just a rescaling of information measured in nats.</p><h1 id=shannon-entropy>Shannon entropy</h1><p><code>H(x) = E[I(x) = E[log P(x)]]</code></p><p>when x is continuous the Shannon entropy is known as the differential entropy.</p><h1 id=kl-divergence>KL Divergence</h1><p>we are basically measuring distance between two distributions.
<code>D_{KL}(P||Q) = E[log(P(x)/Q(x))]</code></p><p>important is that <code>D(P||Q) != D(Q||P)</code> and <code>D(P||Q) >= 0</code>.</p><p>if both distributions are same than KL divergence is 0.</p><p>let&rsquo;s say we have distribution representing true labels and models predictions. We can rate model with KL divergence.</p><p><code>D(P||Q) = CrossEntropy(P, Q) - Entropy(P)</code></p><h1 id=crossentropy>CrossEntropy</h1><p>Similar to KL Divergence</p><p><code>H(P, Q) = H(P) + D_{KL}(P||Q)</code></p><h1 id=factorization>Factorization</h1><p>Factoring consists of writing a number or another mathematical object as a product of several factors, usually smaller or simpler objects of the same kind</p><h1 id=structured-probabilistic-model>Structured Probabilistic Model</h1><p>In ML we are working with a large number of random variables. Often these distributions are involved in direct interactions between relatively few variables. And instead difficulty crafting single
function we can split distribution into many factors that we multiply together. These factorization can greatly reduce the number of parameters needed to describe the distribution.</p><p>We can describe these kinds of factorizations using graphs that we call Structured Probabilistic Model or Graphical Model.
Could be directed or undirected. Each node represents random variable and edge represents direct interactions between two random variables.</p><h2 id=directed>Directed</h2><p>with directed edges; represent factorization into conditional probability distributions. A directed model contains one factor for every random variable x_i in the distribution and that factor consits of
the conditional distribution over x_i given the parents of x_i, denoted P_ag(x_i):</p><p>pi - capital Pi for multiplication</p><p><code>p(x) = pi(p(x_i | P_ag(x_i)))</code></p><img src=/images/directed.png alt=Directed class=center><h2 id=undirected>Undirected</h2><p>with undirected edges; represent factorizations into a set of functions. Unlike in directed case, these functions are usually not probability distributions.
Any set of nodes that are all connected to each other i G is called clique. Each clique c_i is associated with a factor o_i. These factors are functions and the output of each factor must be
non-negative, but there is no constint that the factor must sum or integrate to 1 like probability distribution.</p><img src=/images/undirected.png alt=undirected class=center><p>The probability of a configuration of random variables is proportional to the product od all these factors. Because we have no guarantee that the product will sum to 1, we divide by a normalizing
constant Z, defined to be the sum or integral over all states of the prodcut of the o fcuntions in order to obtain a normalized probability distribution:</p><p><code>p(x) = 1/Z * pi(o_i * (C_i))</code></p><p><strong>The graph are just representation, any probability distribution can be described by both graphs. Graphs are not property of distributions</strong></p></div></article><hr><div class=post-info></div></main></div><footer class=footer></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></div><script type=text/javascript src=/bundle.min.e8a56c89d5ca304d6922b22e38d0af2db97fa70a6623ba17a492092da773dfa4b9aaffa0f682ecfd03d7c7964e89bdbef18f0d4183c698831978c7ca44959d10.js integrity="sha512-6KVsidXKME1pIrIuONCvLbl/pwpmI7oXpJIJLadz36S5qv+g9oLs/QPXx5ZOib2+8Y8NQYPGmIMZeMfKRJWdEA=="></script></body></html>