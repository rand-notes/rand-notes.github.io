<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=author content>
<meta name=description content="Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).
Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 .">
<meta name=keywords content>
<meta name=robots content="noodp">
<meta name=theme-color content>
<link rel=canonical href=https://rand-notes.github.io/iv111/exam/>
<title>
exam :: idk
</title>
<link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css>
<link rel=stylesheet href=/main.7209ab6422eb371a6b685a56274cc88eff3db604665c3bdb698389c0585d4211.css>
<meta itemprop=name content="exam">
<meta itemprop=description content="Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).
Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 .">
<meta itemprop=wordCount content="744"><meta itemprop=image content="https://rand-notes.github.io">
<meta itemprop=keywords content>
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://rand-notes.github.io">
<meta name=twitter:title content="exam">
<meta name=twitter:description content="Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).
Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 .">
<meta property="og:title" content="exam">
<meta property="og:description" content="Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).
Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 .">
<meta property="og:type" content="article">
<meta property="og:url" content="https://rand-notes.github.io/iv111/exam/"><meta property="og:image" content="https://rand-notes.github.io"><meta property="article:section" content="fi">
<meta property="og:site_name" content="idk">
</head>
<body>
<div class=container>
<header class=header>
<span class=header__inner>
<a href=/ style=text-decoration:none>
<div class=logo>
title
</div>
</a>
<span class=header__right>
<nav class=menu>
<ul class=menu__inner><li><a href=/posts>notes</a></li>
</ul>
</nav>
<span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg>
</span>
<span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span>
</span>
</span>
</header>
<div class=content>
<main class=post>
<div class=post-info>
</p>
</div>
<article>
<h2 class=post-title><a href=https://rand-notes.github.io/iv111/exam/>exam</a></h2>
<div class=post-content>
<p>Events A1, A2, . . . An are (mutually) independent iff for any set {i1, i2, . . . ik }⊆{1 . . . n} (2 ≤k ≤n) of distinct indices it holds that P (Ai1 ∩Ai2 ∩···∩Aik ) = P (Ai1 )P (Ai2 ) . . . P (Aik ).</p>
<p>Events A1, A2, . . . An are pairwise independent iff for all distinct indices i , j ∈{1 . . . n} it holds that P (Ai ∩Aj ) = P (Ai )P (Aj ).</p>
<h1 id=law-of-total-probability>Law of Total Probability</h1>
<p>prob of A given event space, gives prob of A</p>
<p>Let A be an event and {B1, . . . , Bn } be an event space. Then \( P(A) = \sum_{i=1}^{n} P(A|B_i) P(B_i) \)</p>
<h1 id=random-variable>Random Variable</h1>
<p>to each sample point (HEAD/TAIL) assigns a real number.</p>
<p>def: A (discrete) random variable X on a sample space S is a function X : S→R that assigns a real number X(s) to each sample point s∈S.</p>
<img src=/images/iv111/random_variable.jpg alt="random variable" class=center>
<hr>
<p>For a random variable X and a real number x , we define <strong>inverse image</strong> of x to be the event</p>
<p>\( [X = x] = {s \in S | X(s) = x} \)</p>
<p>i.e. the set of all sample points from S to which X assigns the value x.</p>
<h1 id=probability-mass-function-pmf>Probability Mass Function PMF</h1>
<p>probability for discrete</p>
<p>def: Probability mass function (or probability distribution) of a random variable X is a function pX : R → [0,1] given by</p>
<p>\( p_X(x) = P(X = x) = \sum_{s:X(s)=x} P(s) \)</p>
<h1 id=cumulative-distribution-function---cdf>Cumulative Distribution Function - CDF</h1>
<p>def: The cumulative distribution function (probability distribution function or simply distribution functiona) of a random variable X is a function FX : R→[0,1] given by</p>
<p>\( F_X(x) = P (X \leq x) = \sum_{t \leq x} p_X(t) \)</p>
<h1 id=probability-density-function>Probability Density Function</h1>
<p>def: (or density) of a random variable X is a derivative function of the cumulative distribution function.</p>
<h1 id=discrete-random-vectors>Discrete Random Vectors</h1>
<p>The joint (or compound) probability distribution of a random vector X is defined to be</p>
<p>\( p_X(x) = P(X=x) = P(X_1 = x_1 \and X_2 = x_2 \and &mldr;) \)</p>
<p>The joint (or compound) distribution functionaof a random vector X is defined to be</p>
<p>\( F_X(x) = P(X_1 \leq x_1 \and X_2 \leq x_2 \and &mldr;) \)</p>
<p>Let pX(x) be a joint probability distribution of a random variable X = (X1,X2). The marginal probability distribution of X1 is defined as</p>
<p>\( p_X_1(x) = P(X1=x) = \sum_{x_2 \in lm(X_2)} p_X((x ,x2)) \)</p>
<h1 id=independent-random-variables>Independent Random Variables</h1>
<p>X and Y are independent:</p>
<p>\( p_{X,Y}(x, y) = p_X(x)p_Y(y) \) for all x and y</p>
<p>we can use there also F instead of p</p>
<h1 id=expectation>Expectation</h1>
<p>Expectation of a random variable X is defined as</p>
<p>E(X) = \sum_{x \in lm(X)} x \cdot P(X=x)</p>
<p>provided the sum is absolutely convergent. In case the sum is convergent, but not absolutely convergent, we say that no finite expectation exists.
In case the sum is not convergent, the expectation has no meaning.</p>
<h1 id=linearity-of-expectation>Linearity of Expectation</h1>
<h2 id=expectation-of-sum>Expectation of sum</h2>
<p>Let X and Y be random variables. Then \( E(X+Y) = E(X) + E(Y) \)</p>
<h2 id=multiplication-by-a-constant>Multiplication by a constant</h2>
<p>Let X be random variable and c be a real number. Then \( E(cX) = cE(X) \)</p>
<h2 id=expectation-of-independent-random-variables>Expectation of Independent Random Variables</h2>
<p>If X and Y are independent random variables, then \( E(XY) = E(X)E(Y) \)</p>
<h2 id=jensons-inequality>Jensons Inequality</h2>
<p>\( E[f(X)] \geq f(E(X)) \)</p>
<h2 id=observation>Observation</h2>
<p>If Im(X) \in N then \( E(X) = \sum_{i=1}^{\inf} P(X \geq i) \)</p>
<h1 id=conditional-probability>Conditional probability</h1>
<p>p_{X|Y}(x|y) = P(X=x|Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)} = \frac{p_{X,Y}(x,y)}{p_Y(y)}</p>
<h1 id=markov-inequality>Markov inequality</h1>
<p>def: Let X be a nonnegative random variable with finite mean value E(X). Then for all t > 0 it holds that:</p>
<p>\( P(X \geq t) \leq \frac{E(X)}{t} \)</p>
<h1 id=moments>Moments</h1>
<p>The k th moment of a random variable X is defined as E (X^k).
Note that for k = 1 we get the expectation of X</p>
<p>Usually we center the expected value to 0 – we use moments of X - E(X)
i.e. μ_k = E([X-E(X)]^k)</p>
<h1 id=variance>Variance</h1>
<p>The second central moment is known as the variance</p>
<p>Var(X) = E(X^2) - [E(X)]^2</p>
<h1 id=covariance>Covariance</h1>
<p>Cov(X,Y) = E(XY) − E(X)E(Y)</p>
<p>Covariance measures linear dependence between two random variables</p>
<p>We define the correlation coefficient ρ(X ,Y ) as the normalized covariance, i.e.</p>
<p>ρ(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}</p>
<p>For independent random variables X and Y , it holds that Cov (X ,Y ) = 0.</p>
<p>( X and Y independent \Rightarrow Cov(X,Y) = 0 )
( X and Y independent \not\Leftarrow Cov(X,Y) = 0 )</p>
</div>
</article>
<hr>
<div class=post-info>
</div>
</main>
</div>
<footer class=footer>
</footer>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
</div>
<script type=text/javascript src=/bundle.min.b2a4161afad463d91113a005f8f24a5efff27a6cc48754e9e93adc7bccb8a876a153a953c0b12653f3b5910e41e620da731f8184f2973058100d159c09fa8346.js integrity="sha512-sqQWGvrUY9kRE6AF+PJKXv/yemzEh1Tp6Trce8y4qHahU6lTwLEmU/O1kQ5B5iDacx+BhPKXMFgQDRWcCfqDRg=="></script>
</body>
</html>