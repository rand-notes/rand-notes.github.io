<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=author content>
<meta name=description content="Expectation of Random Variables  Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value  Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists.">
<meta name=keywords content>
<meta name=robots content="noodp">
<meta name=theme-color content>
<link rel=canonical href=http://localhost/iv111/0/>
<title>
IV111 lec 03 :: idk
</title>
<link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css>
<link rel=stylesheet href=/main.a8c3cebdaa6aa6bd29a03d964d02f21ed25c63e2f368f62e3585a5244491acbf.css>
<meta itemprop=name content="IV111 lec 03">
<meta itemprop=description content="Expectation of Random Variables  Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value  Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists."><meta itemprop=datePublished content="2021-09-26T00:00:00+00:00">
<meta itemprop=dateModified content="2021-09-26T00:00:00+00:00">
<meta itemprop=wordCount content="1231"><meta itemprop=image content="http://localhost">
<meta itemprop=keywords content>
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="http://localhost">
<meta name=twitter:title content="IV111 lec 03">
<meta name=twitter:description content="Expectation of Random Variables  Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value  Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists.">
<meta property="og:title" content="IV111 lec 03">
<meta property="og:description" content="Expectation of Random Variables  Often we need a shorter description than PMF or CDF - single number, or a few number. First such characteristic describing a random variable is the expectation, also known as the mean value  Expectation of a random varialbe X is defined as:
$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$
provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost/iv111/0/"><meta property="og:image" content="http://localhost"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-09-26T00:00:00+00:00">
<meta property="article:modified_time" content="2021-09-26T00:00:00+00:00"><meta property="og:site_name" content="idk">
<meta property="article:published_time" content="2021-09-26 00:00:00 +0000 UTC">
</head>
<body>
<div class=container>
<header class=header>
<span class=header__inner>
<a href=/ style=text-decoration:none>
<div class=logo>
title
</div>
</a>
<span class=header__right>
<nav class=menu>
<ul class=menu__inner><li><a href=/posts>notes</a></li>
</ul>
</nav>
<span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg>
</span>
<span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span>
</span>
</span>
</header>
<div class=content>
<main class=posts>
<div class=posts-info>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
6 minutes
</p>
</div>
<article>
<h1 class=posts-title>
<a href=http://localhost/iv111/0/>IV111 lec 03</a>
</h1>
<div class=posts-content>
<h1 id=expectation-of-random-variables>Expectation of Random Variables</h1>
<ul>
<li>Often we need a shorter description than PMF or CDF - single number, or a few number.</li>
<li>First such characteristic describing a random variable is the <strong>expectation</strong>, also known as the <strong>mean value</strong></li>
</ul>
<p>Expectation of a random varialbe X is defined as:</p>
<p>$$E(X) = \sum_{x\in lm(X)} x \cdot P(X = x)$$</p>
<p>provided the sum is absolutely convergent. In case the sum is convergent, but not absoutely convergent, we say that no finite expectation exists. In case the sum is not convergent, the expectation has
not meaning.</p>
<p>for continuous variable:</p>
<p>$$E(X) = \int x \cdot f_X (x) dx$$</p>
<h2 id=discrete-uniform-probability-distribution>Discrete Uniform Probability Distribution</h2>
<p>e.g. value on 6-sided die</p>
<p>Let
\(lm(X) = {x_1, x_2,&mldr;, x_n}\)</p>
<p>The expectation of X is:
$$E(X) = \sum_{x\in lm(X)} x\cdot P(X=x) = \sum_{x\in lm(X)} x\cdot\frac{1}{n} = \frac{1}{n} \sum^n_{i=1} x_i $$</p>
<h2 id=bernoulli-probability-distribution>Bernoulli Probability Distribution</h2>
<p>Bernoulli probability distribution has one parameter p and models single Bernoulli trial or (biased) coin toss.</p>
<p>the distribution is given by:</p>
<p>$$p_X(x) = P(X = 0) = 1 -p$$
$$p_X(x) = P(X = 0) = p$$</p>
<p>the expectation is:</p>
<p>$$E(X) = 0 \cdot (1 - p) + 1 \cdot p = p$$</p>
<h2 id=indicator-random-variable>Indicator Random Variable</h2>
<p>Indicator random variable of an event A.</p>
<p>the indicator of an event A is the random variable I_A defined by:</p>
<p>\( I_A(s) = 1\) if \(s \in A \)</p>
<p>and</p>
<p>\( I_A(s) = 0\)if \(s \notin A\)</p>
<p>The probability distribution is:</p>
<p>$$p_{I_A}(0) = P(\overline{A}) = 1 - P(A)$$
$$p_{I_A}(1) = P(A)</p>
<p>the distribution function reads</p>
<p>$$
F_{I_A}(x) =
\begin{cases}
\displaylines{0 \hskip 1em \text{for} x \lt 0 \\ P(\overline{A}) \hskip 1em \text{for} 0 \leq x \geq 1) \\ 1 \hskip 1em \text{for} x \geq 1}
\end{cases}
$$</p>
<p>It is a Bernoulli distribution, and so \( E(I_A) = P(A) \)</p>
<h2 id=constant-random-variable>Constant Random Variable</h2>
<p>For \(c \in \mathbb{R}\) the function defined for all \(s \in S \text{ by } X(s) = c\)</p>
<p>The probability distribution of this variable is:</p>
<p>$$
p_X(x) =
\begin{cases}
\displaylines{1 \hskip 1em \text{ if } x = c \\ 0 \hskip 1em \text{otherwise}}
\end{cases}
$$</p>
<p>The cumulative distribution function is:</p>
<p>$$
F_X(x) =
\begin{cases}
\displaylines{0 \hskip 1em \text{ for } x \lt c \\ 1 \hskip 1em \text{for} x \geq c}
\end{cases}
$$</p>
<p>The expectation:</p>
<p>$$E(X) = 1 \cdot c = c$$</p>
<h2 id=geometric-probability-distribution>Geometric Probability Distribution</h2>
<p>Geometric probability distribution has one parameter p and models the number of Bernoulli trials until the first &lsquo;success&rsquo; occurs.</p>
<p>The probability function</p>
<p>$$p(x) = P(X = x) = (1 - p)^{x-1}p$$</p>
<p>The probability distribution function:</p>
<p>$$F(x) = P(X \leq x) = 1 - (1 - p)^{x}p$$</p>
<p>The expectation is</p>
<p>$$E(X) = \sum_{x \in lm(X)} x \cdot P(X = x)$$</p>
<h2 id=binomial-probability-distribution>Binomial Probability Distribution</h2>
<p>Binomial random variable X, denoted by B(n, p)</p>
<p>The probability distribution is:</p>
<p>$$
p_X(x) = P(x = x) =
\begin{cases}
\displaylines{\binom{n}{x} p^x (1 - p)^{n-x} \hskip 1em \text{for } 0 \leq x \geq n \\ 0 \hskip 5em \text{otherwise}}
\end{cases}
$$</p>
<p>the expectation is</p>
<p>$$E(X) = \sum_{x \in lm(X)} x \cdot P(X = x) = &mldr; = np$$</p>
<p>or n independent Bernoulli trials = np. To use the easiest way, we need to define independence on random variables. Hence, we need more random variables.</p>
<h1 id=functions-of-random-variables>Functions of Random Variables</h1>
<h2 id=sum-of-independent-random-variables>Sum of Independent Random Variables</h2>
<p>Let X and U be independent random variables. Let \( Z = X + Y \) is a new random variable defined as sum of X and Y. If X and Y are non-negative integer values, the probability distribution of Z is</p>
<p>$$ p_Z(t) = p_{X+Y}(t) = \sum^t_{x=0} p_X(x) p_Y(t-x)$$</p>
<blockquote>
<p>Observe that</p>
<ul>
<li>Sum of two Bernoulli distributions with probability of success p is a binomial distribution B(2, p)</li>
<li>Sum of two binomial distributions B(n_1, p) and B(n_2, p) is a binomial distribution B(n_1 + n_2, p)</li>
</ul>
</blockquote>
<h3 id=sums-of-distributions>Sums of Distributions</h3>
<p>Bernoulli + Bernoulli = Binomial
Binomial + Binomial = Binomial
Geometric + Geometric = Negative Binomial
Negative Binomial + Negative Binomial = Negative Binomial
Poisson + Poisson = Poisson
Exponential + Exponential = Erlang
Erlang + Erlang = Erlang</p>
<h2 id=negative-binomial-distribution>Negative Binomial Distribution</h2>
<p>Negative binomial distribution has two parameters p and r and expresses the number of Bernoulli trials to the r th success. Hence, it is a convolution of r geometric distributions.</p>
<p>The probability function
$$
p_X(x) = P(X = x) =
\begin{cases}
\displaylines{\binom{x - 1}{r - 1} p^r (1 - p)^{x - r} \hskip 1em \text{ for } x \geq r \\ 0 \hskip 2em \text{otherwise}}
\end{cases}
$$</p>
<p>The expectation is
$$
E(X) = \sum_{x \in lm(x)} xP(X = x) =
$$</p>
<p>The probability distribution - pr for r successes, \( (1 −p)^{x −r} \) failures, the last is successs, hence, \( \binom{x −1}{r −1} \) stands for placement of x − 1 successes in r − 1 trials.</p>
<h2 id=linearity-of-expectation>Linearity of Expectation</h2>
<h3 id=theorem-expecation-of-sum>Theorem (Expecation of sum)</h3>
<p>Let X and Y be random variables. Then</p>
<p>$$E(X + Y) = E(X) + E(Y)$$</p>
<p>//TODO: proof</p>
<h3 id=theorem-multiplication-by-a-constant>Theorem (Multiplication by a constant)</h3>
<p>Let X be random variable and c be a real number. Then
$$E(cX) = cE(X)$$</p>
<p>In general, the above theorem implies the following result for any linear combination of n random variables i.e.</p>
<h3 id=corollary-linearity-of-expectation>Corollary (Linearity of expectation)</h3>
<p>Let \( X_1, X_2,&mldr;, X_n \) be random variables and \(c_1,c_2,&mldr;,c_n \in \mathbb{R} \). Then</p>
<p>$$
E(\sum^n_{i=1}c_i X_i) = \sum^n_{i=1} c_i E(X_i)
$$</p>
<h2 id=expectation-of-independent-random-variables>Expectation of Independent Random Variables</h2>
<h3 id=theorem>Theorem</h3>
<p>If X and Y are independent random variables, then:</p>
<p>$$
E(XY) = E(X) E(Y)
$$</p>
<p>//TODO: proof
//TODO: observation</p>
<h1 id=conditional-expectation>Conditional Expectation</h1>
<p>Using the derivation of conditional probability of two events, we can derive conditional probability of (a pair of) random variables.</p>
<h2 id=definition>Definition</h2>
<p>The conditional probability distribution of random variable X given random variable Y (where \( p_{X ,Y} (x , y) \) is their joint distribution) is</p>
<p>p_{X|Y}(x|y) = P(X = x|Y = y) = \frac{P(X = x,Y = y)}{P(Y = y)} = \frac{p_{X, Y}(x, y)}{p_Y(y)}</p>
<p>provided the marginal probability \( p_Y(y) \neq 0 \)</p>
<hr>
<p>We may consider Y|(X = x) to be a new random variable that is given by the conditional probability distribution \( p_{Y|X} \). therefore, we can define its mean.</p>
<h2 id=definition-1>Definition</h2>
<p>The <strong>conditional expectation</strong> of Y given X = x is defined</p>
<p>$$
E(Y|X = x) = \sum_y yP(Y = y|X = x) = \sum_y yp_{Y|X}(y|x)
$$</p>
<p>We can derive the expectation of Y from the conditional expectations.</p>
<h2 id=theorem-of-total-expectation>Theorem of total expectation</h2>
<p>Let X and Y be random variables, then</p>
<p>$$
E(Y) = \sum E(Y|X = x)p_X(x)
$$</p>
<p>//TODO random sums example</p>
<h1 id=markov-inequality>Markov Inequality</h1>
<p>It is important to derive as much information as possible even from a partial description of random variable. The mean value already gives more information than one might expect, as captured by Markov
inequality.</p>
<h2 id=theorem-markov-inequality>Theorem (Markov inequality)</h2>
<p>Let X be a nonnegative random variable with finite mean value E (X ). Then for all t >0 it holds that</p>
<p>$$
P(X \geq t) \leq \frac{E(X)}{t}
$$</p>
<p>//TODO: examples</p>
<h2 id=proof>Proof</h2>
<p>Let t > 0. We define a random variable \( Y_t \) (for fixed t ) as</p>
<p>$$
Y_t =
\begin{cases}
\displaylines{0 \hskip 1em \text{ if } X \lt t \\ t \hskip 1em X \geq t}
\end{cases}
$$</p>
<p>Then \( Y_t \) is a discrete random variable with probability distribution \( p_{Y_{y}}(x) = P(X \lt t), p_{Y_{t}}(t) = P(X \geq t) \) . We have</p>
<p>$$
E(Y_t) = tP(X \geq t)
$$</p>
<p>The observation \( X \geq Y_t \) gives</p>
<p>$$
E(X) \geq E(Y_y) = tP(X \geq t)
$$</p>
<p>what is the Markov Inequality</p>
</div>
</article>
<hr>
<div class=posts-info>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
1231 Words
</p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
2021-09-26
</p>
</div>
<div class=pagination>
<div class=pagination__title>
<span class=pagination__title-h></span>
<hr>
</div>
<div class=pagination__buttons>
<span class="button next">
<a href=http://localhost/iv126/0/>
<span class=button__text>ai 0</span>
<span class=button__icon>→</span>
</a>
</span>
</div>
</div>
</main>
</div>
<footer class=footer>
</footer>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
</div>
<script type=text/javascript src=/bundle.min.ba70d1dfb9f72657fd85f4384c3fbc99234f4a7f180d2f0acd861c2fc45ea30291b3ead9386c3e9a2660fc88bf39b5d99de30d383ec320962bdb830140169fae.js integrity="sha512-unDR37n3Jlf9hfQ4TD+8mSNPSn8YDS8KzYYcL8ReowKRs+rZOGw+miZg/Ii/ObXZneMNOD7DIJYr24MBQBafrg=="></script>
</body>
</html>