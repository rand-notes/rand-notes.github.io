---
date: 2021-10-02
url: ma012/1
title: "ma012 1"
---

- z method
- f distribution
- t test
- F test 


- Mnohonasobne porovnavani
- Tukeyho
- Scheffeho metoda
- Bartlettuv test
- Kruskal-Wallisuv Test
- Jednovyberovy Wilconxonuv test (signed rank test)
Dvouv ́ybˇerov ́y Wilcoxon ̊uv test (rank-sum test)

---

dvojne trideni 
hypotezy ve dvojnem trideni

linearni modely
Znam ́enkov ́y test (sign test)
Parovy znamenkovy test
ANOVA
Postup testovani ve dvojnem trideni
Dvojne trideni s interakcemi

Testovani v modelu s interakcemi
Interpretace indexu determinace R-squared

Index determinace




# Error vs Residual

> Error (or disturbance) is theis the deviation of the observed value from the (unobservable) true value of quantity of interest

> Residual is the difference between the observed value and the estimated value of the quantity of interest

Residual = observed value - predicted value: \\(e = y - \hat{y}\\)

The distinction is most important in regression analysis, where the concepts are sometimes called the **regression errors** and **regression residuals** and were they lead to the concept of studentized
residuals.


# Degrees of freedom

> Degrees of freedom is the number of values in the final calculation of a statistic that are free to vary


# Z-test

> A z-test is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large1.

The test statistic is assumed to have a normal distribution, and nuisance parameters such as standard deviation should be known in order for an accurate z-test to be performed.

- is a hypothesis test in which the z-statistic follows a normal distribution
- A **z-statistic**, or **z-score**, is a number representing the result from the z-test
- are closely related to t-tests, but t-tests are best performed when an experiment has a small sample size. 
- assume the standard deviation is known, while t-tests assume it is unknown.


# t-distribution

*or Student's t-distribution*

> is any continuous probability distribution that arise when astimating the mean of a **normally distributed** population in situations where the sample size is small and the population's standard
> deviation is unknown.

# t-test

*or student's t-test*
> is any statistical hypothesis test in which the test statistic follows a Student's t-dristribution under the null hypothesis.

Used to determine wheter the means of two groups are equal to each other. The assumption for the test is that both groups are sampled from normal distributions with equal variances. The null hypothesis
is that the two means are equal, and the alternative is that they are not. It is known that under the null hypothesis, we can calculate a t-statistic that will follow a t-distribution with \\( n1 + n2 -
2 \\) degrees of freedom.

There is also a widely used modification of the t-test, known as **Welch's t-test** that adjusts the number of degrees of freedom when the variances are thought not to be equal to each other.


```R
x = rnorm(10)
y = rnorm(10)
t.test(x, y)

# 	Welch Two Sample t-test
# 
# data:  x and y
# t = -1.6264, df = 16.666, p-value = 0.1226
# alternative hypothesis: true difference in means is not equal to 0
# 95 percent confidence interval:
#  -1.4494854  0.1886137
# sample estimates:
#   mean of x   mean of y 
# 0.007872345 0.638308171
```


# F-distribution

*or F-ratio also known as Snedecor's F distribution or the Fisher–Snedecor distribution*

> is a continuous probability distribution that arises frequently as the null distribution of a test statistic, most notably in the ANOVA and other F-tests.

The F-distribution with \\(d_1\\) and \\(d_2\\) degrees of freedom is the distribution of:

$$X = \frac{S_1/d_1}{S_2/d_2}$$

where \\(S_1\\) and \\(S_2\\) are independent random variables with chi-square distributions respective degrees of freedom \\(d_1\\) and \\(d_2\\)

# F-test

> is any statistical test in which the test statistic has an F-distribution under the null hypothesis.

It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exact "F-tests"
mainly arise when the models have been fitted to the data using least squares.

The test statistic can be obtained by computing the ration of the two variances \\( S_A^2 \\) and \\( S_B^2 \\)

$$F = \frac{S_A^2}{S_B^2}$$

The degrees of freedom are \\( n_A - a \\) (for the numerator) and \\( n_B -1 \\) (for the denominator)

# Chi-square distribution

*also chi-square or \\( \chi^2 \\)-distribution*

> with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. 

if \\(Z_1, ..., Z_k \\) are independent, standard normal random variables, then sum of their squares:

$$Q=\sum^k_{i=1} Z_i^2$$

is distributed according to the chi-squared distribution with k degrees of freedom. This is usually denoted as:

$$Q \sim \chi^2 (k) \text{ or } Q \sim \chi^2_k $$

The chi-squared distribution has one parameter: a positive integer k that specifies the number of degrees of freedom (the number of random variables being summed, Zi s)



# Multiple Comparisons Problem

*or multiple comparisons, multiplicity or multiple testing problem*

> occurs when one considers a set of statistical inferences simultaneously or inders a subset of parameters selected based on the observed values.


## Tukey's Range Test

*or Tukey's test, Tukey method ...*

> is a single-step multiple comparsion procedure and statistical test. It can be used to find means that are significantly different from each other.

Tukey's test is based on a formula very similar to that of the t-test. In fact, Tukey's test is essentially a t-test, except that it corrects for family-wise error rate.

The formula for Tukey's test is:

$$q_s = \frac{Y_A - Y_B}{SE}$$

where Y_A is the larger of the two means being compared, Y_B is the smaller, and SE is the standard error of the sum of the means.


## Scheffé's method

> is a method for adjusting significance levels in a linear regression analysis to account for multiple comparsions.

It is particularly useful in ANOVA and in constructing simultaneous confidence bands for regressions involving basis functions.

# Bartlett's test

> is used to test homoscedasticity, that is, if multiple samples are from populations with equal variances.

Some statistical tests, such as the ANOVA assume that variances are equal across groups or samples, which can be verified with Bartlett's test.

# Levene's test

> is an inferential statistic used to asses the equality of variances for a variable calculated for two or more groups.

# Sign Test

> is a statistical method to test for consistent differences between pairs of observations

# Wilcoxon signed-rank test

*Jednovýběrový Wilcoxonův párový test*

> is a non-parametric statistical hypothesis test used either to test the location of a set of samples or to compare the locations of two populations using a set of matched samples.


# Mann–Whitney U test

*Dvouvýběrový Wilcoxonův párový test or Wilcoxon rank-sum test*

> is a non=parametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater
> than X.


# Kruskal–Wallis test

*or Kruskal–Wallis one-way analysis of variance or one-way ANOVA on ranks*

> is a non-parametric method for testing whether samples originate from the same distribution.
> 
> The parametric equivalent of the Kruskal–Wallis test is the ANOVA

It used for comparing two or more independent samples of equal or different sample sizes. It exteds the *Mann-Whitney U test*, which is used for comparing only two groups.


# Kolmogorov–Smirnov test

*or KS test*

> is a nonparametric test of the equality of continuous, one dimensional probability distributions

It can be used to compare a sample with a reference probability distribution (one-sample KS) or to comapre two-samples (two-sample KS).









