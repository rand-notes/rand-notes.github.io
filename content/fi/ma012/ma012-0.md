---
date: "2021-09-28"
url: fi/ma012/0
title: ma012 lec0
math: true
---

> A bell curve is a graph depicting the normal distribution

# p-value

A p-value is a measure of the probability that an observed difference could have occurred just
by random chance. p-values are numbers in interval (0, 1) and commonly used threshold is 0.05.
The lower the p-value, the greater the statistical significance of the observed difference.
While a small p-value helps us decide if one group differs from another, it does not tell us how
different they are.

e.g.  
two groups (A: 73, B:125) and (A: 71, B: 127) results in p-value P=0.9. This means that the
difference is too small for us to be confident about it.

---

two groups (A: 60, B:138) and (A: 84, B: 114) - (30% vs 42%) results in p-value P=0.01, thus we
can say that these groups differs.

However it might be just an coincidence, if we get a small p-value when there is no difference,
it's called a **False Positive**

---

If we need to be really sure the results are correct, we use smaller threshold. e.g. using a
threshold 0.00001 means that we would get a False Positive only once every 100 000 experiments.


# Central Limit Theorem

The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size gets larger, regardless of the population's distribution.

# Population Distribution

The population is the whole set of values, we are interested in.

# Sample Distribution

The sample is a subset of the population, and the set of values you actually use in your
estimation.

# Sampling Distribution

Researchers often use a sample to draw inferences about the population that sample is from. To
do that, they make use of a probability distribution that is very important in the world of
statistics: the sampling distribution.

# Descriptive statistics
- summarizes sample data via summarization techniques and indexes
- frequency tables, moments (mean, variance, skewness, kurtoisis) and quantiles (median, quartiles, interquartiles spans), contingency tables, correlation coefficients

# Frequency distribution i.e. frequency list/table
is a list, table or graph that display the frequency of various outcomes in a sample

# Exploratory Data Analysis (EDA)
- analysis of sample data, usually via visualization methods
- frequency plots, boxplots, histograms, scatter plots, PCA

# Statistical Inference
- derives probability properties (arguments, distribution) of population based on analysis of sample data
- requires a model - establishing a assumptions about population and data sample
- point and interval parameter estimates (confidence intervals), testing statistics hypothesis, predictions, classifications, clustering

# Parametric methods
- model establish probability distribution or some set, parameters are being examined via these probability distributions
- most of classic methods like t-test, linear regression model, Multivariate regression

# Nonparametric methods
or distribution-free statistical methods do not depend on having a normal distribution and can be used with skewed data or with categorical data (Spearman's rho, Mann-Whitney, Wilcoxon Test)

# Semiparametric methods
- combination of parametric and nonparametric mothods
- Cox model of proportional risks in survival analysis

# Nominal data (categorical)
distinguishes different categories for qualitative variables; 
dummy code: gender, ethnic background
R: factor

# Ordinal data
data exists in categories that are ordered but differences cannot be determined or they are meaningless. (Example: 1st, 2nd, 3rd)
R: ordered factor

# Interval data
Differences between values can be found, but there is no absolute 0. (Temp. and Time),
R: numeric

# Ratio Data
data with an absolute 0. Ratios are meaningful. (Length, Width, Weight, Distance)

# Null Hypothesis (H0)
a statement that the performance of treatment groups is so similar that the groups must belong to the same population; a way of saying that the experimental manipulation had no important effect

- all level factors have same mean values

# Alternative hypothesis (H1)
The hypothesis that states there is a difference between two or more sets of data.

- at least one pair of level factors differ in mean value

# significance level (alpha)
The acceptable level of error selected by the researcher, usually set at 0.05;
what we compare our p-value to

# Power of a test
- The probability of correctly detecting a false null hypothesis
- could be increased by increasing sample data size

# classical statistical hypothesis testing; according to critical field
- establish H0 and H1
- establish model - i.e. statistics assumptions
- choose test and testing statistics T with known probability distribution (under the validity of H0)
- choose significance level (Î±) - usually 0.05
- compute observed values *t* of testing statistics *T*
- establish critical region *W* according to *H1* and *t*
- H0 is rejected, if $$t \in W$$

# statistical hypothesis testing; via p-value
- compute p-value *p*
- H0 is rejected, if $$p \lt \alpha$$

# p-value
The probability level which forms basis for deciding if results are statistically significant (not due to chance).

# t-test
a statistical test used to evaluate the size and significance of the difference between two means

# multiple testing
the more statistical tests we do, the more likely we are to make an erroneous inference (unless we adjust our significance threshold).

# ANOVA (analysis of variance)
-differences among MEANS of continuous (numerical) variables
-more flexible than t-tests-->can analyze differences among MORE THAN 2 groups (even if diff sample sizes)

# multiple comparison tests
statistical tests used to make pairwise comparisons to find which means differ significantly from one another in a one-factor multilevel design

- Tukey or Scheffe method

# Tukey's method
it's preferred especially in case that all partial random selections have approximately same range

# Scheffe method
it's recommended especially in case that partial random selections have distinctly different ranges.

# The means / minimalni (nulovy) model
$$Y_{i,j} = \mu + \epsilon_{i,j}$$

- \mu (micro, u) - grand mean of Y, same for all levels of factor A
- eps - stochaistic independent random variables with probability distribution N(0, \o^2)

# The effect model / jednoducheho trideni
$$Y_{i,j} = \mu + \alpha + \epsilon_{i,j}$$

- alpha - effect of i-th level of factor A

# Grand mean / spolecna stredni hodnota
$$M: \mu^{\wedge} = Y^{\wedge}$$


# effect i-th category in model M
$$\alpha_{i}^{\wedge} = \overline{Y}_{i.} \overline{Y}_{..}$$

# mean value of i-th category in model M
$$\mu_{i}^{\wedge} = \mu^{\wedge} + \alpha^{\wedge}_{i} = \overline{Y}_{i.}$$

# common part of mean value in submodel M_0
$$\mu^{\wedge} = \overline{Y}$$

# Square sums describing variability
- Total sum of squares , SS / celkovy soucet ctvercu
- Between-groups SS / skupinovy soucet ctvercu
- Within-groups/error/residual SS / rezidualni soucet ctvercu

# Parameters estimates in models
- Gran mean / spolecna stredni hodnota
- effect i-th category in model M
- mean value of i-th category in model M
- common part of mean value in submodel M_0
